import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import sys
from datetime import datetime
import glob
import argparse

# æ›¿æ¢ä¸ºä½ çš„å®é™…å‡­è¯å’Œè¡¨æ ¼ä¿¡æ¯
author = "guoxy"

coverage_url = "https://coveralls.io/github/taosdata/TDengine?branch=cover/3.0"

feishu_head = "Testing coverage report for taosd/taosc"

send_to_feishu_notify_url_test = (
    #è¦†ç›–ç‡Platform notifyç¾¤-test
    "https://open.feishu.cn/open-apis/bot/v2/hook/11e9e452-34a0-4c88-b014-10e21cb521dd"
)

send_to_feishu_alert_url_test = (
    #è¦†ç›–ç‡Platform Alertsç¾¤-test
    "https://open.feishu.cn/open-apis/bot/v2/hook/c2bebe49-2cc1-4566-81ce-893d029aefb1"
)

send_to_feishu_notify_url = (
    #è¦†ç›–ç‡Platform notifyç¾¤-true
    "https://open.feishu.cn/open-apis/bot/v2/hook/56c333b5-eae9-4c18-b0b6-7e4b7174f5c9"
)

send_to_feishu_alert_url = (
    #è¦†ç›–ç‡Platform Alertsç¾¤-true
    "https://open.feishu.cn/open-apis/bot/v2/hook/02363732-91f1-49c4-879c-4e98cf31a5f3"
)

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='TDengineè¦†ç›–ç‡å‘Šè­¦è„šæœ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python3 tdengine_coveage_alarm.py                    # æ­£å¸¸æ¨¡å¼
  python3 tdengine_coveage_alarm.py --test-mode        # æµ‹è¯•æ¨¡å¼
  python3 tdengine_coveage_alarm.py --preview          # é¢„è§ˆæ¨¡å¼
  python3 tdengine_coveage_alarm.py --data-clean       # å¯ç”¨æ•°æ®æ¸…æ´—
  python3 tdengine_coveage_alarm.py -p -clean -test    # ç»„åˆä½¿ç”¨
  python3 tdengine_coveage_alarm.py -url https://coveralls.io/jobs/172828865  # å¤–éƒ¨ä½¿ç”¨
        """
    )
    
    parser.add_argument('--test-mode', '-test', 
                       action='store_true', 
                       help='ä½¿ç”¨æµ‹è¯•æ¨¡å¼ï¼ˆå‘é€åˆ°æµ‹è¯•ç¾¤ç»„ï¼‰')
    
    parser.add_argument('--data-clean', '-clean', 
                       action='store_true',
                       help='å¯ç”¨æ•°æ®æ¸…æ´—ï¼Œè¿‡æ»¤å¼‚å¸¸æ³¢åŠ¨çš„è¦†ç›–ç‡æ•°æ®')
    
    parser.add_argument('--outlier-threshold', '-threshold', 
                       type=float, 
                       default=3.0,
                       metavar='FLOAT',
                       help='å¼‚å¸¸å€¼é˜ˆå€¼ï¼ˆç™¾åˆ†ç‚¹ï¼‰ï¼Œé»˜è®¤3.0%%')
    
    parser.add_argument('--preview', '-p', 
                       action='store_true',
                       help='é¢„è§ˆæ¨¡å¼ï¼šåªæ‰“å°æœ€æ–°5æ¡è¦†ç›–ç‡æ•°æ®å¹¶å‘é€åˆ°æµ‹è¯•å‘Šè­¦ç¾¤')
    
    parser.add_argument('-url', 
                       type=str,
                       help='è·å–æŒ‡å®šURLçš„è¯¦ç»†è¦†ç›–ç‡ä¿¡æ¯ï¼Œè¿”å›JSONæ ¼å¼')
    
    # æ·»åŠ ç‰ˆæœ¬ä¿¡æ¯
    parser.add_argument('--version', '-v', 
                       action='version', 
                       version='TDengineè¦†ç›–ç‡å‘Šè­¦è„šæœ¬ v1.0')
    
    return parser.parse_args()

# ä¿®æ”¹URLé€‰æ‹©é€»è¾‘
def get_webhook_urls(test_mode=False):
    """æ ¹æ®æ¨¡å¼è·å–å¯¹åº”çš„webhook URLs"""
    if test_mode:
        return {
            'notify': send_to_feishu_notify_url_test,
            'alert': send_to_feishu_alert_url_test
        }
    else:
        return {
            'notify': send_to_feishu_notify_url,
            'alert': send_to_feishu_alert_url
        }

def get_coverage_data():
    """
    è·å–Recent buildsè¡¨æ ¼ä¸­çš„æ‰€æœ‰è¦†ç›–ç‡æ•°æ®
    return: è¿”å›æŒ‰æ—¶é—´æ’åºçš„è¦†ç›–ç‡æ•°æ®åˆ—è¡¨ï¼ŒåŒ…å«å®Œæ•´çš„commitä¿¡æ¯å’Œè¯¦ç»†é“¾æ¥
    """
    url = coverage_url
    response = requests.get(url)
    response.raise_for_status()

    soup = BeautifulSoup(response.text, "html.parser")
    
    coverage_data = []
    
    # æŸ¥æ‰¾ Recent builds è¡¨æ ¼
    table = None
    
    # æ–¹æ³•1: æŸ¥æ‰¾åŒ…å« Buildsã€Branchã€Commit ç­‰åˆ—æ ‡é¢˜çš„è¡¨æ ¼
    tables = soup.find_all("table")
    for t in tables:
        header_row = t.find("tr")
        if header_row:
            header_text = header_row.get_text()
            if "Builds" in header_text and "Coverage" in header_text:
                table = t
                break
    
    # æ–¹æ³•2: å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾classåŒ…å«buildsç›¸å…³çš„è¡¨æ ¼
    if not table:
        table = soup.find("table", class_=lambda x: x and ("builds" in x.lower() or "recent" in x.lower()))
    
    # æ–¹æ³•3: æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼ï¼Œå–æœ€å¯èƒ½çš„ä¸€ä¸ª
    if not table and tables:
        table = tables[0]  # é€šå¸¸ç¬¬ä¸€ä¸ªè¡¨æ ¼å°±æ˜¯buildsè¡¨æ ¼
    
    if not table:
        print("æœªæ‰¾åˆ° Recent builds è¡¨æ ¼")
        return coverage_data
    
    print(f"æ‰¾åˆ°è¡¨æ ¼ï¼Œå¼€å§‹è§£æ...")
    
    # è§£æè¡¨æ ¼è¡Œï¼ˆè·³è¿‡è¡¨å¤´ï¼‰
    rows = table.find_all("tr")[1:]  # è·³è¿‡ç¬¬ä¸€è¡Œè¡¨å¤´
    
    print(f"æ‰¾åˆ° {len(rows)} æ¡æ„å»ºè®°å½•")
    
    for idx, row in enumerate(rows):
        try:
            cells = row.find_all(["td", "th"])
            if len(cells) < 8:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åˆ—
                continue
                
            # æ ¹æ®è¡¨æ ¼ç»“æ„è§£æï¼š
            # Builds	Branch	Commit	Type	Ran	Committer	Via	Coverage
            
            # 1. Build Number å’Œè¯¦ç»†é“¾æ¥
            build_cell = cells[0]
            build_number = build_cell.get_text(strip=True).replace("#", "")
            
            # è·å–Buildè¯¦ç»†é“¾æ¥ (ç±»ä¼¼ https://coveralls.io/builds/76002268)
            build_detail_url = ""
            link_element = build_cell.find("a")
            if link_element:
                href = link_element.get("href")
                if href:
                    if href.startswith("/"):
                        build_detail_url = "https://coveralls.io" + href
                    else:
                        build_detail_url = href
            
            # 2. Branch
            coverage_branch = cells[1].get_text(strip=True)
            
            # 3. Commit - è·å–å®Œæ•´çš„commitä¿¡æ¯
            commit_cell = cells[2]
            commit_message = commit_cell.get_text(strip=True)
            
            # 4. å…¶ä»–ä¿¡æ¯
            build_type = cells[3].get_text(strip=True)
            created_time = cells[4].get_text(strip=True)
            committer = cells[5].get_text(strip=True)
            via = cells[6].get_text(strip=True)
            coverage_number = cells[7].get_text(strip=True)
            
            # ç¡®ä¿coverage_numberåŒ…å«%ç¬¦å·
            if "%" not in coverage_number:
                coverage_number = coverage_number + "%"
            
            coverage_data.append({
                "build_number": build_number,
                "build_detail_url": build_detail_url,  # æ–°å¢ï¼šBuildè¯¦ç»†é“¾æ¥
                "coverage_branch": coverage_branch,
                "commit_message": commit_message,      # å®Œæ•´çš„commitä¿¡æ¯
                "coverage_number": coverage_number,
                "created_time": created_time,
                "build_type": build_type,
                "committer": committer,
                "via": via,
                "index": idx,  # æ·»åŠ ç´¢å¼•ï¼Œ0è¡¨ç¤ºæœ€æ–°çš„
                "coverage_result_url": build_detail_url  # ä¿æŒå…¼å®¹æ€§ï¼Œä¸build_detail_urlç›¸åŒ
            })
            
            print(f"è®°å½• {idx + 1}: Build #{build_number}, è¦†ç›–ç‡: {coverage_number}, æ—¶é—´: {created_time}")
            print(f"    Commit: {commit_message[:200]}{'...' if len(commit_message) > 200 else ''}")
            print(f"    è¯¦ç»†é“¾æ¥: {build_detail_url}")
            
        except (IndexError, AttributeError) as e:
            print(f"è§£æç¬¬ {idx + 1} æ¡è®°å½•æ—¶å‡ºé”™: {e}")
            continue

    print(f"æˆåŠŸè§£æ {len(coverage_data)} æ¡è¦†ç›–ç‡è®°å½•")
    return coverage_data


def clean_coverage_data(coverage_data, outlier_threshold=5.0):
    """
    æ¸…æ´—è¦†ç›–ç‡æ•°æ®ï¼Œç§»é™¤å¼‚å¸¸æ³¢åŠ¨çš„æ•°æ®ç‚¹
    
    ç­–ç•¥ï¼š
    1. ç§»é™¤55%ä»¥ä¸‹çš„æ— æ•ˆæ•°æ®
    2. ä½¿ç”¨ç§»åŠ¨å¹³å‡æ£€æµ‹å¼‚å¸¸å€¼
    3. ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹ç¦»ç¾¤å€¼
    
    :param coverage_data: åŸå§‹è¦†ç›–ç‡æ•°æ®åˆ—è¡¨
    :param outlier_threshold: å¼‚å¸¸å€¼é˜ˆå€¼ï¼ˆç™¾åˆ†ç‚¹ï¼‰
    :return: æ¸…æ´—åçš„æ•°æ®åˆ—è¡¨
    """
    if len(coverage_data) < 3:
        return coverage_data
    
    print(f"ğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—ï¼Œå¼‚å¸¸å€¼é˜ˆå€¼: {outlier_threshold}%")
    
    # æå–è¦†ç›–ç‡æ•°å€¼
    coverage_values = []
    valid_data = []
    
    for data in coverage_data:
        coverage_num = float(data['coverage_number'].replace('%', ''))
        # ç­–ç•¥1: ç§»é™¤55%ä»¥ä¸‹çš„æ— æ•ˆæ•°æ®
        if coverage_num > 55.0:
            coverage_values.append(coverage_num)
            valid_data.append(data)
        else:
            print(f"ğŸš« ç§»é™¤ä½è¦†ç›–ç‡æ•°æ®: Build #{data['build_number']} ({coverage_num}%) - ä½äº55%é˜ˆå€¼")
    
    if len(valid_data) < 3:
        print("âš ï¸ æœ‰æ•ˆæ•°æ®ä¸è¶³3æ¡ï¼Œè·³è¿‡æ¸…æ´—")
        return coverage_data
    
    print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(coverage_data)} æ¡ï¼Œæœ‰æ•ˆæ•°æ®: {len(valid_data)} æ¡")
    
    # ç­–ç•¥2: ä½¿ç”¨ç§»åŠ¨å¹³å‡æ£€æµ‹å¼‚å¸¸å€¼
    cleaned_data = []
    
    for i, data in enumerate(valid_data):
        current_value = coverage_values[i]
        
        # è·å–å‰åçª—å£çš„æ•°æ®è¿›è¡Œå¹³æ»‘
        window_start = max(0, i - 2)
        window_end = min(len(coverage_values), i + 3)
        window_values = coverage_values[window_start:window_end]
        
        # è®¡ç®—çª—å£å†…çš„ä¸­ä½æ•°å’Œæ ‡å‡†å·®
        if len(window_values) >= 3:
            window_median = sorted(window_values)[len(window_values)//2]
            window_mean = sum(window_values) / len(window_values)
            
            # è®¡ç®—ä¸ä¸­ä½æ•°çš„åå·®
            deviation_from_median = abs(current_value - window_median)
            deviation_from_mean = abs(current_value - window_mean)
            
            # å¦‚æœåå·®è¶…è¿‡é˜ˆå€¼ï¼Œæ ‡è®°ä¸ºå¼‚å¸¸
            if deviation_from_median > outlier_threshold and deviation_from_mean > outlier_threshold:
                print(f"ğŸš« æ£€æµ‹åˆ°å¼‚å¸¸å€¼: Build #{data['build_number']} ({current_value}%) - åå·®ä¸­ä½æ•° {deviation_from_median:.2f}%, åå·®å‡å€¼ {deviation_from_mean:.2f}%")
                continue
        
        cleaned_data.append(data)
    
    # æ‰“å°æ¸…æ´—åçš„æ•°æ®æ¦‚è§ˆ
    if cleaned_data:
        print("ğŸ“‹ æ¸…æ´—åçš„æ•°æ®:")
        for i, data in enumerate(cleaned_data[:10]):  # åªæ˜¾ç¤ºå‰10æ¡
            print(f"   {i+1}. Build #{data['build_number']}: {data['coverage_number']}")
    
    return cleaned_data

def find_stable_comparison_data(coverage_data, current_data, threshold=3.0):
    """
    å¯»æ‰¾ç¨³å®šçš„å¯¹æ¯”æ•°æ®ï¼Œé¿å…ä¸å¼‚å¸¸å€¼å¯¹æ¯”
    
    :param coverage_data: è¦†ç›–ç‡æ•°æ®åˆ—è¡¨
    :param current_data: å½“å‰æ•°æ®
    :param threshold: ç¨³å®šæ€§é˜ˆå€¼
    :return: æœ€é€‚åˆå¯¹æ¯”çš„å†å²æ•°æ®
    """
    if len(coverage_data) < 2:
        return None
    
    current_value = float(current_data['coverage_number'].replace('%', ''))
    
    # å¯»æ‰¾æœ€è¿‘çš„ç¨³å®šæ•°æ®ç‚¹
    for i in range(1, min(len(coverage_data), 5)):  # æ£€æŸ¥æœ€è¿‘5æ¡è®°å½•
        candidate = coverage_data[i]
        candidate_value = float(candidate['coverage_number'].replace('%', ''))
        
        # å¦‚æœæ‰¾åˆ°ç›¸å¯¹ç¨³å®šçš„æ•°æ®ç‚¹ï¼Œä½¿ç”¨å®ƒ
        deviation = abs(current_value - candidate_value)
        if deviation <= threshold:
            print(f"ğŸ“Š æ‰¾åˆ°ç¨³å®šå¯¹æ¯”æ•°æ®: Build #{candidate['build_number']} ({candidate['coverage_number']}) - åå·® {deviation:.2f}%")
            return candidate
    
    # å¦‚æœæ²¡æ‰¾åˆ°ç¨³å®šçš„ï¼Œä½¿ç”¨æœ€è¿‘çš„æœ‰æ•ˆæ•°æ®
    return coverage_data[1] if len(coverage_data) > 1 else None

def extract_failed_info(log_file_paths):
    """æå–è¿‡æ»¤åçš„å¤±è´¥æµ‹è¯•ç”¨ä¾‹"""
    failed_info = []
    for log_file_path in log_file_paths:
        with open(log_file_path, 'r') as file:
            for line in file:
                line = line.strip()
                # æ£€æŸ¥è¡Œæ˜¯å¦ä»¥ "case" å¼€å¤´å¹¶ä¸”åŒ…å« "failed"
                if line.startswith("cases") and  "failed" in line:
                    if ('taostest' not in line ):
                        # 'passed' not in line and 
                        # 'warning' not in line and 
                        # 'ql' not in line and 
                        #'0;31m failed' not in line ):
                        failed_info.append(line.strip())
                # if ('failed' in line and 
                #     'taostest' not in line and 
                #     'passed' not in line and 
                #     'warning' not in line and 
                #     'output' not in line and 
                #     'args' not in line and 
                #     'Total' not in line and 
                #     'Exception' not in line and 
                #     'stream' not in line and 
                #     'tsma' not in line and 
                #     'cases/' not in line and 
                #     'ql' not in line and 
                #     #'valgrind' not in line and 
                #     '31m failed' not in line ):
                #     failed_info.append(line.strip())
                    
                    # # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ–‡ä»¶è·¯å¾„ï¼ˆ.py/.sim ç»“å°¾ï¼‰
                    # match = re.match(r'.*?/(.*?\.py|.*?\.sim)\b', line)
                    # if match:
                    #     test_case = match.group(1).lstrip('/')
                    #     failed_info.append(test_case)
    return failed_info


def compare_cells(data1, data2, webhook_urls):
    """
    å¯¹æ¯”ä¸¤æ¬¡ç»“æœ
    :param data1: å½“å‰æ•°æ®
    :param data2: ä¸Šä¸€æ¬¡æ•°æ®
    :param webhook_urls: webhook URLså­—å…¸
    """

    data1_1 = float(str(data1).replace("%", "")) / 100
    data2_1 = float(str(data2).replace("%", "")) / 100

    
    # ğŸ”§ ä¿®å¤ï¼šä»æ–‡ä»¶è¯»å–å†å²æœ€é«˜è¦†ç›–ç‡
    highest_coverage = get_highest_coverage()
    highest_coverage_value = highest_coverage  # ä¿æŒåŸå§‹çš„å†å²æœ€é«˜å€¼
    
    # å½“å‰è¦†ç›–ç‡ï¼ˆç™¾åˆ†æ¯”å½¢å¼ï¼‰
    current_coverage_value = data1_1 * 100
 
    # æ£€æŸ¥æ˜¯å¦äº§ç”Ÿæ–°çš„å†å²æœ€é«˜è¦†ç›–ç‡
    if current_coverage_value > highest_coverage_value:
        print(f"ğŸ‰ æ–°çš„è¦†ç›–ç‡è®°å½•ï¼å½“å‰è¦†ç›–ç‡ {current_coverage_value:.3f}% è¶…è¿‡å†å²æœ€é«˜è¦†ç›–ç‡ {highest_coverage_value:.3f}%")
        update_highest_coverage(current_coverage_value)
        # ğŸ”§ æ›´æ–°åï¼Œhighest_coverage_value åº”è¯¥ç­‰äºå½“å‰å€¼
        highest_coverage_value = current_coverage_value
    else:
        difference = round(highest_coverage_value - current_coverage_value, 3)
        print(f"å½“å‰è¦†ç›–ç‡ {current_coverage_value:.3f}% ç¦»å†å²æœ€é«˜è¦†ç›–ç‡ {highest_coverage_value:.3f}% è¿˜å·® {difference}%")

    log_file_paths = glob.glob('/root/coverage_test_2*.log')
    failed_info = extract_failed_info(log_file_paths)
    if failed_info:
        fail_case_message = "\n".join(failed_info)
        print(fail_case_message)
    else:
        fail_case_message = 'NULL'
        print("æ²¡æœ‰æ‰¾åˆ°åŒ…å« 'failed' çš„ä¿¡æ¯")
        
    # ğŸ”§ ä¿®å¤ï¼šæ‰€æœ‰è¿”å›éƒ½ä½¿ç”¨æ­£ç¡®çš„ highest_coverage_valueï¼ˆå†å²æœ€é«˜å€¼ï¼‰
    if data1_1 < 0.56:
        message = " Taosd && taosc ä»£ç è¦†ç›–ç‡ä½äº56%ï¼Œè¯·å¯†åˆ‡å…³æ³¨ï¼" 
        notifier = "xyguo@taosdata.com"
        return message, notifier, highest_coverage_value, webhook_urls['alert']
    elif (data1_1 - data2_1) > 0 and (current_coverage_value > highest_coverage):
        # æ³¨æ„ï¼šè¿™é‡Œæ¯”è¾ƒçš„æ˜¯æ›´æ–°å‰çš„å†å²æœ€é«˜å€¼
        message = f" Taosd && taosc ä»£ç è¦†ç›–ç‡ç”±{data2}ä¸Šå‡åˆ°{data1}ï¼Œäº§ç”Ÿæ–°çš„è¦†ç›–ç‡è®°å½•ï¼Œå½“å‰è¦†ç›–ç‡ä¸º {current_coverage_value:.3f}% è¶…è¿‡å†å²æœ€é«˜è¦†ç›–ç‡ {highest_coverage:.3f}%ï¼Œç»§ç»­åŠ æ²¹!å¤±è´¥çš„case:{fail_case_message}" 
        notifier = "slguan@taosdata.com"
        return message, notifier, highest_coverage_value, webhook_urls['alert']
    elif (data1_1 - data2_1) > 0 and (current_coverage_value <= highest_coverage):
        message = f" Taosd && taosc ä»£ç è¦†ç›–ç‡ç”±{data2}ä¸Šå‡åˆ°{data1}ï¼Œå½“å‰è¦†ç›–ç‡ä¸º {current_coverage_value:.3f}% ç¦»å†å²æœ€é«˜è¦†ç›–ç‡ {highest_coverage_value:.3f}% è¿˜å·® {round(highest_coverage_value - current_coverage_value, 3)}%ï¼Œç»§ç»­åŠ æ²¹!å¤±è´¥çš„case:{fail_case_message}" 
        notifier = "slguan@taosdata.com"
        return message, notifier, highest_coverage_value, webhook_urls['alert']
    elif (data1_1 - data2_1) < -0.003:
        message = f" Taosd && taosc ä»£ç è¦†ç›–ç‡ç”±{data2}ä¸‹é™åˆ°{data1}ï¼Œå½“å‰è¦†ç›–ç‡ä¸º {current_coverage_value:.3f}% ç¦»å†å²æœ€é«˜è¦†ç›–ç‡ {highest_coverage_value:.3f}% è¿˜å·® {round(highest_coverage_value - current_coverage_value, 3)}%ï¼Œè¯·å…³æ³¨!å¤±è´¥çš„case:{fail_case_message}" 
        notifier = "slguan@taosdata.com"
        return message, notifier, highest_coverage_value, webhook_urls['alert']
    else:
        message = f" Taosd && taosc æœ¬æ¬¡ä»£ç è¦†ç›–ç‡åŸºæœ¬ä¸å˜ï¼ˆ<0.3%ï¼‰ï¼Œå½“å‰è¦†ç›–ç‡ä¸º {current_coverage_value:.3f}% ç¦»å†å²æœ€é«˜è¦†ç›–ç‡ {highest_coverage_value:.3f}% è¿˜å·® {round(highest_coverage_value - current_coverage_value, 3)}%ï¼Œè¯·ç»§ç»­ä¿æŒ!" 
        notifier = "xyguo@taosdata.com"
        return message, notifier, highest_coverage_value, webhook_urls['notify']
    
def compare_cells_alarm(data1, data2, current_data, webhook_urls):
    """
    å¯¹æ¯”ä¸¤æ¬¡ç»“æœå¹¶å‘é€å‘Šè­¦
    :param data1: å½“å‰æ•°æ®
    :param data2: ä¸Šä¸€æ¬¡æ•°æ®  
    :param current_data: å½“å‰è®°å½•çš„å®Œæ•´æ•°æ®
    :param webhook_urls: webhook URLså­—å…¸
    """
    data1_1 = float(str(data1).replace("%", "")) / 100
    data2_1 = float(str(data2).replace("%", "")) / 100
   
    message, notifier, highest_coverage_thistime_value, webhook_url  = compare_cells(data1, data2, webhook_urls)
    
    print("message=", message)    
    print("notifier=", notifier)
    print("highest_coverage_thistime_value=", highest_coverage_thistime_value)
    print("webhook_url=", webhook_url)
    
    # ç›´æ¥ä½¿ç”¨å½“å‰æ•°æ®ä¸­çš„URLå’Œcommitä¿¡æ¯
    coverage_result_url = current_data.get('coverage_result_url', '')
    coverage_git_commit = current_data.get('commit_message', '')

    # åˆ¤æ–­æ˜¯å¦éœ€è¦å‘é€å‘Šè­¦
    if data1_1 < 0.56:
        print('è¦†ç›–ç‡ä½äº56%ï¼Œå‘é€ç´§æ€¥å‘Šè­¦')
        send_to_feishu(webhook_url, message, notifier, coverage_result_url, coverage_git_commit)
    elif (data1_1 - data2_1) > 0 and (data1_1 * 100 > highest_coverage_thistime_value):
        print('è¦†ç›–ç‡ä¸Šå‡ä¸”äº§ç”Ÿæ–°çš„è¦†ç›–ç‡è®°å½•')
        send_to_feishu(webhook_url, message, notifier, coverage_result_url, coverage_git_commit)
    elif (data1_1 - data2_1) > 0 and (data1_1 * 100 <= highest_coverage_thistime_value):
        print('è¦†ç›–ç‡ä¸Šå‡ä½†æœªäº§ç”Ÿæ–°çš„è¦†ç›–ç‡è®°å½•')
        send_to_feishu(webhook_url, message, notifier, coverage_result_url, coverage_git_commit)
    elif (data1_1 - data2_1) < -0.003:
        print('è¦†ç›–ç‡ä¸‹é™è¶…è¿‡0.5%ï¼Œå‘é€è­¦å‘Šæ¶ˆæ¯')
        send_to_feishu(webhook_url, message, notifier, coverage_result_url, coverage_git_commit)
    else:
        print('è¦†ç›–ç‡ä¿æŒä¸å˜ï¼Œå‘é€åˆ°Notifyç¾¤')
        send_to_feishu(webhook_url, message, notifier, coverage_result_url, coverage_git_commit)


def send_to_feishu(send_to_feishu_url, message, notifier, coverage_result_url,coverage_git_commit):
    """
    å‘é€æ¶ˆæ¯åˆ°é£ä¹¦
    :param send_to_feishu_url: é£ä¹¦æœºå™¨äººwebhookåœ°å€
    :param message: æ¶ˆæ¯å†…å®¹
    :param coverage_result_url: è¦†ç›–ç‡ç»“æœurl
    """
    headers = {"Content-Type": "application/json"}
    current_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    data = {
        "msg_type": "interactive",
        "card": {
            "type": "template",
            "data": 
                {
                "template_id": "AAqj7BO8oMQYF",
                "template_version_name": "1.0.2",
                "template_variable": {
                "title": "Taosd && taosc coverage report",
                "tag": "text",
                "message": message,
                "notifier":notifier,
                "coverage_result_url": coverage_result_url,
                "coverage_git_commit": coverage_git_commit,
                "current_date": current_date,
                "author": author,
                "bg_color": "green"
            }
        }
    }
    }

    response = requests.post(send_to_feishu_url, headers=headers, data=json.dumps(data))
    if response.status_code == 200:
        print("æ¶ˆæ¯å‘é€æˆåŠŸ")
    else:
        print(
            f"æ¶ˆæ¯å‘é€å¤±è´¥ï¼Œé”™è¯¯ä»£ç : {response.status_code}, é”™è¯¯ä¿¡æ¯ï¼š{response.text}"
        )


def get_highest_coverage():
    """
    è·å–å†å²æœ€é«˜è¦†ç›–ç‡
    """
    try:
        with open("highest_coverage.json", "r") as file:
            data = json.load(file)
            return data.get("highest_coverage", 0)
    except FileNotFoundError:
        return 0

def update_highest_coverage(new_coverage):
    """
    æ›´æ–°å†å²æœ€é«˜è¦†ç›–ç‡
    """
    data = {"highest_coverage": new_coverage}
    with open("highest_coverage.json", "w") as file:
        json.dump(data, file)

def get_detailed_coverage_info(build_detail_url):
    """
    è·å–æ„å»ºè¯¦ç»†é¡µé¢çš„å…³é”®è¦†ç›–ç‡ä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
    :param build_detail_url: æ„å»ºè¯¦ç»†é¡µé¢URL
    :return: è¯¦ç»†è¦†ç›–ç‡ä¿¡æ¯å­—ç¬¦ä¸²
    """
    if not build_detail_url:
        return None
    
    try:
        response = requests.get(build_detail_url, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        text_content = soup.get_text()
        
        import re
        
        # æå–å…³é”®ä¿¡æ¯å¹¶æ ¼å¼åŒ–æˆä¸€è¡Œ
        details = []
        
        # åˆ†æ”¯è¦†ç›–ç‡: 155455 of 324369 branches covered (47.93%)
        branch_match = re.search(r'(\d+)\s+of\s+(\d+)\s+branches\s+covered\s+\(([\d.]+)%\)', text_content)
        if branch_match:
            details.append(f"{branch_match.group(1)} of {branch_match.group(2)} branches covered ({branch_match.group(3)}%)")
        
        # ğŸ”§ ä¿®å¤ï¼šæ–°å¢ä»£ç è¦†ç›–ç‡çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ŒåŒ¹é…å•æ•°å’Œå¤æ•°å½¢å¼
        new_lines_match = re.search(r'(\d+)\s+of\s+(\d+)\s+new\s+or\s+added\s+lines?\s+(?:in\s+\d+\s+files?\s+)?covered[^(]*\(([\d.]+)%\)', text_content)
        if new_lines_match:
            details.append(f"{new_lines_match.group(1)} of {new_lines_match.group(2)} new or added line covered ({new_lines_match.group(3)}%)")
        
        # æœªè¦†ç›–ä»£ç : 2521 existing lines in 113 files now uncovered
        uncovered_match = re.search(r'(\d+)\s+existing\s+lines\s+in\s+(\d+)\s+files?\s+now\s+uncovered', text_content)
        if uncovered_match:
            details.append(f"{uncovered_match.group(1)} existing lines in {uncovered_match.group(2)} files now uncovered")
        
        # ç›¸å…³ä»£ç è¡Œ: 207702 of 269535 relevant lines covered (77.06%)
        relevant_lines_match = re.search(r'(\d+)\s+of\s+(\d+)\s+relevant\s+lines\s+covered\s+\(([\d.]+)%\)', text_content)
        if relevant_lines_match:
            details.append(f"{relevant_lines_match.group(1)} of {relevant_lines_match.group(2)} relevant lines covered ({relevant_lines_match.group(3)}%)")
        
        # å‘½ä¸­æ•°: 126661557.72 hits per line
        hits_match = re.search(r'([\d.]+)\s+hits\s+per\s+line', text_content)
        if hits_match:
            details.append(f"{hits_match.group(1)} hits per line")
        
        return ". ".join(details) if details else None
        
    except Exception as e:
        print(f"âš ï¸ è·å–è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
        return None

def send_preview_to_feishu(coverage_data, webhook_url):
    """
    å‘é€è¦†ç›–ç‡é¢„è§ˆæ•°æ®åˆ°é£ä¹¦æµ‹è¯•å‘Šè­¦ç¾¤ï¼ˆåŒ…å«å‰3æ¡çš„è¯¦ç»†ä¿¡æ¯å’Œé“¾æ¥ï¼‰
    :param coverage_data: æœ€æ–°çš„è¦†ç›–ç‡æ•°æ®åˆ—è¡¨
    :param webhook_url: é£ä¹¦webhook URL
    """
    if not coverage_data:
        print("âŒ æ²¡æœ‰è¦†ç›–ç‡æ•°æ®å¯é¢„è§ˆ")
        return
    
    # æ„å»ºé¢„è§ˆæ¶ˆæ¯ - æ˜¾ç¤º5æ¡åŸºæœ¬ä¿¡æ¯ï¼Œå‰3æ¡åŒ…å«è¯¦ç»†ä¿¡æ¯
    preview_data = coverage_data[:5]
    
    message_lines = ["ğŸ“Š TDengine æœ€æ–°è¦†ç›–ç‡æ•°æ®é¢„è§ˆ:\n"]
    
    for i, data in enumerate(preview_data):
        build_num = data['build_number']
        coverage = data['coverage_number']
        commit = data['commit_message'][:35] + "..." if len(data['commit_message']) > 35 else data['commit_message']
        created_time = data['created_time']
        detail_url = data.get('build_detail_url', '')
        
        # æ·»åŠ è¶‹åŠ¿æŒ‡ç¤ºç¬¦
        if i == 0:
            trend = "ğŸ”¥ æœ€æ–°"
        elif i < len(preview_data) - 1:
            current_val = float(coverage.replace('%', ''))
            next_val = float(preview_data[i + 1]['coverage_number'].replace('%', ''))
            if current_val > next_val:
                trend = "ğŸ“ˆ ä¸Šå‡"
            elif current_val < next_val:
                trend = "ğŸ“‰ ä¸‹é™"
            else:
                trend = "â¡ï¸ æŒå¹³"
        else:
            trend = "ğŸ“‹ åŸºå‡†"
        
        message_lines.append(f"{i+1}. Build #{build_num}: {coverage} {trend}")
        message_lines.append(f"   æ—¶é—´: {created_time}")
        message_lines.append(f"   æäº¤: {commit}")
        
        # ğŸ†• æ·»åŠ è¯¦ç»†é“¾æ¥
        if detail_url:
            message_lines.append(f"   ğŸ”— è¯¦æƒ…: {detail_url}")
        
        # ğŸ†• ä¸ºå‰3æ¡è®°å½•æ·»åŠ è¯¦ç»†ä¿¡æ¯ï¼ˆæ¯ç±»ä¿¡æ¯æ¢è¡Œæ˜¾ç¤ºï¼‰
        if i < 3 and detail_url:
            detailed_info = get_detailed_coverage_info(detail_url)
            if detailed_info:
                message_lines.append(f"   ğŸ“Š è¯¦ç»†ä¿¡æ¯:")
                
                # å°†è¯¦ç»†ä¿¡æ¯æŒ‰å¥å·åˆ†å‰²ï¼Œæ¯ç±»ä¿¡æ¯æ¢è¡Œæ˜¾ç¤º
                detail_parts = detailed_info.split('. ')
                for part in detail_parts:
                    if part.strip():  # ç¡®ä¿ä¸æ˜¯ç©ºå­—ç¬¦ä¸²
                        # ä¸ºæ¯è¡Œè¯¦ç»†ä¿¡æ¯æ·»åŠ é€‚å½“çš„ç¼©è¿›
                        message_lines.append(f"      {part.strip()}")
        
        message_lines.append("")
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    coverage_values = [float(d['coverage_number'].replace('%', '')) for d in preview_data]
    max_coverage = max(coverage_values)
    min_coverage = min(coverage_values)
    avg_coverage = sum(coverage_values) / len(coverage_values)
    
    message_lines.append(f"ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    message_lines.append(f"   æœ€é«˜: {max_coverage}% | æœ€ä½: {min_coverage}% | å¹³å‡: {avg_coverage:.2f}%")
    message_lines.append(f"   æ³¢åŠ¨èŒƒå›´: {max_coverage - min_coverage:.2f}%")
    
    # æ·»åŠ è¶‹åŠ¿åˆ†æ
    if len(coverage_values) >= 2:
        recent_trend = coverage_values[0] - coverage_values[1]
        if abs(recent_trend) >= 1.0:
            if recent_trend > 0:
                message_lines.append(f"\nğŸš¨ æ³¨æ„: æœ€æ–°è¦†ç›–ç‡è¾ƒä¸Šæ¬¡ä¸Šå‡ {recent_trend:.2f}%")
            else:
                message_lines.append(f"\nâš ï¸ è­¦å‘Š: æœ€æ–°è¦†ç›–ç‡è¾ƒä¸Šæ¬¡ä¸‹é™ {abs(recent_trend):.2f}%")
        else:
            message_lines.append(f"\nâœ… è¦†ç›–ç‡å˜åŒ–å¹³ç¨³ ({recent_trend:+.2f}%)")
    
    message = "\n".join(message_lines)
    
    print("ğŸ“¤ å‘é€é¢„è§ˆæ•°æ®åˆ°é£ä¹¦æµ‹è¯•å‘Šè­¦ç¾¤...")
    
    # å‘é€åˆ°é£ä¹¦
    headers = {"Content-Type": "application/json"}
    
    data = {
        "msg_type": "text",
        "content": {
            "text": message
        }
    }
    
    try:
        response = requests.post(webhook_url, headers=headers, data=json.dumps(data), timeout=30)
        
        if response.status_code == 200:
            result = response.json()
            if result.get("code") == 0:
                print("âœ… é¢„è§ˆæ•°æ®å‘é€æˆåŠŸåˆ°æµ‹è¯•å‘Šè­¦ç¾¤")
            else:
                print(f"âŒ é£ä¹¦APIè¿”å›é”™è¯¯: {result}")
        else:
            print(f"âŒ HTTPè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
            
    except Exception as e:
        print(f"âŒ å‘é€é¢„è§ˆæ•°æ®æ—¶å‡ºé”™: {e}")

def print_preview_data(coverage_data):
    """
    åœ¨æ§åˆ¶å°æ‰“å°æœ€æ–°çš„5æ¡è¦†ç›–ç‡æ•°æ®ï¼ˆæ˜¾ç¤ºå‰3æ¡çš„è¯¦ç»†ä¿¡æ¯ï¼Œæ¯ç±»ä¿¡æ¯æ¢è¡Œï¼‰
    :param coverage_data: è¦†ç›–ç‡æ•°æ®åˆ—è¡¨
    """
    if not coverage_data:
        print("âŒ æ²¡æœ‰è¦†ç›–ç‡æ•°æ®å¯æ˜¾ç¤º")
        return
    
    print("\nğŸ“Š TDengine æœ€æ–°è¦†ç›–ç‡æ•°æ®é¢„è§ˆ:")
    print("="*120)
    
    preview_data = coverage_data[:5]
    
    for i, data in enumerate(preview_data):
        build_num = data['build_number']
        coverage = data['coverage_number']
        commit = data['commit_message']
        created_time = data['created_time']
        detail_url = data.get('build_detail_url', '')
        
        # æ·»åŠ è¶‹åŠ¿æ ‡ç­¾
        if i == 0:
            trend = "ğŸ”¥ æœ€æ–°"
        elif i < len(preview_data) - 1:
            current_val = float(coverage.replace('%', ''))
            next_val = float(preview_data[i + 1]['coverage_number'].replace('%', ''))
            if current_val > next_val:
                trend = "ğŸ“ˆ ä¸Šå‡"
            elif current_val < next_val:
                trend = "ğŸ“‰ ä¸‹é™"
            else:
                trend = "â¡ï¸ æŒå¹³"
        else:
            trend = "ğŸ“‹ åŸºå‡†"
        
        print(f"\n{i+1}. Build #{build_num}: {coverage} {trend}")
        print(f"   æ—¶é—´: {created_time}")
        print(f"   æäº¤: {commit}")
        print(f"   è¯¦ç»†é“¾æ¥: {detail_url}")
        
        # ğŸ†• ä¸ºå‰3æ¡è®°å½•è·å–è¯¦ç»†ä¿¡æ¯ï¼ˆæ¯ç±»ä¿¡æ¯æ¢è¡Œæ˜¾ç¤ºï¼‰
        if i < 3 and detail_url:
            print(f"   ğŸ” è·å–è¯¦ç»†è¦†ç›–ç‡ä¿¡æ¯...")
            detailed_info = get_detailed_coverage_info(detail_url)
            if detailed_info:
                print(f"   ğŸ“Š è¯¦ç»†ä¿¡æ¯:")
                
                # å°†è¯¦ç»†ä¿¡æ¯æŒ‰å¥å·åˆ†å‰²ï¼Œæ¯ç±»ä¿¡æ¯æ¢è¡Œæ˜¾ç¤º
                detail_parts = detailed_info.split('. ')
                for part in detail_parts:
                    if part.strip():  # ç¡®ä¿ä¸æ˜¯ç©ºå­—ç¬¦ä¸²
                        print(f"      {part.strip()}")
            else:
                print(f"   âš ï¸ æ— æ³•è·å–è¯¦ç»†ä¿¡æ¯")
        
        print("-" * 120)


def get_build_time_from_url(url):
    """
    ä»æ„å»ºé¡µé¢è·å–æ„å»ºæ—¶é—´
    :param url: æ„å»ºURL
    :return: æ„å»ºæ—¶é—´å­—ç¬¦ä¸²ï¼Œæ ¼å¼å¦‚ "16 Oct 2025 01:24PM UTC"
    """
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        
        # æ–¹æ³•1: æŸ¥æ‰¾æ—¶é—´ç›¸å…³çš„å…ƒç´ 
        # å¯»æ‰¾åŒ…å«æ—¶é—´ä¿¡æ¯çš„å…ƒç´ ï¼Œå¯èƒ½çš„é€‰æ‹©å™¨ï¼š
        time_selectors = [
            'time',  # HTML5 time å…ƒç´ 
            '[datetime]',  # æœ‰ datetime å±æ€§çš„å…ƒç´ 
            '.build-time',  # å¯èƒ½çš„classå
            '.created-at',  # å¯èƒ½çš„classå
            '.timestamp'    # å¯èƒ½çš„classå
        ]
        
        for selector in time_selectors:
            time_element = soup.select_one(selector)
            if time_element:
                # å°è¯•ä»datetimeå±æ€§è·å–
                datetime_attr = time_element.get('datetime')
                if datetime_attr:
                    # è½¬æ¢ISOæ ¼å¼åˆ°æ‰€éœ€æ ¼å¼
                    try:
                        dt = datetime.fromisoformat(datetime_attr.replace('Z', '+00:00'))
                        return dt.strftime("%d %b %Y %I:%M%p UTC")
                    except:
                        pass
                
                # ä»æ–‡æœ¬å†…å®¹è·å–
                time_text = time_element.get_text(strip=True)
                if time_text and ('UTC' in time_text or 'GMT' in time_text or any(month in time_text for month in ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])):
                    return time_text
        
        # æ–¹æ³•2: ä»é¡µé¢æ–‡æœ¬ä¸­æŸ¥æ‰¾æ—¶é—´æ¨¡å¼
        page_text = soup.get_text()
        
        # åŒ¹é…ç±»ä¼¼ "16 Oct 2025 01:24PM UTC" çš„æ¨¡å¼
        import re
        time_patterns = [
            r'\d{1,2}\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{4}\s+\d{1,2}:\d{2}(?:AM|PM)\s+UTC',
            r'\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}\s+UTC',
            r'(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},?\s+\d{4}\s+\d{1,2}:\d{2}(?::\d{2})?\s*(?:AM|PM)?\s*UTC'
        ]
        
        for pattern in time_patterns:
            match = re.search(pattern, page_text)
            if match:
                return match.group(0)
        
        # æ–¹æ³•3: å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•ä»URLå¯¹åº”çš„æ•°æ®ä¸­è·å–
        # å¦‚æœURLæ˜¯buildé¡µé¢ï¼Œå°è¯•ä»å·²è·å–çš„coverage_dataä¸­æŸ¥æ‰¾
        if "builds/" in url:
            build_id = url.split("builds/")[-1]
            # è¿™é‡Œå¯ä»¥æŸ¥æ‰¾ä¹‹å‰è·å–çš„coverage_data
            # ä½†ç”±äºè¿™æ˜¯ç‹¬ç«‹è°ƒç”¨ï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°è·å–
            coverage_data = get_coverage_data()
            for data in coverage_data:
                if data.get('build_detail_url') == url:
                    return data.get('created_time', '')
        
        return None
        
    except Exception as e:
        print(f"âš ï¸ è·å–æ„å»ºæ—¶é—´å¤±è´¥: {e}")
        return None
   
def get_coverage_change_info(build_detail_url):
    """
    ä»æ„å»ºé¡µé¢è·å–è¦†ç›–ç‡å˜åŒ–ä¿¡æ¯
    :param build_detail_url: æ„å»ºè¯¦ç»†é¡µé¢URL
    :return: è¦†ç›–ç‡å˜åŒ–ä¿¡æ¯ï¼Œæ ¼å¼å¦‚ "coverage: 53.935% (-7.1%) from 61.083%"
    """
    if not build_detail_url:
        return None
    
    try:
        response = requests.get(build_detail_url, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        page_text = soup.get_text()
        
        import re
        
        # åŒ¹é…è¦†ç›–ç‡å˜åŒ–ä¿¡æ¯çš„æ¨¡å¼
        coverage_patterns = [
            # æ ‡å‡†æ ¼å¼ï¼šcoverage: XX% (Â±XX%) from XX%
            r'coverage:\s*([\d.]+%)\s*\(([+-][\d.]+%)\)\s*from\s*([\d.]+%)',
            # å¯èƒ½çš„å˜ä½“æ ¼å¼
            r'Coverage:\s*([\d.]+%)\s*\(([+-][\d.]+%)\)\s*from\s*([\d.]+%)',
            # æ›´å®½æ¾çš„åŒ¹é…
            r'([\d.]+%)\s*\(([+-][\d.]+%)\)\s*from\s*([\d.]+%)'
        ]
        
        for pattern in coverage_patterns:
            match = re.search(pattern, page_text)
            if match:
                current_cov = match.group(1)
                change = match.group(2)
                from_cov = match.group(3)
                
                return f"coverage: {current_cov} ({change}) from {from_cov}"
        
        return None
        
    except Exception as e:
        print(f"âš ï¸ è·å–è¦†ç›–ç‡å˜åŒ–ä¿¡æ¯å¤±è´¥: {e}")
        return None 
    
def get_build_details_by_url(url):
    """
    æ ¹æ®ä¼ å…¥çš„URLè·å–å¯¹åº”æ„å»ºçš„è¯¦ç»†ä¿¡æ¯
    :param url: coverallsæ„å»ºURL
    :return: è¯¦ç»†ä¿¡æ¯JSON
    """
    try:
        # print(f"ğŸ” å¤„ç†URL: {url}")
        
        # è·å–è¯¦ç»†è¦†ç›–ç‡ä¿¡æ¯
        detailed_info = get_detailed_coverage_info(url)
        
        # è·å–æ„å»ºæ—¶é—´
        build_time = get_build_time_from_url(url)
        
        # è·å–è¦†ç›–ç‡å˜åŒ–ä¿¡æ¯
        coverage_change = get_coverage_change_info(url)
        
        if detailed_info:
            # æ ¼å¼åŒ–è¿”å›è¯¦ç»†ä¿¡æ¯
            detail_parts = detailed_info.split('. ')
            formatted_details = []
            for part in detail_parts:
                if part.strip():
                    formatted_details.append(part.strip())
            
            return {
                "status": "success",
                "url": url,
                "coverage_change": coverage_change,
                "detailed_info": {
                    # "raw": detailed_info,
                    "formatted": formatted_details
                },
                "build_time": build_time
            }
        else:
            return {
                "status": "error",
                "message": "æ— æ³•è·å–è¯¦ç»†ä¿¡æ¯",
                "url": url,
                "coverage_change": coverage_change,
                "build_time": build_time
            }
            
    except Exception as e:
        return {
            "status": "error",
            "message": str(e),
            "url": url,
            "coverage_change": None,
            "build_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        
if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    # ğŸ†• URLæ¨¡å¼ï¼šå¤„ç†å•ä¸ªURLè¯·æ±‚
    if args.url:
        # print(f"ğŸ”Œ APIæ¨¡å¼ - å¤„ç†URL: {args.url}")
        result = get_build_details_by_url(args.url)
        
        # è¾“å‡ºJSONç»“æœ
        print(json.dumps(result, indent=2, ensure_ascii=False))
        exit(0)
    
    # æ ¹æ®å‚æ•°é€‰æ‹©URL
    webhook_urls = get_webhook_urls(args.test_mode)
    
    # æ˜¾ç¤ºå½“å‰æ¨¡å¼
    mode_text = "æµ‹è¯•æ¨¡å¼" if args.test_mode else "æ­£å¼æ¨¡å¼"
    clean_text = "å¯ç”¨" if args.data_clean else "ç¦ç”¨"
    preview_text = "å¯ç”¨" if args.preview else "ç¦ç”¨"
    
    print(f"ğŸš€ è¿è¡Œæ¨¡å¼: {mode_text}")
    print(f"ğŸ§¹ æ•°æ®æ¸…æ´—: {clean_text}")
    print(f"ğŸ‘ï¸ é¢„è§ˆæ¨¡å¼: {preview_text}")
    
    if args.data_clean:
        print(f"ğŸ“Š å¼‚å¸¸å€¼é˜ˆå€¼: {args.outlier_threshold}%")
    if args.preview:
        print(f"ğŸ“¤ é¢„è§ˆæ•°æ®å°†å‘é€åˆ°: {send_to_feishu_alert_url_test}")
    else:
        print(f"ğŸ“¢ é€šçŸ¥ç¾¤URL: {webhook_urls['notify']}")
        print(f"ğŸš¨ å‘Šè­¦ç¾¤URL: {webhook_urls['alert']}")
    
    coverage_data = get_coverage_data()
        
    # ğŸ†• é¢„è§ˆæ¨¡å¼ï¼šåªæ˜¾ç¤ºå’Œå‘é€æœ€æ–°æ•°æ®
    if args.preview:
        print_preview_data(coverage_data)
        send_preview_to_feishu(coverage_data, send_to_feishu_alert_url_test)
        print("\nâœ… é¢„è§ˆæ¨¡å¼å®Œæˆï¼Œç¨‹åºç»“æŸ")
        exit(0)
        
    # æ•°æ®æ¸…æ´—ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if args.data_clean and coverage_data:
        coverage_data = clean_coverage_data(coverage_data, args.outlier_threshold)
        
    if len(coverage_data) >= 2:
        # è·å–æœ€æ–°çš„ä¸¤æ¡è®°å½•è¿›è¡Œå¯¹æ¯”
        current_data = coverage_data[0]  # æœ€æ–°çš„è®°å½•
        
        # æ™ºèƒ½é€‰æ‹©å¯¹æ¯”æ•°æ®
        if args.data_clean:
            comparison_data = find_stable_comparison_data(coverage_data, current_data, args.outlier_threshold)
        else:
            comparison_data = coverage_data[1]  # ç›´æ¥ä½¿ç”¨ä¸Šä¸€æ¡è®°å½•
        
        if comparison_data:
            current_coverage = current_data['coverage_number']
            previous_coverage = comparison_data['coverage_number']
            
            print(f"å½“å‰è¦†ç›–ç‡: {current_coverage}")
            print(f"å¯¹æ¯”è¦†ç›–ç‡: {previous_coverage} (Build #{comparison_data['build_number']})")
            
            # è¿›è¡Œå¯¹æ¯”å’Œå‘Šè­¦
            compare_cells_alarm(current_coverage, previous_coverage, current_data, webhook_urls)
        else:
            print("âš ï¸ æ— æ³•æ‰¾åˆ°åˆé€‚çš„å¯¹æ¯”æ•°æ®")
        
    elif len(coverage_data) == 1:
        # åªæœ‰ä¸€æ¡è®°å½•ï¼Œä¸é»˜è®¤å€¼å¯¹æ¯”æˆ–è·³è¿‡å¯¹æ¯”
        current_data = coverage_data[0]
        current_coverage = current_data['coverage_number']
        print(f"åªæœ‰ä¸€æ¡è®°å½•ï¼Œå½“å‰è¦†ç›–ç‡: {current_coverage}")
        
        # å¯ä»¥ä¸å†å²æœ€é«˜è¦†ç›–ç‡å¯¹æ¯”
        highest_coverage = get_highest_coverage()
        if highest_coverage > 0:
            compare_cells_alarm(current_coverage, f"{highest_coverage}%", current_data, webhook_urls)
            
    else:
        print("æœªè·å–åˆ°ä»»ä½•è¦†ç›–ç‡æ•°æ®")