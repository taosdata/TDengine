import json
import subprocess
import threading
import psutil
import time
import taos
import shutil
import argparse
import os
import signal
import sys
import datetime

"""TDengine New Stream Computing Performance Test
TDengine æ–°æ•°æ®æµè®¡ç®—æ€§èƒ½æµ‹è¯•å·¥å…·

Purpose/ç”¨é€”:
    TDengine æµè®¡ç®—æ€§èƒ½æµ‹è¯•å·¥å…·ï¼Œæ”¯æŒå¤šç§æµ‹è¯•æ¨¡å¼å’Œæµè®¡ç®—ç±»å‹ã€‚
    æä¾›å®Œæ•´çš„æ•°æ®ç”Ÿæˆã€æµè®¡ç®—åˆ›å»ºã€æ€§èƒ½ç›‘æ§ã€å»¶è¿Ÿæ£€æµ‹ç­‰åŠŸèƒ½ã€‚
    é€‚ç”¨äºæµè®¡ç®—æ€§èƒ½åŸºå‡†æµ‹è¯•ã€ç³»ç»Ÿè°ƒä¼˜ã€å‹åŠ›æµ‹è¯•ç­‰åœºæ™¯ã€‚

Catalog/ç›®å½•:
    - Performance Testing / æ€§èƒ½æµ‹è¯•
    - Stream Computing / æµè®¡ç®—  
    - Real-time Analytics / å®æ—¶åˆ†æ
    - Benchmarking / åŸºå‡†æµ‹è¯•

Features/åŠŸèƒ½:
    âœ¨ æµè®¡ç®—æ€§èƒ½æµ‹è¯•
        - æ”¯æŒæ»‘åŠ¨çª—å£ã€ä¼šè¯çª—å£ã€è®¡æ•°çª—å£ã€äº‹ä»¶çª—å£ã€çŠ¶æ€çª—å£ã€å®šæ—¶è§¦å‘ç­‰6ç§çª—å£ç±»å‹
        - æ”¯æŒè¶…çº§è¡¨å’Œå­è¡¨æ•°æ®æºï¼Œæ”¯æŒæŒ‰å­è¡¨åå’Œtagåˆ†ç»„
        - æ”¯æŒèšåˆæŸ¥è¯¢å’ŒæŠ•å½±æŸ¥è¯¢ä¸¤ç§è®¡ç®—æ¨¡å¼
        - æ”¯æŒå•æµå’Œå¤šæµå¹¶å‘æµ‹è¯•
        
    ğŸ“Š å®æ—¶æ€§èƒ½ç›‘æ§
        - ç³»ç»Ÿèµ„æºç›‘æ§ï¼šCPUã€å†…å­˜ã€ç£ç›˜I/Oå®æ—¶ç›‘æ§
        - æµè®¡ç®—å»¶è¿Ÿç›‘æ§ï¼šæºè¡¨ä¸ç›®æ ‡è¡¨æ•°æ®å»¶è¿Ÿæ£€æµ‹
        - åŠ¨æ€é˜ˆå€¼å‘Šè­¦ï¼šåŸºäºæ£€æŸ¥é—´éš”çš„æ™ºèƒ½å»¶è¿Ÿåˆ†çº§
        - è¯¦ç»†æ€§èƒ½æŠ¥å‘Šï¼šå›¾è¡¨åŒ–å±•ç¤ºå’Œé—®é¢˜åˆ†æå»ºè®®
        
    ğŸ—„ï¸ æ•°æ®ç®¡ç†
        - è‡ªåŠ¨æ•°æ®ç”Ÿæˆï¼šæ”¯æŒå¯é…ç½®çš„è¡¨æ•°é‡ã€è®°å½•æ•°ã€ä¹±åºç‡
        - æ•°æ®å¤‡ä»½æ¢å¤ï¼šæ”¯æŒå®Œæ•´çš„æ•°æ®å¤‡ä»½å’Œå¿«é€Ÿæ¢å¤
        - å†å²æ•°æ®æµ‹è¯•ï¼šåŸºäºå·²æœ‰æ•°æ®è¿›è¡Œæµè®¡ç®—æ€§èƒ½æµ‹è¯•
        - å¢é‡æ•°æ®å†™å…¥ï¼šæ¨¡æ‹Ÿå®æ—¶æ•°æ®æµåœºæ™¯
        
    ğŸ—ï¸ éƒ¨ç½²æ¶æ„
        - å•èŠ‚ç‚¹æ¨¡å¼ï¼šé€‚ç”¨äºå¼€å‘æµ‹è¯•å’Œå°è§„æ¨¡éªŒè¯
        - é›†ç¾¤æ¨¡å¼ï¼šæ”¯æŒ3èŠ‚ç‚¹é›†ç¾¤çš„é«˜å¯ç”¨æµ‹è¯•
        - è‡ªåŠ¨ç¯å¢ƒå‡†å¤‡ï¼šè‡ªåŠ¨é…ç½®TDengineé›†ç¾¤ç¯å¢ƒ
        - çµæ´»å‚æ•°è°ƒä¼˜ï¼šæ”¯æŒvgroupsã€è°ƒè¯•çº§åˆ«ç­‰å‚æ•°è°ƒæ•´
        
Test Scenarios/æµ‹è¯•åœºæ™¯:
    ğŸš€ åŸºç¡€æµè®¡ç®—æµ‹è¯•
        - åˆ›å»ºæµ‹è¯•æ•°æ®å¹¶æ‰§è¡Œå®æ—¶æµè®¡ç®—
        - æŒç»­æ•°æ®å†™å…¥ï¼Œè§‚å¯Ÿæµè®¡ç®—å»¶è¿Ÿå˜åŒ–
        - é€‚ç”¨äºéªŒè¯æµè®¡ç®—åŸºæœ¬åŠŸèƒ½å’Œæ€§èƒ½
        
    ğŸ“ˆ å‹åŠ›æµ‹è¯•
        - å¤§æ•°æ®é‡ä¸‹çš„æµè®¡ç®—æ€§èƒ½æµ‹è¯•
        - å¤šæµå¹¶å‘å¤„ç†èƒ½åŠ›æµ‹è¯•  
        - ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µåˆ†æ
        
    ğŸ” å»¶è¿Ÿæµ‹è¯•
        - æµè®¡ç®—å®æ—¶æ€§éªŒè¯
        - å»¶è¿Ÿåˆ†å¸ƒç»Ÿè®¡å’Œåˆ†æ
        - å»¶è¿Ÿé˜ˆå€¼å‘Šè­¦æµ‹è¯•
        
    ğŸ‹ï¸ å†å²æ•°æ®æµ‹è¯•
        - åŸºäºå·²æœ‰å¤§æ•°æ®é›†çš„æµè®¡ç®—æµ‹è¯•
        - é¿å…é‡å¤æ•°æ®ç”Ÿæˆï¼Œå¿«é€ŸéªŒè¯ä¸åŒæµSQL
        - é€‚ç”¨äºç®—æ³•éªŒè¯å’Œæ€§èƒ½å¯¹æ¯”

Window Types/çª—å£ç±»å‹:
    ğŸ“Š æ»‘åŠ¨çª—å£ (SLIDING WINDOW)
        - å›ºå®šæ—¶é—´é—´éš”çš„æ»‘åŠ¨è®¡ç®—
        - é€‚ç”¨äºè¿ç»­æ•°æ®çš„å¹³æ»‘ç»Ÿè®¡
        
    ğŸ”— ä¼šè¯çª—å£ (SESSION WINDOW)  
        - åŸºäºæ•°æ®é—´éš”çš„åŠ¨æ€çª—å£
        - é€‚ç”¨äºç”¨æˆ·ä¼šè¯åˆ†æ
        
    ğŸ“ è®¡æ•°çª—å£ (COUNT WINDOW)
        - åŸºäºè®°å½•æ•°é‡çš„å›ºå®šçª—å£
        - é€‚ç”¨äºæ‰¹é‡æ•°æ®å¤„ç†
        
    âš¡ äº‹ä»¶çª—å£ (EVENT WINDOW)
        - åŸºäºäº‹ä»¶æ¡ä»¶çš„åŠ¨æ€çª—å£
        - é€‚ç”¨äºå¼‚å¸¸æ£€æµ‹å’Œæ¨¡å¼è¯†åˆ«
        
    ğŸ“ çŠ¶æ€çª—å£ (STATE WINDOW)
        - åŸºäºçŠ¶æ€å˜åŒ–çš„çª—å£åˆ’åˆ†
        - é€‚ç”¨äºçŠ¶æ€æœºåˆ†æ
        
    â° å®šæ—¶è§¦å‘ (PERIOD TRIGGER)
        - å®šæ—¶æ‰§è¡Œçš„è®¡ç®—è§¦å‘
        - é€‚ç”¨äºå®šæœŸæŠ¥è¡¨ç”Ÿæˆ

Query Types/æŸ¥è¯¢ç±»å‹:
    ğŸ“Š èšåˆæŸ¥è¯¢ (AGG): avg(), max(), min() ç­‰ç»Ÿè®¡å‡½æ•°
    ğŸ“‹ æŠ•å½±æŸ¥è¯¢ (SELECT): å­—æ®µé€‰æ‹©å’ŒæŠ•å½±æ“ä½œ

Data Sources/æ•°æ®æº:
    ğŸ¢ è¶…çº§è¡¨ (STB): stream_from.stb - é€‚ç”¨äºå¤§è§„æ¨¡æ•°æ®æŸ¥è¯¢
    ğŸ“„ å­è¡¨ (TB): stream_from.ctb0_X - é€‚ç”¨äºå•è¡¨æ€§èƒ½æµ‹è¯•

Partitioning/åˆ†ç»„æ–¹å¼:
    ğŸ“‚ æ— åˆ†ç»„ (NONE): å…¨è¡¨ç»Ÿè®¡
    ğŸ·ï¸ æŒ‰å­è¡¨å (BY_TBNAME): partition by tbname
    ğŸ”– æŒ‰æ ‡ç­¾ (BY_TAG): partition by tag

Performance Monitoring/æ€§èƒ½ç›‘æ§:
    ğŸ’» ç³»ç»Ÿèµ„æº: CPUä½¿ç”¨ç‡ã€å†…å­˜å ç”¨ã€ç£ç›˜I/O
    â±ï¸ å»¶è¿Ÿç›‘æ§: æºè¡¨ä¸ç›®æ ‡è¡¨çš„æ—¶é—´å·®åˆ†æ
    ğŸ“ˆ å®æ—¶ç»Ÿè®¡: æµè®¡ç®—å¤„ç†é€Ÿåº¦å’Œæ•ˆç‡
    ğŸš¨ æ™ºèƒ½å‘Šè­¦: å»¶è¿Ÿé˜ˆå€¼å‘Šè­¦å’Œæ€§èƒ½å»ºè®®

Requirements/ç³»ç»Ÿè¦æ±‚:
    TDengine: v3.3.7.0+
    Python: 3.7+
    ä¾èµ–åŒ…: taos, psutil, json

Authors/ä½œè€…:
    Guo Xiangyang / éƒ­å‘é˜³

Labels/æ ‡ç­¾: 
    performance, stream, testing

History/å†å²:
    - 2025-08-15 Initial commit
                 é¦–æ¬¡æäº¤

Usage Examples/ä½¿ç”¨ç¤ºä¾‹:

    # å¿«é€Ÿå¼€å§‹ - åŸºç¡€æµè®¡ç®—æµ‹è¯•
    python3 stream_perf_test.py --stream-perf-test-dir /home/stream_perf_test_dir -m 1 --table-count 1000 --time 30
    
    # åˆ›å»ºå¹¶å¤‡ä»½æµ‹è¯•æ•°æ®  
    python3 stream_perf_test.py --stream-perf-test-dir /home/stream_perf_test_dir --create-data --table-count 5000 --histroy-rows 1000
    
    # æ¢å¤æ•°æ®å¹¶æµ‹è¯•ç‰¹å®šæµè®¡ç®—
    python3 stream_perf_test.py --stream-perf-test-dir /home/stream_perf_test_dir -m 3 --sql-type sliding_stb_partition_by_tbname --time 60
    
    # å¤šæµå¹¶å‘æµ‹è¯•
    python3 stream_perf_test.py --stream-perf-test-dir /home/stream_perf_test_dir -m 1 --sql-type sliding_stb --stream-num 100 --time 30
    
    # å»¶è¿Ÿç›‘æ§æµ‹è¯•
    python3 stream_perf_test.py --stream-perf-test-dir /home/stream_perf_test_dir -m 1 --check-stream-delay --delay-check-interval 5 --time 60
    
    # é›†ç¾¤æ¨¡å¼æµ‹è¯•
    python3 stream_perf_test.py --stream-perf-test-dir /home/stream_perf_test_dir -m 1 --deployment-mode cluster --vgroups 20 --time 30
    
    # å‹åŠ›æ§åˆ¶æµ‹è¯•
    python3 stream_perf_test.py --stream-perf-test-dir /home/stream_perf_test_dir -m 1 --real-time-batch-sleep 10 --real-time-batch-rows 500


"""

# ========== å…¨å±€é¢œè‰²å®šä¹‰ ==========
class Colors:
    """ç»ˆç«¯é¢œè‰²å¸¸é‡å®šä¹‰"""
    # åŸºç¡€é¢œè‰²
    RED = '\033[91m'        # çº¢è‰² - é”™è¯¯ã€ä¸¥é‡è­¦å‘Š
    GREEN = '\033[92m'      # ç»¿è‰² - æˆåŠŸã€æ­£å¸¸çŠ¶æ€
    YELLOW = '\033[93m'     # é»„è‰² - è­¦å‘Šã€æ³¨æ„
    BLUE = '\033[94m'       # è“è‰² - ä¿¡æ¯ã€è¯¦æƒ…
    PURPLE = '\033[95m'     # ç´«è‰² - æ ‡é¢˜ã€é‡è¦ä¿¡æ¯
    CYAN = '\033[96m'       # é’è‰² - æ—¶é—´æˆ³ã€æ•°æ®
    WHITE = '\033[97m'      # ç™½è‰² - æ™®é€šæ–‡æœ¬
    
    # ç‰¹æ®Šæ ¼å¼
    BOLD = '\033[1m'        # ç²—ä½“
    UNDERLINE = '\033[4m'   # ä¸‹åˆ’çº¿
    BLINK = '\033[5m'       # é—ªçƒ
    REVERSE = '\033[7m'     # åè‰²
    
    # èƒŒæ™¯è‰²
    BG_RED = '\033[41m'     # çº¢è‰²èƒŒæ™¯
    BG_GREEN = '\033[42m'   # ç»¿è‰²èƒŒæ™¯
    BG_YELLOW = '\033[43m'  # é»„è‰²èƒŒæ™¯
    BG_BLUE = '\033[44m'    # è“è‰²èƒŒæ™¯
    
    # ç»“æŸç¬¦
    END = '\033[0m'         # é‡ç½®æ‰€æœ‰æ ¼å¼
    
    @staticmethod
    def supports_color():
        """æ£€æµ‹ç»ˆç«¯æ˜¯å¦æ”¯æŒé¢œè‰²è¾“å‡º"""
        return (
            hasattr(os.sys.stdout, 'isatty') and os.sys.stdout.isatty() and
            os.environ.get('TERM') != 'dumb' and
            os.environ.get('NO_COLOR') is None
        )
    
    @staticmethod
    def get_colors():
        """è·å–é¢œè‰²å¯¹è±¡ï¼Œå¦‚æœä¸æ”¯æŒé¢œè‰²åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²"""
        if Colors.supports_color():
            return Colors
        else:
            # åˆ›å»ºä¸€ä¸ªæ‰€æœ‰é¢œè‰²éƒ½ä¸ºç©ºå­—ç¬¦ä¸²çš„ç±»
            class NoColors:
                RED = GREEN = YELLOW = BLUE = PURPLE = CYAN = WHITE = ''
                BOLD = UNDERLINE = BLINK = REVERSE = ''
                BG_RED = BG_GREEN = BG_YELLOW = BG_BLUE = ''
                END = ''
            return NoColors


# ========== é¢œè‰²æ‰“å°è¾…åŠ©å‡½æ•° ==========
def print_success(message):
    """æ‰“å°æˆåŠŸæ¶ˆæ¯"""
    c = Colors.get_colors()
    print(f"{c.GREEN}âœ“ {message}{c.END}")

def print_warning(message):
    """æ‰“å°è­¦å‘Šæ¶ˆæ¯"""
    c = Colors.get_colors()
    print(f"{c.YELLOW}âš  {message}{c.END}")

def print_error(message):
    """æ‰“å°é”™è¯¯æ¶ˆæ¯"""
    c = Colors.get_colors()
    print(f"{c.RED}âœ— {message}{c.END}")

def print_info(message):
    """æ‰“å°ä¿¡æ¯æ¶ˆæ¯"""
    c = Colors.get_colors()
    print(f"{c.BLUE}â„¹ {message}{c.END}")

def print_title(message):
    """æ‰“å°æ ‡é¢˜æ¶ˆæ¯"""
    c = Colors.get_colors()
    print(f"{c.BOLD}{c.PURPLE}{message}{c.END}")

def print_subtitle(message):
    """æ‰“å°å‰¯æ ‡é¢˜æ¶ˆæ¯"""
    c = Colors.get_colors()
    print(f"{c.CYAN}{message}{c.END}")

def print_timestamp(message):
    """æ‰“å°å¸¦æ—¶é—´æˆ³çš„æ¶ˆæ¯"""
    c = Colors.get_colors()
    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
    print(f"{c.CYAN}[{timestamp}]{c.END} {message}")

def print_separator(char='-', length=80, color=None):
    """æ‰“å°åˆ†éš”çº¿"""
    c = Colors.get_colors()
    if color:
        print(f"{getattr(c, color.upper(), '')}{char * length}{c.END}")
    else:
        print(char * length)
        
        
class MonitorSystemLoad:

    def __init__(self, name_pattern, count, perf_file='/tmp/perf.log', use_signal=True, interval=1, deployment_mode='cluster') -> None:
        """åˆå§‹åŒ–ç³»ç»Ÿè´Ÿè½½ç›‘æ§
        
        Args:
            name_pattern: è¿›ç¨‹åæ¨¡å¼,ä¾‹å¦‚ 'taosd.*dnode1/conf'
            count: ç›‘æ§æ¬¡æ•°
            perf_file: æ€§èƒ½æ•°æ®è¾“å‡ºæ–‡ä»¶
            interval: æ€§èƒ½é‡‡é›†é—´éš”(ç§’),é»˜è®¤1ç§’
            deployment_mode: éƒ¨ç½²æ¨¡å¼ 'single' æˆ– 'cluster'
        """
        self.name_pattern = name_pattern  # ä¿å­˜è¿›ç¨‹åæ¨¡å¼
        self.count = count
        self.perf_file = perf_file
        self.interval = interval
        self.deployment_mode = deployment_mode
        
        # æ ¹æ®éƒ¨ç½²æ¨¡å¼ç¡®å®šéœ€è¦ç›‘æ§çš„èŠ‚ç‚¹
        if deployment_mode == 'single':
            self.monitor_nodes = ['dnode1']
            print(f"å•èŠ‚ç‚¹æ¨¡å¼: ä»…ç›‘æ§ dnode1")
        else:
            self.monitor_nodes = ['dnode1', 'dnode2', 'dnode3']
            print(f"é›†ç¾¤æ¨¡å¼: ç›‘æ§ dnode1, dnode2, dnode3")
        
        # ä¸ºæ¯ä¸ªdnodeåˆ›å»ºå¯¹åº”çš„æ€§èƒ½æ–‡ä»¶å¥æŸ„
        self.perf_files = {}
        for dnode in self.monitor_nodes:
            file_path = f"{os.path.splitext(perf_file)[0]}-{dnode}.log"
            try:
                self.perf_files[dnode] = open(file_path, 'w+')
                print(f"åˆ›å»ºæ€§èƒ½æ—¥å¿—æ–‡ä»¶: {file_path}")
            except Exception as e:
                print(f"åˆ›å»ºæ—¥å¿—æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
                
        # åˆ›å»ºæ±‡æ€»æ—¥å¿—æ–‡ä»¶
        try:
            all_file = f"{os.path.splitext(perf_file)[0]}-all.log"
            self.perf_files['all'] = open(all_file, 'w+')
            print(f"åˆ›å»ºæ±‡æ€»æ—¥å¿—æ–‡ä»¶: {all_file}")
        except Exception as e:
            print(f"åˆ›å»ºæ±‡æ€»æ—¥å¿—æ–‡ä»¶å¤±è´¥: {str(e)}")
            
        # è·å–è¿›ç¨‹ID
        self.pids = self.get_pids_by_pattern()
        self.processes = {
            dnode: psutil.Process(pid) if pid else None
            for dnode, pid in self.pids.items()
        }
        for process in self.processes.values():
            if process:
                process.cpu_percent()
        self.stop_monitoring = False
        self._should_stop = False
        if use_signal and threading.current_thread() is threading.main_thread():
            signal.signal(signal.SIGINT, self.signal_handler)
            signal.signal(signal.SIGTERM, self.signal_handler)

    def __del__(self):
        """ç¡®ä¿æ‰€æœ‰æ–‡ä»¶éƒ½è¢«æ­£ç¡®å…³é—­"""
        for f in self.perf_files.values():
            try:
                f.close()
            except:
                pass
            
    def stop(self):
        """æä¾›å¤–éƒ¨åœæ­¢ç›‘æ§çš„æ–¹æ³•"""
        self.stop_monitoring = True
        self._should_stop = True
        print("\nåœæ­¢æ€§èƒ½ç›‘æ§...")
        # ç­‰å¾…æ‰€æœ‰æ–‡ä»¶å†™å…¥å®Œæˆ
        for f in self.perf_files.values():
            try:
                f.flush()
            except:
                pass
            
    def signal_handler(self, signum, frame):
        """å¤„ç†ä¸­æ–­ä¿¡å·"""
        print("\næ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢ç›‘æ§...")
        self.stop_monitoring = True
        time.sleep(1)
        # å…³é—­æ‰€æœ‰æ–‡ä»¶
        for f in self.perf_files.values():
            try:
                f.close()
            except:
                pass
        # é€€å‡ºç›‘æ§ä½†ä¸å½±å“taosdè¿›ç¨‹
        print("\nç›‘æ§å·²åœæ­¢ï¼Œtaosdè¿›ç¨‹ç»§ç»­è¿è¡Œ")
        os._exit(0)
            
    def get_pid_by_name(self, name):
        for proc in psutil.process_iter(['pid', 'name']):
            if proc.info['name'] == name:
                return proc.info['pid']
        return None
    
    def write_metrics(self, dnode, status, timestamp=None):
        """å†™å…¥æ€§èƒ½æŒ‡æ ‡
        
        Args:
            dnode: èŠ‚ç‚¹åç§°
            status: æ€§èƒ½æ•°æ®
            timestamp: æ—¶é—´æˆ³(å¯é€‰)
        """
        # å†™å…¥å•ä¸ªèŠ‚ç‚¹çš„æ—¥å¿—æ–‡ä»¶
        self.perf_files[dnode].write(status + '\n')
        
        # åŒæ—¶å†™å…¥æ±‡æ€»æ—¥å¿—æ–‡ä»¶
        self.perf_files['all'].write(status + '\n')
        
        # è¾“å‡ºåˆ°æ§åˆ¶å°
        print(status)

    def get_pids_by_pattern(self):
        """æ ¹æ®è¿›ç¨‹åæ¨¡å¼è·å–æ‰€æœ‰åŒ¹é…çš„è¿›ç¨‹ID"""
        pids = {}
        # ä½¿ç”¨ ps å‘½ä»¤è·å–è¯¦ç»†è¿›ç¨‹ä¿¡æ¯
        result = subprocess.run(
            'ps -ef | grep taosd | grep -v grep', 
            shell=True, 
            capture_output=True, 
            text=True
        )
        
        for line in result.stdout.splitlines():
            if self.name_pattern in line:
                parts = line.split()
                pid = int(parts[1])
                # ä»é…ç½®æ–‡ä»¶è·¯å¾„ä¸­æå–dnodeä¿¡æ¯
                cfg_path = next((p for p in parts if '/conf/taos.cfg' in p), None)
                if cfg_path:
                    # ä»è·¯å¾„ä¸­æå–dnodeåç§°
                    dnode = next((part for part in cfg_path.split('/') if part.startswith('dnode')), None)
                    if dnode and dnode in self.monitor_nodes:
                        pids[dnode] = pid
                        print(f"æ‰¾åˆ° {dnode} è¿›ç¨‹, PID: {pid}, é…ç½®æ–‡ä»¶: {cfg_path}")
        
        if not pids:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ°åŒ¹é…æ¨¡å¼ '{self.name_pattern}' çš„è¿›ç¨‹")
        else:
            print(f"å…±æ‰¾åˆ° {len(pids)} ä¸ªtaosdè¿›ç¨‹")
        
        # æ— è®ºæ˜¯å¦æ‰¾åˆ°è¿›ç¨‹,éƒ½åˆå§‹åŒ–æ‰€æœ‰dnodeçš„æ–‡ä»¶å¥æŸ„
        for dnode in self.monitor_nodes:
            if dnode not in self.perf_files:
                file_path = f"{os.path.splitext(self.perf_file)[0]}-{dnode}.log"
                try:
                    self.perf_files[dnode] = open(file_path, 'w+')
                    print(f"åˆ›å»ºæ€§èƒ½æ—¥å¿—æ–‡ä»¶: {file_path}")
                except Exception as e:
                    print(f"åˆ›å»ºæ—¥å¿—æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
        
        return pids
    
    def write_zero_metrics(self, dnode, timestamp):
        """å†™å…¥é›¶å€¼æŒ‡æ ‡
        
        Args:
            dnode: èŠ‚ç‚¹åç§°
            timestamp: æ—¶é—´æˆ³
        """
        status = (
            f"{timestamp} [{dnode}] "
            f"CPU: 0.0%, "
            f"Memory: 0.00MB (0.00%), "
            f"Read: 0.00MB (0), "
            f"Write: 0.00MB (0)"
        )
        self.perf_files[dnode].write(status + '\n')
        print(status)
        
    def get_proc_status_old(self):
        """ç›‘æ§æ‰€æœ‰åŒ¹é…è¿›ç¨‹çš„çŠ¶æ€"""
        try:
            processes = {
                dnode: psutil.Process(pid) if pid else None
                for dnode, pid in self.pids.items()
            }
            
            while self.count > 0 and not self.stop_monitoring:
                try:
                    sys_load = psutil.getloadavg()
                    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
                    
                    # è®°å½•ç³»ç»Ÿæ•´ä½“è´Ÿè½½
                    load_info = f"\n{timestamp} System Load: {sys_load[0]:.2f}\n"
                    for dnode in self.perf_files.keys():
                        self.perf_files[dnode].write(load_info)
                    print(load_info)
                    
                    # è®°å½•æ¯ä¸ªèŠ‚ç‚¹çš„çŠ¶æ€
                    for dnode in ['dnode1', 'dnode2', 'dnode3']:
                        process = processes.get(dnode)
                        try:
                            if process and process.is_running():
                                cpu_percent = process.cpu_percent(interval=self.interval)
                                memory_info = process.memory_info()
                                memory_percent = process.memory_percent()
                                io_counters = process.io_counters()

                                status = (
                                    f"{timestamp} [{dnode}] "
                                    f"CPU: {cpu_percent:.1f}%, "
                                    f"Memory: {memory_info.rss/1048576.0:.2f}MB ({memory_percent:.2f}%), "
                                    f"Read: {io_counters.read_bytes/1048576.0:.2f}MB ({io_counters.read_count}), "
                                    f"Write: {io_counters.write_bytes/1048576.0:.2f}MB ({io_counters.write_count})"
                                )
                                self.write_metrics(dnode, status)
                            else:
                                # è¿›ç¨‹ä¸å­˜åœ¨æ—¶å†™å…¥é›¶å€¼
                                self.write_zero_metrics(dnode, timestamp)
                                
                        except (psutil.NoSuchProcess, psutil.AccessDenied):
                            # è¿›ç¨‹å·²ç»ˆæ­¢æˆ–æ— æ³•è®¿é—®æ—¶å†™å…¥é›¶å€¼
                            self.write_zero_metrics(dnode, timestamp)
                            processes[dnode] = None
                        except Exception as e:
                            print(f"ç›‘æ§ {dnode} å‡ºé”™: {str(e)}")
                            self.write_zero_metrics(dnode, timestamp)
                    
                    # æ·»åŠ åˆ†éš”çº¿
                    separator = "-" * 80 + "\n"
                    for f in self.perf_files.values():
                        f.write(separator)
                        f.flush()
                    print(separator.strip())
                        
                    time.sleep(self.interval)
                    self.count -= 1
                    
                    if self.stop_monitoring:
                        print("æ­£åœ¨å®Œæˆæœ€åçš„ç›‘æ§è®°å½•...")
                        break
                    
                except Exception as e:
                    print(f"ç›‘æ§å‡ºé”™: {str(e)}")
                    if not self.stop_monitoring:  # åªæœ‰åœ¨éä¸»åŠ¨åœæ­¢æ—¶æ‰è·³å‡º
                            break
                
            print("\nç›‘æ§å·²åœæ­¢")
        
        finally:
            # å…³é—­æ‰€æœ‰æ–‡ä»¶
            for f in self.perf_files.values():
                try:
                    f.close()
                except:
                    pass

    def get_proc_status(self):
        """ç›‘æ§æ‰€æœ‰åŒ¹é…è¿›ç¨‹çš„çŠ¶æ€"""
        try:
            while self.count > 0 and not self.stop_monitoring and not self._should_stop:
                start_time = time.time()
                
                sys_load = psutil.getloadavg()
                timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
                
                # è®°å½•ç³»ç»Ÿè´Ÿè½½
                load_info = f"\n{timestamp} System Load: {sys_load[0]:.2f}\n"
                for f in self.perf_files.values():
                    f.write(load_info)
                print(load_info)
                
                # æ”¶é›†è¿›ç¨‹æŒ‡æ ‡
                for dnode in self.monitor_nodes:
                    process = self.processes.get(dnode)
                    try:
                        if process and process.is_running():
                            # ç›´æ¥è·å–CPUä½¿ç”¨ç‡ï¼Œä¸ä½¿ç”¨intervalå‚æ•°
                            cpu_percent = process.cpu_percent()
                            memory_info = process.memory_info()
                            memory_percent = process.memory_percent()
                            io_counters = process.io_counters()

                            status = (
                                f"{timestamp} [{dnode}] "
                                f"CPU: {cpu_percent:.1f}%, "
                                f"Memory: {memory_info.rss/1048576.0:.2f}MB ({memory_percent:.2f}%), "
                                f"Read: {io_counters.read_bytes/1048576.0:.2f}MB ({io_counters.read_count}), "
                                f"Write: {io_counters.write_bytes/1048576.0:.2f}MB ({io_counters.write_count})"
                            )
                            self.write_metrics(dnode, status)
                        else:
                            if dnode in self.pids or self.deployment_mode == 'cluster':
                                self.write_zero_metrics(dnode, timestamp)
                            
                    except (psutil.NoSuchProcess, psutil.AccessDenied):
                        if dnode in self.pids or self.deployment_mode == 'cluster':
                            self.write_zero_metrics(dnode, timestamp)
                        self.processes[dnode] = None
                    except Exception as e:
                        print(f"ç›‘æ§ {dnode} å‡ºé”™: {str(e)}")
                        if dnode in self.pids or self.deployment_mode == 'cluster':
                            self.write_zero_metrics(dnode, timestamp)
                
                # æ·»åŠ åˆ†éš”çº¿
                separator = "-" * 80 + "\n"
                for f in self.perf_files.values():
                    f.write(separator)
                    f.flush()
                print(separator.strip())
                
                # ç²¾ç¡®æ§åˆ¶é—´éš”æ—¶é—´
                elapsed = time.time() - start_time
                if elapsed < self.interval:
                    time.sleep(self.interval - elapsed)
                
                self.count -= 1
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
                if self.stop_monitoring or self._should_stop:
                    print("ç›‘æ§æå‰ç»“æŸ")
                    break
                
                # ä½¿ç”¨åˆ†æ®µsleepï¼Œä»¥ä¾¿èƒ½å¤ŸåŠæ—¶å“åº”åœæ­¢ä¿¡å·
                elapsed = time.time() - start_time
                remaining_time = self.interval - elapsed
                
                if remaining_time > 0:
                    # å°†å¤§çš„é—´éš”åˆ†è§£ä¸ºå°çš„ç‰‡æ®µï¼Œä»¥ä¾¿åŠæ—¶å“åº”åœæ­¢ä¿¡å·
                    sleep_chunks = max(1, int(remaining_time))  # æ¯æ¬¡æœ€å¤šsleep 1ç§’
                    chunk_size = remaining_time / sleep_chunks
                    
                    for _ in range(sleep_chunks):
                        if self.stop_monitoring or self._should_stop:
                            print("åœ¨sleepæœŸé—´æ”¶åˆ°åœæ­¢ä¿¡å·")
                            break
                        time.sleep(chunk_size)
                
        except Exception as e:
            print(f"ç›‘æ§å‡ºé”™: {str(e)}")  
        finally:
            print("ç›‘æ§çº¿ç¨‹æ­£åœ¨æ¸…ç†èµ„æº...")
            # å…³é—­æ‰€æœ‰æ–‡ä»¶
            for f in self.perf_files.values():
                try:
                    f.close()
                except:
                    pass
            print("ç›‘æ§çº¿ç¨‹å·²ç»“æŸ")              

def do_monitor(runtime, perf_file, deployment_mode='cluster'):
    """ç›‘æ§çº¿ç¨‹å‡½æ•°"""
    try:
        # ä¸åœ¨å­çº¿ç¨‹ä¸­ä½¿ç”¨ä¿¡å·å¤„ç†
        loader = MonitorSystemLoad(
            'taosd -c', 
            runtime, 
            perf_file, 
            use_signal=False,
            deployment_mode=deployment_mode
        )
        loader.get_proc_status()
    except Exception as e:
        print(f"ç›‘æ§çº¿ç¨‹å‡ºé”™: {str(e)}")
    finally:
        print("ç›‘æ§çº¿ç¨‹ç»“æŸ")

def get_table_list(cursor):
    cursor.execute('use stream_from')

    sql = "select table_name from information_schema.ins_tables where db_name = 'stream_from' and stable_name='stb' order by table_name"
    cursor.execute(sql)

    res = cursor.fetchall()
    return res

def do_multi_insert(index, total, host, user, passwd, conf, tz):
    conn = taos.connect(
        host=host, user=user, password=passwd, config=conf, timezone=tz
    )

    cursor = conn.cursor()
    cursor.execute('use stream_from')

    start_ts = 1609430400000
    step = 5

    cursor.execute("create stable if not exists stb_result(wstart timestamp, minx float, maxx float, countx bigint) tags(gid bigint unsigned)")

    list = get_table_list(cursor)

    list = list[index*total: (index+1)*total]

    print("there are %d tables" % len(list))

    for index, n in enumerate(list):
        cursor.execute(f"create table if not exists {n[0]}_1 using stb_result tags(1)")
        count = 1
        while True:
            sql = (f"select cast({start_ts + step * 1000 * (count - 1)} as timestamp), min(c1), max(c2), count(c3) from stream_from.{n[0]} "
                   f"where ts >= {start_ts + step * 1000 * (count - 1)} and ts < {start_ts + step * 1000 * count}")
            cursor.execute(sql)

            res = cursor.fetchall()
            if res[0][3] == 0:
                break

            insert = f"insert into {n[0]}_1 values ({start_ts + step * 1000 * (count - 1)}, {res[0][1]}, {res[0][2]}, {res[0][3]})"
            cursor.execute(insert)
            count += 1
    conn.close()


'''
class StreamSQLTemplates_bak_for_phase1:
    """æµè®¡ç®— SQL æ¨¡æ¿é›†åˆ"""
    
    s2_2 = """
    create stream s2_2 trigger at_once 
        ignore expired 0 ignore update 0 into stream_to.stb2_2 
        as select _wstart as wstart,
        avg(c0), avg(c1),avg(c2), avg(c3),
        max(c0), max(c1), max(c2), max(c3),
        min(c0), min(c1), min(c2), min(c3)
        from stream_from.stb partition by tbname interval(15s);
    """
    
    s2_3 = """
    create stream s2_3 trigger window_close 
        ignore expired 0 ignore update 0 into stream_to.stb2_3 
        as select _wstart as wstart,
        avg(c0), avg(c1),avg(c2), avg(c3),
        max(c0), max(c1), max(c2), max(c3),
        min(c0), min(c1), min(c2), min(c3)
        from stream_from.stb partition by tbname interval(15s);
    """
    
    s2_4 = """
    create stream s2_4 trigger max_delay 5s 
        ignore expired 0 ignore update 0 into stream_to.stb2_4 
        as select _wstart as wstart,
        avg(c0), avg(c1),avg(c2), avg(c3),
        max(c0), max(c1), max(c2), max(c3),
        min(c0), min(c1), min(c2), min(c3)
        from stream_from.stb partition by tbname interval(15s);
    """
    
    s2_5 = """
    create stream s2_5 trigger FORCE_WINDOW_CLOSE into stream_to.stb2_5 
        as select _wstart as wstart,
        avg(c0), avg(c1),avg(c2), avg(c3),
        max(c0), max(c1), max(c2), max(c3),
        min(c0), min(c1), min(c2), min(c3)
        from stream_from.stb partition by tbname interval(15s);
    """
    
    s2_6 = """
    create stream s2_6 trigger CONTINUOUS_WINDOW_CLOSE 
        ignore expired 0 ignore update 0 into stream_to.stb2_6 
        as select _wstart as wstart,
        avg(c0), avg(c1),avg(c2), avg(c3),
        max(c0), max(c1), max(c2), max(c3),
        min(c0), min(c1), min(c2), min(c3)
        from stream_from.stb partition by tbname interval(15s);
    """
    
    s2_7 = """
    create stream stream_from.s2_7 INTERVAL(15s) SLIDING(15s)
            from stream_from.stb 
            partition by tbname 
            into stream_to.stb2_7
            as select _twstart ts, avg(c0), avg(c1), avg(c2), avg(c3),
            max(c0), max(c1), max(c2), max(c3),
            min(c0), min(c1), min(c2), min(c3)
            from %%tbname where ts >= _twstart and ts < _twend;
    """
    
    s2_8 = """
    create stream stream_from.s2_8 INTERVAL(15s) SLIDING(15s)
            from stream_from.stb 
            partition by tbname 
            into stream_to.stb2_8
            as select _twstart ts, avg(c0), avg(c1), avg(c2), avg(c3),
            max(c0), max(c1), max(c2), max(c3),
            min(c0), min(c1), min(c2), min(c3)
            from %%trows ;
    """
    
    s2_9 = """
    create stream stream_from.s2_9 INTERVAL(15s) SLIDING(15s)
            from stream_from.stb partition by tbname 
            STREAM_OPTIONS(MAX_DELAY(5s)) 
            into stream_to.stb2_9
            as select _twstart ts, avg(c0), avg(c1), avg(c2), avg(c3),
            max(c0), max(c1), max(c2), max(c3),
            min(c0), min(c1), min(c2), min(c3)
            from %%tbname where ts >= _twstart and ts < _twend;
    """
    
    s2_10 = """
    create stream stream_from.s2_10 INTERVAL(15s) SLIDING(15s) 
            from stream_from.stb 
            STREAM_OPTIONS(MAX_DELAY(5s))
            into stream_to.stb2_10
            as select _twstart ts, avg(c0), avg(c1), avg(c2), avg(c3),
            max(c0), max(c1), max(c2), max(c3),
            min(c0), min(c1), min(c2), min(c3)
            from %%trows ;
    """
    
    s2_11 = """
    create stream stream_from.s2_11 period(15s) 
            from stream_from.stb partition by tbname  
            into stream_to.stb2_11
            as select cast(_tlocaltime/1000000 as timestamp) ts, avg(c0), avg(c1), avg(c2), avg(c3),
            max(c0), max(c1), max(c2), max(c3),
            min(c0), min(c1), min(c2), min(c3)
            from %%tbname ;
    """    
    
    s2_12 = """
    create stream stream_from.s2_12 session(ts,10a)
            from stream_from.stb partition by tbname 
            into stream_to.stb2_12
            as select _twstart ts, avg(c0), avg(c1), avg(c2), avg(c3),
            max(c0), max(c1), max(c2), max(c3),
            min(c0), min(c1), min(c2), min(c3)
            from %%tbname where ts >= _twstart and ts <= _twend;
    """
    
    s2_13 = """
    create stream stream_from.s2_13 COUNT_WINDOW(1000)
            from stream_from.stb partition by tbname 
            into stream_to.stb2_13
            as select _twstart ts, avg(c0), avg(c1), avg(c2), avg(c3),
            max(c0), max(c1), max(c2), max(c3),
            min(c0), min(c1), min(c2), min(c3)
            from %%tbname where ts >= _twstart and ts <= _twend;
    """
    
    s2_14 = """
    create stream stream_from.s2_14 EVENT_WINDOW(START WITH c0 > -10000000 END WITH c0 < 10000000) 
            from stream_from.stb partition by tbname 
            into stream_to.stb2_14
            as select _twstart ts, avg(c0), avg(c1), avg(c2), avg(c3),
            max(c0), max(c1), max(c2), max(c3),
            min(c0), min(c1), min(c2), min(c3)
            from %%tbname where ts >= _twstart and ts <= _twend;
    """
    
    s2_15 = """
    create stream stream_from.s2_15 STATE_WINDOW(c0) 
            from stream_from.stb partition by tbname 
            into stream_to.stb2_15
            as select _twstart ts, avg(c0), avg(c1), avg(c2), avg(c3),
            max(c0), max(c1), max(c2), max(c3),
            min(c0), min(c1), min(c2), min(c3)
            from %%tbname where ts >= _twstart and ts <= _twend;
    """
    
    @classmethod
    def get_sql(cls, sql_type):
        """
        è·å–æŒ‡å®šç±»å‹çš„ SQL æ¨¡æ¿
        Args:
            sql_type: SQL ç±»å‹æ ‡è¯†ç¬¦ (case_id)
        Returns:
            å¯¹åº”çš„ SQL æ¨¡æ¿
        """
        sql_map = {
            's2_2': cls.s2_2,
            's2_3': cls.s2_3,
            's2_4': cls.s2_4,
            's2_5': cls.s2_5,
            's2_6': cls.s2_6,
            's2_7': cls.s2_7,
            's2_8': cls.s2_8,
            's2_9': cls.s2_9,
            's2_10': cls.s2_10,
            's2_11': cls.s2_11,
            's2_12': cls.s2_12,
            's2_13': cls.s2_13,
            's2_14': cls.s2_14,
            's2_15': cls.s2_15,
        }
        
        # å®šä¹‰æ‰¹é‡ SQL ç»„åˆ
        batch_map = {
            'all': {
                's2_7': cls.s2_7,
                's2_8': cls.s2_8,
                's2_9': cls.s2_9,
                's2_10': cls.s2_10,
                's2_11': cls.s2_11,
                's2_12': cls.s2_12,
                's2_13': cls.s2_13,
                's2_14': cls.s2_14,
                's2_15': cls.s2_15,
            }
        }
        # å¦‚æœè¯·æ±‚æ‰¹é‡ SQL
        if sql_type in batch_map:
            return batch_map[sql_type]
        
        # è¿”å›å•ä¸ª SQLï¼Œé»˜è®¤è¿”å› s2_7
        return sql_map.get(sql_type, cls.s2_7)  
'''


class StreamSQLTemplates:
    """æµè®¡ç®— SQL æ¨¡æ¿é›†åˆ - é‡æ„ç‰ˆæœ¬"""
    
    def __init__(self):
        # å®šä¹‰é»˜è®¤çš„èšåˆå‡½æ•°ç»„åˆ
        self.default_agg_columns = [
                "avg(c0), avg(c1), avg(c2), avg(c3)",
                "max(c0), max(c1), max(c2), max(c3)", 
                "min(c0), min(c1), min(c2), min(c3)"
        ]
        
        # å®šä¹‰é»˜è®¤çš„æŠ•å½±åˆ—
        self.default_select_columns = ["c0", "c1", "c2", "c3"]
        
    def get_select_stream(self, **kwargs):
        """æŸ¥è¯¢æ‰€æœ‰æµä¿¡æ¯"""
        return "select * from information_schema.ins_streams;"
    
    def _build_columns(self, agg_or_select='agg', custom_columns=None):
        """æ„å»ºæŸ¥è¯¢åˆ—
        
        Args:
            agg_or_select: 'agg' è¡¨ç¤ºèšåˆæŸ¥è¯¢, 'select' è¡¨ç¤ºæŠ•å½±æŸ¥è¯¢
            custom_columns: è‡ªå®šä¹‰åˆ—ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨è‡ªå®šä¹‰åˆ—
            
        Returns:
            str: æ„å»ºå¥½çš„åˆ—å­—ç¬¦ä¸²
        """
        if custom_columns:
            if isinstance(custom_columns, list):
                return ", ".join(custom_columns)
            return custom_columns
            
        if agg_or_select == 'agg':
            return ",\n        ".join(self.default_agg_columns)
        else:  
            return ", ".join(self.default_select_columns)
    
    def _build_from_clause(self, tbname_or_trows='tbname'):
        """æ„å»º FROM å­å¥
        
        Args:
            tbname_or_trows: 'tbname' æˆ– 'trows'
            
        Returns:
            str: FROM å­å¥å­—ç¬¦ä¸²
        """
        if tbname_or_trows == 'trows':
            return "%%trows "
        else:
            return "%%tbname where ts >= _twstart and ts <= _twend"
    
    def _build_partition_clause(self, partition_type='none'):
        """æ„å»ºåˆ†åŒºå­å¥
        
        Args:
            partition_type: åˆ†åŒºç±»å‹
                - 'none': ä¸åˆ†ç»„
                - 'tbname': æŒ‰å­è¡¨ååˆ†ç»„  
                - 'tag': æŒ‰tagåˆ†ç»„
                
        Returns:
            str: åˆ†åŒºå­å¥å­—ç¬¦ä¸²
        """
        if partition_type == 'tbname':
            return "partition by tbname"
        elif partition_type == 'tag':
            return "partition by t0"  # å‡è®¾ç¬¬ä¸€ä¸ªtagå­—æ®µåä¸ºt0
        else:  # 'none'
            return ""
    
    def _build_from_source(self, source_type='stb', stream_index=None):
        """æ„å»ºæ•°æ®æº
        
        Args:
            source_type: æ•°æ®æºç±»å‹
                - 'stb': è¶…çº§è¡¨ stream_from.stb
                - 'tb': å­è¡¨ stream_from.ctb0_X
            stream_index: æµçš„ç´¢å¼•ç¼–å·ï¼ˆç”¨äºå¤šæµæ—¶é€‰æ‹©ä¸åŒå­è¡¨ï¼‰
                
        Returns:
            str: æ•°æ®æºå­—ç¬¦ä¸²
        """
        if source_type == 'tb':
            # å¦‚æœæœ‰æµç´¢å¼•ï¼Œä½¿ç”¨å¯¹åº”çš„å­è¡¨ï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤çš„ctb0_0
            if stream_index is not None:
                # å­è¡¨ç¼–å·ä»1å¼€å§‹
                table_index = stream_index 
                #print(f"è°ƒè¯•: ç”Ÿæˆå­è¡¨å stream_from.ctb0_{table_index}")
                return f"stream_from.ctb0_{table_index}"
            else:
                #print(f"è°ƒè¯•: ä½¿ç”¨é»˜è®¤å­è¡¨ stream_from.ctb0_0")
                return "stream_from.ctb0_0"  # é»˜è®¤å­è¡¨
        else:  # 'stb'
            return "stream_from.stb"
    
    def _generate_stream_name(self, base_type, source_type, partition_type):
        """ç”Ÿæˆæµåç§°
        
        Args:
            base_type: åŸºç¡€ç±»å‹ (å¦‚ 's2_7', 'sliding')
            source_type: æ•°æ®æºç±»å‹ ('stb', 'tb') 
            partition_type: åˆ†åŒºç±»å‹ ('none', 'tbname', 'tag')
            
        Returns:
            str: ç”Ÿæˆçš„æµåç§°
        """
        # æ„å»ºåç§°ç»„ä»¶
        source_part = "stb" if source_type == "stb" else "tb"
        
        if partition_type == 'none':
            partition_part = ""
        elif partition_type == 'tbname':
            partition_part = "_tbname" 
        elif partition_type == 'tag':
            partition_part = "_tag"
        else:
            partition_part = ""
            
        return f"stream_from.{base_type}_{source_part}{partition_part}"
    
    def _generate_target_table(self, base_type, source_type, partition_type):
        """ç”Ÿæˆç›®æ ‡è¡¨åç§°"""
        source_part = "stb" if source_type == "stb" else "tb"
        
        if partition_type == 'none':
            partition_part = ""
        elif partition_type == 'tbname':
            partition_part = "_tbname"
        elif partition_type == 'tag':
            partition_part = "_tag"
        else:
            partition_part = ""
            
        return f"stream_to.{base_type}_{source_part}{partition_part}"
    
        
    # ========== é€šç”¨æ¨¡æ¿ç”Ÿæˆæ–¹æ³• ==========
    def get_sliding_template(self, source_type='stb', partition_type='tbname', 
                           agg_or_select='agg', tbname_or_trows='tbname', custom_columns=None, stream_index=None):
        """ç”Ÿæˆæ»‘åŠ¨çª—å£æ¨¡æ¿
        
        Args:
            source_type: 'stb'(è¶…çº§è¡¨) æˆ– 'tb'(å­è¡¨)
            partition_type: 'none'(ä¸åˆ†ç»„), 'tbname'(æŒ‰å­è¡¨å), 'tag'(æŒ‰tag)
            agg_or_select: 'agg' æˆ– 'select'
            tbname_or_trows: 'tbname' æˆ– 'trows'
            custom_columns: è‡ªå®šä¹‰åˆ—
        """
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows)
        partition_clause = self._build_partition_clause(partition_type)
        from_source = self._build_from_source(source_type, stream_index) 
        stream_name = self._generate_stream_name('sliding', source_type, partition_type)
        target_table = self._generate_target_table('sliding', source_type, partition_type)
        
        # å¦‚æœæœ‰æµç´¢å¼•ï¼Œåœ¨æµåç§°å’Œç›®æ ‡è¡¨ä¸­æ·»åŠ ç´¢å¼•
        if stream_index is not None:
            stream_name += f"_{stream_index}"
            target_table += f"_{stream_index}"
        
        # æ„å»ºå®Œæ•´çš„SQL
        partition_line = f"\n            {partition_clause}" if partition_clause else ""
        
        return f"""
    create stream {stream_name} INTERVAL(15s) SLIDING(15s)
            from {from_source}{partition_line}
            into {target_table}
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_session_template(self, source_type='stb', partition_type='tbname',
                           agg_or_select='agg', tbname_or_trows='tbname', custom_columns=None, stream_index=None):
        """ç”Ÿæˆä¼šè¯çª—å£æ¨¡æ¿"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows)
        partition_clause = self._build_partition_clause(partition_type)
        from_source = self._build_from_source(source_type, stream_index) 
        stream_name = self._generate_stream_name('session', source_type, partition_type)
        target_table = self._generate_target_table('session', source_type, partition_type)
        
        # å¦‚æœæœ‰æµç´¢å¼•ï¼Œåœ¨æµåç§°å’Œç›®æ ‡è¡¨ä¸­æ·»åŠ ç´¢å¼•
        if stream_index is not None:
            stream_name += f"_{stream_index}"
            target_table += f"_{stream_index}"
        
        partition_line = f"\n            {partition_clause}" if partition_clause else ""
        
        return f"""
    create stream {stream_name} session(ts,10a)
            from {from_source}{partition_line}
            into {target_table}
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_count_template(self, source_type='stb', partition_type='tbname',
                         agg_or_select='agg', tbname_or_trows='tbname', custom_columns=None, stream_index=None):
        """ç”Ÿæˆè®¡æ•°çª—å£æ¨¡æ¿"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows)
        partition_clause = self._build_partition_clause(partition_type)
        from_source = self._build_from_source(source_type, stream_index) 
        stream_name = self._generate_stream_name('count', source_type, partition_type)
        target_table = self._generate_target_table('count', source_type, partition_type)
        
        # å¦‚æœæœ‰æµç´¢å¼•ï¼Œåœ¨æµåç§°å’Œç›®æ ‡è¡¨ä¸­æ·»åŠ ç´¢å¼•
        if stream_index is not None:
            stream_name += f"_{stream_index}"
            target_table += f"_{stream_index}"
        
        partition_line = f"\n            {partition_clause}" if partition_clause else ""
        
        return f"""
    create stream {stream_name} COUNT_WINDOW(1000)
            from {from_source}{partition_line}
            into {target_table}
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_event_template(self, source_type='stb', partition_type='tbname',
                         agg_or_select='agg', tbname_or_trows='tbname', custom_columns=None, stream_index=None):
        """ç”Ÿæˆäº‹ä»¶çª—å£æ¨¡æ¿"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows)
        partition_clause = self._build_partition_clause(partition_type)
        from_source = self._build_from_source(source_type, stream_index) 
        stream_name = self._generate_stream_name('event', source_type, partition_type)
        target_table = self._generate_target_table('event', source_type, partition_type)
        
        # å¦‚æœæœ‰æµç´¢å¼•ï¼Œåœ¨æµåç§°å’Œç›®æ ‡è¡¨ä¸­æ·»åŠ ç´¢å¼•
        if stream_index is not None:
            stream_name += f"_{stream_index}"
            target_table += f"_{stream_index}"
        
        partition_line = f"\n            {partition_clause}" if partition_clause else ""
        
        return f"""
    create stream {stream_name} EVENT_WINDOW(START WITH c0 > -10000000 END WITH c0 < 10000000)
            from {from_source}{partition_line}
            into {target_table}
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_state_template(self, source_type='stb', partition_type='tbname',
                         agg_or_select='agg', tbname_or_trows='tbname', custom_columns=None, stream_index=None):
        """ç”ŸæˆçŠ¶æ€çª—å£æ¨¡æ¿"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows)
        partition_clause = self._build_partition_clause(partition_type)
        from_source = self._build_from_source(source_type, stream_index) 
        stream_name = self._generate_stream_name('state', source_type, partition_type)
        target_table = self._generate_target_table('state', source_type, partition_type)
        
        # å¦‚æœæœ‰æµç´¢å¼•ï¼Œåœ¨æµåç§°å’Œç›®æ ‡è¡¨ä¸­æ·»åŠ ç´¢å¼•
        if stream_index is not None:
            stream_name += f"_{stream_index}"
            target_table += f"_{stream_index}"
        
        partition_line = f"\n            {partition_clause}" if partition_clause else ""
        
        return f"""
    create stream {stream_name} STATE_WINDOW(c0)
            from {from_source}{partition_line}
            into {target_table}
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_period_template(self, source_type='stb', partition_type='tbname',
                          agg_or_select='agg', tbname_or_trows='tbname', custom_columns=None, stream_index=None):
        """ç”Ÿæˆå®šæ—¶è§¦å‘æ¨¡æ¿ (periodä¸æ”¯æŒtbname_or_trows)"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows)
        partition_clause = self._build_partition_clause(partition_type)
        from_source = self._build_from_source(source_type, stream_index) 
        stream_name = self._generate_stream_name('period', source_type, partition_type)
        target_table = self._generate_target_table('period', source_type, partition_type)
        
        # å¦‚æœæœ‰æµç´¢å¼•ï¼Œåœ¨æµåç§°å’Œç›®æ ‡è¡¨ä¸­æ·»åŠ ç´¢å¼•
        if stream_index is not None:
            stream_name += f"_{stream_index}"
            target_table += f"_{stream_index}"
        
        partition_line = f"\n            {partition_clause}" if partition_clause else ""
        
        return f"""
    create stream {stream_name} period(15s)
            from {from_source}{partition_line}
            into {target_table}
            as select cast(_tlocaltime/1000000 as timestamp) ts, {columns}
            from {from_clause};
    """
    
    # ========== ç»„åˆç”Ÿæˆæ–¹æ³• ==========
    def get_sliding_group_detailed(self, **kwargs):
        """è·å–è¯¦ç»†çš„æ»‘åŠ¨çª—å£ç»„åˆ (4ç§ç»„åˆ)"""
        return {
            'sliding_stb': self.get_sliding_template(source_type='stb', partition_type='none', **kwargs),
            'sliding_stb_partition_by_tbname': self.get_sliding_template(source_type='stb', partition_type='tbname', **kwargs),
            'sliding_stb_partition_by_tag': self.get_sliding_template(source_type='stb', partition_type='tag', **kwargs),
            'sliding_tb': self.get_sliding_template(source_type='tb', partition_type='none', **kwargs)
        }
    
    def get_session_group_detailed(self, **kwargs):
        """è·å–è¯¦ç»†çš„ä¼šè¯çª—å£ç»„åˆ"""
        return {
            'session_stb': self.get_session_template(source_type='stb', partition_type='none', **kwargs),
            'session_stb_partition_by_tbname': self.get_session_template(source_type='stb', partition_type='tbname', **kwargs),
            'session_stb_partition_by_tag': self.get_session_template(source_type='stb', partition_type='tag', **kwargs),
            'session_tb': self.get_session_template(source_type='tb', partition_type='none', **kwargs)
        }
    
    def get_count_group_detailed(self, **kwargs):
        """è·å–è¯¦ç»†çš„è®¡æ•°çª—å£ç»„åˆ"""
        return {
            'count_stb': self.get_count_template(source_type='stb', partition_type='none', **kwargs),
            'count_stb_partition_by_tbname': self.get_count_template(source_type='stb', partition_type='tbname', **kwargs),
            'count_stb_partition_by_tag': self.get_count_template(source_type='stb', partition_type='tag', **kwargs),
            'count_tb': self.get_count_template(source_type='tb', partition_type='none', **kwargs)
        }
    
    def get_event_group_detailed(self, **kwargs):
        """è·å–è¯¦ç»†çš„äº‹ä»¶çª—å£ç»„åˆ"""
        return {
            'event_stb': self.get_event_template(source_type='stb', partition_type='none', **kwargs),
            'event_stb_partition_by_tbname': self.get_event_template(source_type='stb', partition_type='tbname', **kwargs),
            'event_stb_partition_by_tag': self.get_event_template(source_type='stb', partition_type='tag', **kwargs),
            'event_tb': self.get_event_template(source_type='tb', partition_type='none', **kwargs)
        }
    
    def get_state_group_detailed(self, **kwargs):
        """è·å–è¯¦ç»†çš„çŠ¶æ€çª—å£ç»„åˆ"""
        return {
            'state_stb': self.get_state_template(source_type='stb', partition_type='none', **kwargs),
            'state_stb_partition_by_tbname': self.get_state_template(source_type='stb', partition_type='tbname', **kwargs),
            'state_stb_partition_by_tag': self.get_state_template(source_type='stb', partition_type='tag', **kwargs),
            'state_tb': self.get_state_template(source_type='tb', partition_type='none', **kwargs)
        }
    
    def get_period_group_detailed(self, **kwargs):
        """è·å–è¯¦ç»†çš„å®šæ—¶è§¦å‘ç»„åˆ"""
        # period ä¸æ”¯æŒ tbname_or_trows å‚æ•°
        filtered_kwargs = {k: v for k, v in kwargs.items() if k != 'tbname_or_trows'}
        return {
            'period_stb': self.get_period_template(source_type='stb', partition_type='none', **filtered_kwargs),
            'period_stb_partition_by_tbname': self.get_period_template(source_type='stb', partition_type='tbname', **filtered_kwargs),
            'period_stb_partition_by_tag': self.get_period_template(source_type='stb', partition_type='tag', **filtered_kwargs),
            'period_tb': self.get_period_template(source_type='tb', partition_type='none', **filtered_kwargs)
        }
        
    def get_tbname_agg_group(self, **kwargs):
        """è·å– tbname + agg ç»„åˆçš„æ‰€æœ‰çª—å£ç±»å‹
        
        å›ºå®šå‚æ•°: tbname_or_trows='tbname', agg_or_select='agg'
        å¿½ç•¥å‘½ä»¤è¡Œä¼ å…¥çš„è¿™ä¸¤ä¸ªå‚æ•°
        """
        # å›ºå®šå‚æ•°ï¼Œå¿½ç•¥ä¼ å…¥çš„å‚æ•°
        fixed_kwargs = {k: v for k, v in kwargs.items() if k not in ['tbname_or_trows', 'agg_or_select']}
        fixed_kwargs.update({
            'tbname_or_trows': 'tbname',
            'agg_or_select': 'agg'
        })
        
        return {
            # æ»‘åŠ¨çª—å£ - æ‰€æœ‰ç»„åˆ
            'sliding_stb_tbname_agg': self.get_sliding_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'sliding_stb_partition_by_tbname_agg': self.get_sliding_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'sliding_stb_partition_by_tag_agg': self.get_sliding_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'sliding_tb_agg': self.get_sliding_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # ä¼šè¯çª—å£ - æ‰€æœ‰ç»„åˆ
            'session_stb_tbname_agg': self.get_session_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'session_stb_partition_by_tbname_agg': self.get_session_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'session_stb_partition_by_tag_agg': self.get_session_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'session_tb_agg': self.get_session_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # è®¡æ•°çª—å£ - æ‰€æœ‰ç»„åˆ
            'count_stb_tbname_agg': self.get_count_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'count_stb_partition_by_tbname_agg': self.get_count_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'count_stb_partition_by_tag_agg': self.get_count_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'count_tb_agg': self.get_count_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # äº‹ä»¶çª—å£ - æ‰€æœ‰ç»„åˆ
            'event_stb_tbname_agg': self.get_event_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'event_stb_partition_by_tbname_agg': self.get_event_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'event_stb_partition_by_tag_agg': self.get_event_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'event_tb_agg': self.get_event_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # çŠ¶æ€çª—å£ - æ‰€æœ‰ç»„åˆ
            'state_stb_tbname_agg': self.get_state_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'state_stb_partition_by_tbname_agg': self.get_state_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'state_stb_partition_by_tag_agg': self.get_state_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'state_tb_agg': self.get_state_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # å®šæ—¶è§¦å‘ - æ‰€æœ‰ç»„åˆ
            'period_stb_tbname_agg': self.get_period_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'period_stb_partition_by_tbname_agg': self.get_period_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'period_stb_partition_by_tag_agg': self.get_period_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'period_tb_agg': self.get_period_template(source_type='tb', partition_type='none', **fixed_kwargs),
        }

    def get_tbname_select_group(self, **kwargs):
        """è·å– tbname + select ç»„åˆçš„æ‰€æœ‰çª—å£ç±»å‹
        
        å›ºå®šå‚æ•°: tbname_or_trows='tbname', agg_or_select='select'
        å¿½ç•¥å‘½ä»¤è¡Œä¼ å…¥çš„è¿™ä¸¤ä¸ªå‚æ•°
        """
        # å›ºå®šå‚æ•°ï¼Œå¿½ç•¥ä¼ å…¥çš„å‚æ•°
        fixed_kwargs = {k: v for k, v in kwargs.items() if k not in ['tbname_or_trows', 'agg_or_select']}
        fixed_kwargs.update({
            'tbname_or_trows': 'tbname',
            'agg_or_select': 'select'
        })
        
        return {
            # æ»‘åŠ¨çª—å£ - æ‰€æœ‰ç»„åˆ
            'sliding_stb_tbname_select': self.get_sliding_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'sliding_stb_partition_by_tbname_select': self.get_sliding_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'sliding_stb_partition_by_tag_select': self.get_sliding_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'sliding_tb_select': self.get_sliding_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # ä¼šè¯çª—å£ - æ‰€æœ‰ç»„åˆ
            'session_stb_tbname_select': self.get_session_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'session_stb_partition_by_tbname_select': self.get_session_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'session_stb_partition_by_tag_select': self.get_session_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'session_tb_select': self.get_session_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # è®¡æ•°çª—å£ - æ‰€æœ‰ç»„åˆ
            'count_stb_tbname_select': self.get_count_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'count_stb_partition_by_tbname_select': self.get_count_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'count_stb_partition_by_tag_select': self.get_count_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'count_tb_select': self.get_count_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # äº‹ä»¶çª—å£ - æ‰€æœ‰ç»„åˆ
            'event_stb_tbname_select': self.get_event_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'event_stb_partition_by_tbname_select': self.get_event_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'event_stb_partition_by_tag_select': self.get_event_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'event_tb_select': self.get_event_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # çŠ¶æ€çª—å£ - æ‰€æœ‰ç»„åˆ
            'state_stb_tbname_select': self.get_state_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'state_stb_partition_by_tbname_select': self.get_state_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'state_stb_partition_by_tag_select': self.get_state_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'state_tb_select': self.get_state_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # å®šæ—¶è§¦å‘ - æ‰€æœ‰ç»„åˆ
            'period_stb_tbname_select': self.get_period_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'period_stb_partition_by_tbname_select': self.get_period_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'period_stb_partition_by_tag_select': self.get_period_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'period_tb_select': self.get_period_template(source_type='tb', partition_type='none', **fixed_kwargs),
        }

    def get_trows_agg_group(self, **kwargs):
        """è·å– trows + agg ç»„åˆçš„æ‰€æœ‰çª—å£ç±»å‹
        
        å›ºå®šå‚æ•°: tbname_or_trows='trows', agg_or_select='agg'
        å¿½ç•¥å‘½ä»¤è¡Œä¼ å…¥çš„è¿™ä¸¤ä¸ªå‚æ•°
        """
        # å›ºå®šå‚æ•°ï¼Œå¿½ç•¥ä¼ å…¥çš„å‚æ•°
        fixed_kwargs = {k: v for k, v in kwargs.items() if k not in ['tbname_or_trows', 'agg_or_select']}
        fixed_kwargs.update({
            'tbname_or_trows': 'trows',
            'agg_or_select': 'agg'
        })
        
        return {
            # æ»‘åŠ¨çª—å£ - æ‰€æœ‰ç»„åˆ
            'sliding_stb_trows_agg': self.get_sliding_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'sliding_stb_partition_by_tbname_trows_agg': self.get_sliding_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'sliding_stb_partition_by_tag_trows_agg': self.get_sliding_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'sliding_tb_trows_agg': self.get_sliding_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # ä¼šè¯çª—å£ - æ‰€æœ‰ç»„åˆ
            'session_stb_trows_agg': self.get_session_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'session_stb_partition_by_tbname_trows_agg': self.get_session_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'session_stb_partition_by_tag_trows_agg': self.get_session_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'session_tb_trows_agg': self.get_session_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # è®¡æ•°çª—å£ - æ‰€æœ‰ç»„åˆ
            'count_stb_trows_agg': self.get_count_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'count_stb_partition_by_tbname_trows_agg': self.get_count_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'count_stb_partition_by_tag_trows_agg': self.get_count_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'count_tb_trows_agg': self.get_count_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # äº‹ä»¶çª—å£ - æ‰€æœ‰ç»„åˆ
            'event_stb_trows_agg': self.get_event_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'event_stb_partition_by_tbname_trows_agg': self.get_event_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'event_stb_partition_by_tag_trows_agg': self.get_event_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'event_tb_trows_agg': self.get_event_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # çŠ¶æ€çª—å£ - æ‰€æœ‰ç»„åˆ
            'state_stb_trows_agg': self.get_state_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'state_stb_partition_by_tbname_trows_agg': self.get_state_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'state_stb_partition_by_tag_trows_agg': self.get_state_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'state_tb_trows_agg': self.get_state_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # å®šæ—¶è§¦å‘ - æ‰€æœ‰ç»„åˆ (æ³¨æ„: periodä¸æ”¯æŒtrowsï¼Œæ‰€ä»¥è¿™é‡Œå®é™…ä¸Šä»ç„¶ä½¿ç”¨tbname)
            'period_stb_trows_agg': self.get_period_template(source_type='stb', partition_type='none', **{k: v for k, v in fixed_kwargs.items() if k != 'tbname_or_trows'}),
            'period_stb_partition_by_tbname_trows_agg': self.get_period_template(source_type='stb', partition_type='tbname', **{k: v for k, v in fixed_kwargs.items() if k != 'tbname_or_trows'}),
            'period_stb_partition_by_tag_trows_agg': self.get_period_template(source_type='stb', partition_type='tag', **{k: v for k, v in fixed_kwargs.items() if k != 'tbname_or_trows'}),
            'period_tb_trows_agg': self.get_period_template(source_type='tb', partition_type='none', **{k: v for k, v in fixed_kwargs.items() if k != 'tbname_or_trows'}),
        }

    def get_trows_select_group(self, **kwargs):
        """è·å– trows + select ç»„åˆçš„æ‰€æœ‰çª—å£ç±»å‹
        
        å›ºå®šå‚æ•°: tbname_or_trows='trows', agg_or_select='select'
        å¿½ç•¥å‘½ä»¤è¡Œä¼ å…¥çš„è¿™ä¸¤ä¸ªå‚æ•°
        """
        # å›ºå®šå‚æ•°ï¼Œå¿½ç•¥ä¼ å…¥çš„å‚æ•°
        fixed_kwargs = {k: v for k, v in kwargs.items() if k not in ['tbname_or_trows', 'agg_or_select']}
        fixed_kwargs.update({
            'tbname_or_trows': 'trows',
            'agg_or_select': 'select'
        })
        
        return {
            # æ»‘åŠ¨çª—å£ - æ‰€æœ‰ç»„åˆ
            'sliding_stb_trows_select': self.get_sliding_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'sliding_stb_partition_by_tbname_trows_select': self.get_sliding_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'sliding_stb_partition_by_tag_trows_select': self.get_sliding_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'sliding_tb_trows_select': self.get_sliding_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # ä¼šè¯çª—å£ - æ‰€æœ‰ç»„åˆ
            'session_stb_trows_select': self.get_session_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'session_stb_partition_by_tbname_trows_select': self.get_session_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'session_stb_partition_by_tag_trows_select': self.get_session_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'session_tb_trows_select': self.get_session_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # è®¡æ•°çª—å£ - æ‰€æœ‰ç»„åˆ
            'count_stb_trows_select': self.get_count_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'count_stb_partition_by_tbname_trows_select': self.get_count_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'count_stb_partition_by_tag_trows_select': self.get_count_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'count_tb_trows_select': self.get_count_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # äº‹ä»¶çª—å£ - æ‰€æœ‰ç»„åˆ
            'event_stb_trows_select': self.get_event_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'event_stb_partition_by_tbname_trows_select': self.get_event_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'event_stb_partition_by_tag_trows_select': self.get_event_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'event_tb_trows_select': self.get_event_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # çŠ¶æ€çª—å£ - æ‰€æœ‰ç»„åˆ
            'state_stb_trows_select': self.get_state_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'state_stb_partition_by_tbname_trows_select': self.get_state_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'state_stb_partition_by_tag_trows_select': self.get_state_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'state_tb_trows_select': self.get_state_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # å®šæ—¶è§¦å‘ - æ‰€æœ‰ç»„åˆ (æ³¨æ„: periodä¸æ”¯æŒtrowsï¼Œæ‰€ä»¥è¿™é‡Œå®é™…ä¸Šä»ç„¶ä½¿ç”¨tbname)
            'period_stb_trows_select': self.get_period_template(source_type='stb', partition_type='none', **{k: v for k, v in fixed_kwargs.items() if k != 'tbname_or_trows'}),
            'period_stb_partition_by_tbname_trows_select': self.get_period_template(source_type='stb', partition_type='tbname', **{k: v for k, v in fixed_kwargs.items() if k != 'tbname_or_trows'}),
            'period_stb_partition_by_tag_trows_select': self.get_period_template(source_type='stb', partition_type='tag', **{k: v for k, v in fixed_kwargs.items() if k != 'tbname_or_trows'}),
            'period_tb_trows_select': self.get_period_template(source_type='tb', partition_type='none', **{k: v for k, v in fixed_kwargs.items() if k != 'tbname_or_trows'}),
        }
        
    def generate_multiple_streams(self, base_sql, stream_num=1):
        """æ ¹æ®åŸºç¡€SQLæ¨¡æ¿ç”Ÿæˆå¤šä¸ªç¼–å·çš„æµ
        
        Args:
            base_sql: åŸºç¡€SQLæ¨¡æ¿
            stream_num: è¦ç”Ÿæˆçš„æµæ•°é‡
            
        Returns:
            dict: åŒ…å«å¤šä¸ªæµSQLçš„å­—å…¸ï¼Œkeyä¸ºæµåç§°ï¼Œvalueä¸ºSQL
        """
        if stream_num <= 1:
            return {'stream_1': base_sql}
        
        result = {}
        
        if isinstance(base_sql, str):
            for i in range(1, stream_num + 1):
                modified_sql = self._modify_sql_for_multiple_streams(base_sql, i)
                result[f'stream_{i}'] = modified_sql
        else:
            # å¦‚æœéœ€è¦é‡æ–°ç”Ÿæˆï¼ˆç”¨äºæ”¯æŒä¸åŒå­è¡¨çš„æƒ…å†µï¼‰
            # è¿™ç§æƒ…å†µä¸‹base_sqlåº”è¯¥åŒ…å«ç”Ÿæˆå‡½æ•°å’Œå‚æ•°ä¿¡æ¯
            # æš‚æ—¶ä¿æŒåŸæœ‰é€»è¾‘ï¼Œè¿™ä¸ªåˆ†æ”¯ä¸»è¦ç”¨äºå‘åå…¼å®¹
            for i in range(1, stream_num + 1):
                modified_sql = self._modify_sql_for_multiple_streams(base_sql, i)
                result[f'stream_{i}'] = modified_sql
            
        return result

    def _modify_sql_for_multiple_streams(self, sql, stream_index):
        """ä¿®æ”¹SQLä»¥æ”¯æŒå¤šæµåˆ›å»º
        
        Args:
            sql: åŸå§‹SQL
            stream_index: æµç¼–å·
            
        Returns:
            str: ä¿®æ”¹åçš„SQL
        """
        import re
        
        # 1. ä¿®æ”¹æµåç§°ï¼Œæ·»åŠ ç¼–å·åç¼€
        # åŒ¹é… "create stream xxx" æ¨¡å¼
        sql = re.sub(
            r'(create\s+stream\s+)([^\s]+)',
            rf'\1\2_{stream_index}',
            sql,
            flags=re.IGNORECASE
        )
        
        # 2. ä¿®æ”¹ç›®æ ‡è¡¨åç§°ï¼Œæ·»åŠ ç¼–å·åç¼€
        # åŒ¹é… "into stream_to.xxx" æ¨¡å¼
        sql = re.sub(
            r'(into\s+)(stream_to\.)([^\s]+)',
            rf'\1\2\3_{stream_index}',
            sql,
            flags=re.IGNORECASE
        )
        
        return sql
        
    '''                    
    # ========== oldæµæ¨¡æ¿ï¼Œä»ç„¶æ”¯æŒï¼Œä½œä¸ºç¬¬ä¸€è½®çš„æ‘¸åº•æµ‹è¯•ï¼Œä½†ä¸æ¨èä½¿ç”¨äº†ï¼Œä½¿ç”¨ä¸Šé¢çš„å»ºæµæ¨¡ç‰ˆè¿›è¡Œç¬¬äºŒè½®æµ‹è¯• (s2_2 åˆ° s2_6) ==========
    def get_s2_2(self, agg_or_select='agg', custom_columns=None):
        """trigger at_once"""
        columns = self._build_columns(agg_or_select, custom_columns)
        return f"""
    create stream s2_2 trigger at_once 
        ignore expired 0 ignore update 0 into stream_to.stb2_2 
        as select _wstart as wstart,
        {columns}
        from stream_from.stb partition by tbname interval(15s);
    """
    
    def get_s2_3(self, agg_or_select='agg', custom_columns=None):
        """trigger window_close"""
        columns = self._build_columns(agg_or_select, custom_columns)
        return f"""
    create stream s2_3 trigger window_close 
        ignore expired 0 ignore update 0 into stream_to.stb2_3 
        as select _wstart as wstart,
        {columns}
        from stream_from.stb partition by tbname interval(15s);
    """
    
    def get_s2_4(self, agg_or_select='agg', custom_columns=None):
        """trigger max_delay 5s"""
        columns = self._build_columns(agg_or_select, custom_columns)
        return f"""
    create stream s2_4 trigger max_delay 5s 
        ignore expired 0 ignore update 0 into stream_to.stb2_4 
        as select _wstart as wstart,
        {columns}
        from stream_from.stb partition by tbname interval(15s);
    """
    
    def get_s2_5(self, agg_or_select='agg', custom_columns=None):
        """trigger FORCE_WINDOW_CLOSE"""
        columns = self._build_columns(agg_or_select, custom_columns)
        return f"""
    create stream s2_5 trigger FORCE_WINDOW_CLOSE into stream_to.stb2_5 
        as select _wstart as wstart,
        {columns}
        from stream_from.stb partition by tbname interval(15s);
    """
    
    def get_s2_6(self, agg_or_select='agg', custom_columns=None):
        """trigger CONTINUOUS_WINDOW_CLOSE"""
        columns = self._build_columns(agg_or_select, custom_columns)
        return f"""
    create stream s2_6 trigger CONTINUOUS_WINDOW_CLOSE 
        ignore expired 0 ignore update 0 into stream_to.stb2_6 
        as select _wstart as wstart,
        {columns}
        from stream_from.stb partition by tbname interval(15s);
    """
    
    # ========== newæµæ¨¡æ¿ï¼Œä»ç„¶æ”¯æŒï¼Œä½œä¸ºç¬¬ä¸€è½®çš„æ‘¸åº•æµ‹è¯•ï¼Œä½†ä¸æ¨èä½¿ç”¨äº†ï¼Œä½¿ç”¨ä¸Šé¢çš„å»ºæµæ¨¡ç‰ˆè¿›è¡Œç¬¬äºŒè½®æµ‹è¯•  (s2_7 åˆ° s2_15) ==========
    def get_s2_7(self, agg_or_select='agg', tbname_or_trows='tbname', custom_columns=None):
        """INTERVAL(15s) çª—å£"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows)
        return f"""
    create stream stream_from.s2_7 INTERVAL(15s) SLIDING(15s)
            from stream_from.stb 
            partition by tbname 
            into stream_to.stb2_7
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_s2_8(self, agg_or_select='agg', tbname_or_trows='trows', custom_columns=None):
        """INTERVAL with %%trows"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows)
        return f"""
    create stream stream_from.s2_8 INTERVAL(15s) SLIDING(15s)
            from stream_from.stb 
            partition by tbname 
            into stream_to.stb2_8
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_s2_9(self, agg_or_select='agg', tbname_or_trows='tbname', custom_columns=None):
        """INTERVAL with MAX_DELAY"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows)
        return f"""
    create stream stream_from.s2_9 INTERVAL(15s) SLIDING(15s)
            from stream_from.stb partition by tbname 
            STREAM_OPTIONS(MAX_DELAY(5s)) 
            into stream_to.stb2_9
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_s2_10(self, agg_or_select='agg', tbname_or_trows='trows', custom_columns=None):
        """INTERVAL with MAX_DELAY and %%trows"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows)
        return f"""
    create stream stream_from.s2_10 INTERVAL(15s) SLIDING(15s) 
            from stream_from.stb 
            STREAM_OPTIONS(MAX_DELAY(5s))
            into stream_to.stb2_10
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_s2_11(self, agg_or_select='agg', custom_columns=None):
        """PERIOD(15s) å®šæ—¶è§¦å‘"""
        columns = self._build_columns(agg_or_select, custom_columns)
        return f"""
    create stream stream_from.s2_11 period(15s) 
            from stream_from.stb partition by tbname  
            into stream_to.stb2_11
            as select cast(_tlocaltime/1000000 as timestamp) ts, {columns}
            from %%tbname;
    """
    
    def get_s2_12(self, agg_or_select='agg', tbname_or_trows='tbname', custom_columns=None):
        """SESSION(ts,10a) ä¼šè¯çª—å£"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows)
        return f"""
    create stream stream_from.s2_12 session(ts,10a)
            from stream_from.stb partition by tbname 
            into stream_to.stb2_12
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_s2_13(self, agg_or_select='agg', tbname_or_trows='tbname', custom_columns=None):
        """COUNT_WINDOW(1000) è®¡æ•°çª—å£"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows)
        return f"""
    create stream stream_from.s2_13 COUNT_WINDOW(1000)
            from stream_from.stb partition by tbname 
            into stream_to.stb2_13
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_s2_14(self, agg_or_select='agg', tbname_or_trows='tbname', custom_columns=None):
        """EVENT_WINDOW äº‹ä»¶çª—å£"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows)
        return f"""
    create stream stream_from.s2_14 EVENT_WINDOW(START WITH c0 > -10000000 END WITH c0 < 10000000) 
            from stream_from.stb partition by tbname 
            into stream_to.stb2_14
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_s2_15(self, agg_or_select='agg', tbname_or_trows='tbname', custom_columns=None):
        """STATE_WINDOW çŠ¶æ€çª—å£"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows)
        return f"""
    create stream stream_from.s2_15 STATE_WINDOW(c0) 
            from stream_from.stb partition by tbname 
            into stream_to.stb2_15
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    # ========== åˆ†ç»„å’Œæ‰¹é‡è·å–æ–¹æ³• ==========
    def get_sliding_group(self, **kwargs):
        """è·å–æ»‘åŠ¨çª—å£ç»„ (s2_7, s2_8, s2_9, s2_10)"""
        return {
            's2_7': self.get_s2_7(**kwargs),
            's2_8': self.get_s2_8(**kwargs),
            's2_9': self.get_s2_9(**kwargs),
            's2_10': self.get_s2_10(**kwargs)
        }
    
    def get_count_group(self, **kwargs):
        """è·å–è®¡æ•°çª—å£ç»„ (s2_13)"""
        return {
            's2_13': self.get_s2_13(**kwargs)
        }
    
    def get_session_group(self, **kwargs):
        """è·å–ä¼šè¯çª—å£ç»„ (s2_12)"""
        return {
            's2_12': self.get_s2_12(**kwargs)
        }
    
    def get_period_group(self, **kwargs):
        """è·å–å®šæ—¶è§¦å‘ç»„ (s2_11)"""
        # period ç±»å‹ä¸æ”¯æŒ tbname_or_trows å‚æ•°
        filtered_kwargs = {k: v for k, v in kwargs.items() if k != 'tbname_or_trows'}
        return {
            's2_11': self.get_s2_11(**filtered_kwargs)
        }
    
    def get_event_group(self, **kwargs):
        """è·å–äº‹ä»¶çª—å£ç»„ (s2_14)"""
        return {
            's2_14': self.get_s2_14(**kwargs)
        }
    
    def get_state_group(self, **kwargs):
        """è·å–çŠ¶æ€çª—å£ç»„ (s2_15)"""
        return {
            's2_15': self.get_s2_15(**kwargs)
        }
    
    def get_basic_group(self, **kwargs):
        """è·å–åŸºç¡€è§¦å‘ç»„ (s2_2 åˆ° s2_6)"""
        # åŸºç¡€ç»„ä¸æ”¯æŒ tbname_or_trows å‚æ•°
        filtered_kwargs = {k: v for k, v in kwargs.items() if k != 'tbname_or_trows'}
        return {
            's2_2': self.get_s2_2(**filtered_kwargs),
            's2_3': self.get_s2_3(**filtered_kwargs),
            's2_4': self.get_s2_4(**filtered_kwargs),
            's2_5': self.get_s2_5(**filtered_kwargs),
            's2_6': self.get_s2_6(**filtered_kwargs)
        }
    
    def get_all_advanced(self, **kwargs):
        """è·å–æ‰€æœ‰é«˜çº§æµ (s2_7 åˆ° s2_15)"""
        result = {}
        result.update(self.get_sliding_group(**kwargs))
        result.update(self.get_period_group(**kwargs))
        result.update(self.get_session_group(**kwargs))
        result.update(self.get_count_group(**kwargs))
        result.update(self.get_event_group(**kwargs))
        result.update(self.get_state_group(**kwargs))
        return result
    
    def get_all(self, **kwargs):
        """è·å–æ‰€æœ‰æµæ¨¡æ¿"""
        result = {}
        result.update(self.get_basic_group(**kwargs))
        result.update(self.get_all_advanced(**kwargs))
        return result
    
    # @classmethod
    # def get_sql_bak(cls, sql_type, **kwargs):
    #     """
    #     è·å–æŒ‡å®šç±»å‹çš„ SQL æ¨¡æ¿ - ä¸»è¦å…¥å£æ–¹æ³•
        
    #     Args:
    #         sql_type: SQL ç±»å‹æ ‡è¯†ç¬¦æˆ–åˆ†ç»„å
    #         **kwargs: å¯é€‰å‚æ•°
    #             - agg_or_select: 'agg'(é»˜è®¤) æˆ– 'select'ï¼Œæ§åˆ¶èšåˆè¿˜æ˜¯æŠ•å½±æŸ¥è¯¢
    #             - tbname_or_trows: 'tbname'(é»˜è®¤) æˆ– 'trows'ï¼Œæ§åˆ¶FROMå­å¥
    #             - custom_columns: è‡ªå®šä¹‰åˆ—ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨è‡ªå®šä¹‰åˆ—
                
    #     Returns:
    #         str or dict: å•ä¸ªSQLå­—ç¬¦ä¸²æˆ–SQLå­—å…¸
            
    #     Usage Examples:
    #         # è·å–å•ä¸ªæ¨¡æ¿
    #         sql = StreamSQLTemplates.get_sql('s2_7')
            
    #         # ä½¿ç”¨æŠ•å½±æŸ¥è¯¢
    #         sql = StreamSQLTemplates.get_sql('s2_7', agg_or_select='select')
            
    #         # ä½¿ç”¨trows
    #         sql = StreamSQLTemplates.get_sql('s2_8', tbname_or_trows='trows')
            
    #         # è‡ªå®šä¹‰åˆ—
    #         sql = StreamSQLTemplates.get_sql('s2_7', custom_columns=['sum(c0)', 'count(*)'])
            
    #         # è·å–åˆ†ç»„
    #         sqls = StreamSQLTemplates.get_sql('sliding')  # æ»‘åŠ¨çª—å£ç»„
    #         sqls = StreamSQLTemplates.get_sql('all')      # æ‰€æœ‰æ¨¡æ¿
    #     """
    #     instance = cls()
        
    #     # å•ä¸ªæ¨¡æ¿æ˜ å°„
    #     single_templates = {
    #         's2_2': instance.get_s2_2,
    #         's2_3': instance.get_s2_3,
    #         's2_4': instance.get_s2_4,
    #         's2_5': instance.get_s2_5,
    #         's2_6': instance.get_s2_6,
    #         's2_7': instance.get_s2_7,
    #         's2_8': instance.get_s2_8,
    #         's2_9': instance.get_s2_9,
    #         's2_10': instance.get_s2_10,
    #         's2_11': instance.get_s2_11,
    #         's2_12': instance.get_s2_12,
    #         's2_13': instance.get_s2_13,
    #         's2_14': instance.get_s2_14,
    #         's2_15': instance.get_s2_15,
    #     }
        
    #     # åˆ†ç»„æ¨¡æ¿æ˜ å°„
    #     group_templates = {
    #         'basic': instance.get_basic_group,
    #         'sliding': instance.get_sliding_group,
    #         'count': instance.get_count_group,
    #         'session': instance.get_session_group,
    #         'period': instance.get_period_group,
    #         'event': instance.get_event_group,
    #         'state': instance.get_state_group,
    #         'advanced': instance.get_all_advanced,
    #         'all': instance.get_all,
    #     }
        
    #     # å¤„ç†å•ä¸ªæ¨¡æ¿
    #     if sql_type in single_templates:
    #         method = single_templates[sql_type]
    #         # ä¸ºä¸åŒç±»å‹çš„æ¨¡æ¿è¿‡æ»¤å‚æ•°
    #         if sql_type in ['s2_2', 's2_3', 's2_4', 's2_5', 's2_6', 's2_11']:
    #             # åŸºç¡€æ¨¡æ¿å’Œperiodæ¨¡æ¿ä¸æ”¯æŒ tbname_or_trows
    #             filtered_kwargs = {k: v for k, v in kwargs.items() if k != 'tbname_or_trows'}
    #             return method(**filtered_kwargs)
    #         else:
    #             return method(**kwargs)
        
    #     # å¤„ç†åˆ†ç»„æ¨¡æ¿
    #     if sql_type in group_templates:
    #         return group_templates[sql_type](**kwargs)
        
    #     # é»˜è®¤è¿”å› s2_7
    #     print(f"è­¦å‘Š: æœªæ‰¾åˆ°æ¨¡æ¿ '{sql_type}'ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿ 's2_7'")
    #     return instance.get_s2_7(**kwargs)
    
    '''
    
    @classmethod
    def get_sql(cls, sql_type, stream_num=1, **kwargs):
        """
        è·å–æŒ‡å®šç±»å‹çš„ SQL æ¨¡æ¿ - ä¸»è¦å…¥å£æ–¹æ³•
        
        Args:
            sql_type: SQL ç±»å‹æ ‡è¯†ç¬¦æˆ–åˆ†ç»„å
                å•ä¸ªæ¨¡æ¿: 'sliding_stb', 'sliding_stb_partition_by_tbname' ç­‰
                ç»„åˆæ¨¡æ¿: 'sliding_detailed', 'session_detailed' ç­‰
            stream_num: æµæ•°é‡ï¼ˆä»…å¯¹å•ä¸ªæµç±»å‹æœ‰æ•ˆï¼‰
            **kwargs: å¯é€‰å‚æ•°
                
        Returns:
            str or dict: å•ä¸ªSQLå­—ç¬¦ä¸²æˆ–SQLå­—å…¸
            
        Usage Examples:
            # è·å–å•ä¸ªè¯¦ç»†æ¨¡æ¿
            sql = StreamSQLTemplates.get_sql('sliding_stb_partition_by_tbname')
            
            # è·å–è¯¦ç»†ç»„åˆ
            sqls = StreamSQLTemplates.get_sql('sliding_detailed')
            
            # è·å–æ‰€æœ‰çª—å£ç±»å‹çš„è¯¦ç»†ç»„åˆ
            sqls = StreamSQLTemplates.get_sql('all_detailed')
        """
        instance = cls()
        
        # å¤„ç†ç‰¹æ®Šçš„æŸ¥è¯¢è¯­å¥
        if sql_type == 'select_stream':
            return instance.get_select_stream(**kwargs)
        
        if sql_type == 'tbname_agg':
            if stream_num > 1:
                print("è­¦å‘Š: å›ºå®šå‚æ•°ç»„åˆæ¨¡æ¿ä¸æ”¯æŒ --stream-num å‚æ•°ï¼Œå°†å¿½ç•¥è¯¥å‚æ•°")
            return instance.get_tbname_agg_group(**kwargs)
        elif sql_type == 'tbname_select':
            if stream_num > 1:
                print("è­¦å‘Š: å›ºå®šå‚æ•°ç»„åˆæ¨¡æ¿ä¸æ”¯æŒ --stream-num å‚æ•°ï¼Œå°†å¿½ç•¥è¯¥å‚æ•°")
            return instance.get_tbname_select_group(**kwargs)
        elif sql_type == 'trows_agg':
            if stream_num > 1:
                print("è­¦å‘Š: å›ºå®šå‚æ•°ç»„åˆæ¨¡æ¿ä¸æ”¯æŒ --stream-num å‚æ•°ï¼Œå°†å¿½ç•¥è¯¥å‚æ•°")
            return instance.get_trows_agg_group(**kwargs)
        elif sql_type == 'trows_select':
            if stream_num > 1:
                print("è­¦å‘Š: å›ºå®šå‚æ•°ç»„åˆæ¨¡æ¿ä¸æ”¯æŒ --stream-num å‚æ•°ï¼Œå°†å¿½ç•¥è¯¥å‚æ•°")
            return instance.get_trows_select_group(**kwargs)
        
        # å•ä¸ªè¯¦ç»†æ¨¡æ¿æ˜ å°„
        detailed_templates = {
            'sliding_stb': lambda stream_index=None, **kw: instance.get_sliding_template(source_type='stb', partition_type='none', stream_index=stream_index, **kw),
            'sliding_stb_partition_by_tbname': lambda stream_index=None, **kw: instance.get_sliding_template(source_type='stb', partition_type='tbname', stream_index=stream_index, **kw),
            'sliding_stb_partition_by_tag': lambda stream_index=None, **kw: instance.get_sliding_template(source_type='stb', partition_type='tag', stream_index=stream_index, **kw),
            'sliding_tb': lambda stream_index=None, **kw: instance.get_sliding_template(source_type='tb', partition_type='none', stream_index=stream_index, **kw),
            
            'session_stb': lambda stream_index=None, **kw: instance.get_session_template(source_type='stb', partition_type='none', stream_index=stream_index, **kw),
            'session_stb_partition_by_tbname': lambda stream_index=None, **kw: instance.get_session_template(source_type='stb', partition_type='tbname', stream_index=stream_index, **kw),
            'session_stb_partition_by_tag': lambda stream_index=None, **kw: instance.get_session_template(source_type='stb', partition_type='tag', stream_index=stream_index, **kw),
            'session_tb': lambda stream_index=None, **kw: instance.get_session_template(source_type='tb', partition_type='none', stream_index=stream_index, **kw),
            
            'count_stb': lambda stream_index=None, **kw: instance.get_count_template(source_type='stb', partition_type='none', stream_index=stream_index, **kw),
            'count_stb_partition_by_tbname': lambda stream_index=None, **kw: instance.get_count_template(source_type='stb', partition_type='tbname', stream_index=stream_index, **kw),
            'count_stb_partition_by_tag': lambda stream_index=None, **kw: instance.get_count_template(source_type='stb', partition_type='tag', stream_index=stream_index, **kw),
            'count_tb': lambda stream_index=None, **kw: instance.get_count_template(source_type='tb', partition_type='none', stream_index=stream_index, **kw),
            
            'event_stb': lambda stream_index=None, **kw: instance.get_event_template(source_type='stb', partition_type='none', stream_index=stream_index, **kw),
            'event_stb_partition_by_tbname': lambda stream_index=None, **kw: instance.get_event_template(source_type='stb', partition_type='tbname', stream_index=stream_index, **kw),
            'event_stb_partition_by_tag': lambda stream_index=None, **kw: instance.get_event_template(source_type='stb', partition_type='tag', stream_index=stream_index, **kw),
            'event_tb': lambda stream_index=None, **kw: instance.get_event_template(source_type='tb', partition_type='none', stream_index=stream_index, **kw),
            
            'state_stb': lambda stream_index=None, **kw: instance.get_state_template(source_type='stb', partition_type='none', stream_index=stream_index, **kw),
            'state_stb_partition_by_tbname': lambda stream_index=None, **kw: instance.get_state_template(source_type='stb', partition_type='tbname', stream_index=stream_index, **kw),
            'state_stb_partition_by_tag': lambda stream_index=None, **kw: instance.get_state_template(source_type='stb', partition_type='tag', stream_index=stream_index, **kw),
            'state_tb': lambda stream_index=None, **kw: instance.get_state_template(source_type='tb', partition_type='none', stream_index=stream_index, **kw),
            
            'period_stb': lambda stream_index=None, **kw: instance.get_period_template(source_type='stb', partition_type='none', stream_index=stream_index, **kw),
            'period_stb_partition_by_tbname': lambda stream_index=None, **kw: instance.get_period_template(source_type='stb', partition_type='tbname', stream_index=stream_index, **kw),
            'period_stb_partition_by_tag': lambda stream_index=None, **kw: instance.get_period_template(source_type='stb', partition_type='tag', stream_index=stream_index, **kw),
            'period_tb': lambda stream_index=None, **kw: instance.get_period_template(source_type='tb', partition_type='none', stream_index=stream_index, **kw),
        }
        
        # ç»„åˆæ¨¡æ¿æ˜ å°„
        group_templates = {
            'sliding_detailed': instance.get_sliding_group_detailed,
            'session_detailed': instance.get_session_group_detailed,
            'count_detailed': instance.get_count_group_detailed,
            'event_detailed': instance.get_event_group_detailed,
            'state_detailed': instance.get_state_group_detailed,
            'period_detailed': instance.get_period_group_detailed,
        }
        
        # # å¤„ç†å•ä¸ªè¯¦ç»†æ¨¡æ¿
        # if sql_type in detailed_templates:
        #     return detailed_templates[sql_type](**kwargs)
        
        # # å¤„ç†ç»„åˆæ¨¡æ¿
        # if sql_type in group_templates:
        #     return group_templates[sql_type](**kwargs)
        
        # # å¤„ç†å•ä¸ªè¯¦ç»†æ¨¡æ¿
        # if sql_type in detailed_templates:
        #     base_sql = detailed_templates[sql_type](**kwargs)
            
        #     # å¦‚æœstream_num > 1ï¼Œç”Ÿæˆå¤šä¸ªæµ
        #     if stream_num > 1:
        #         print(f"ç”Ÿæˆ {stream_num} ä¸ª {sql_type} ç±»å‹çš„æµ")
        #         return instance.generate_multiple_streams(base_sql, stream_num)
        #     else:
        #         return base_sql
        
        # å¤„ç†å•ä¸ªè¯¦ç»†æ¨¡æ¿æ—¶
        if sql_type in detailed_templates:
            # å¦‚æœstream_num > 1ï¼Œéœ€è¦ä¸ºæ¯ä¸ªæµç”Ÿæˆä¸åŒçš„SQLï¼ˆç‰¹åˆ«æ˜¯tbç±»å‹ï¼‰
            if stream_num > 1:
                print(f"ç”Ÿæˆ {stream_num} ä¸ª {sql_type} ç±»å‹çš„æµ")
                result = {}
                
                for i in range(1, stream_num + 1):
                    # ä¸ºæ¯ä¸ªæµä¼ å…¥stream_indexå‚æ•°
                    stream_sql = detailed_templates[sql_type](stream_index=i, **kwargs)
                    result[f'stream_{i}'] = stream_sql
                    
                return result
            else:
                return detailed_templates[sql_type](**kwargs)
        
        # # å¤„ç†ç»„åˆæ¨¡æ¿
        # if sql_type in group_templates:
        #     if stream_num > 1:
        #         print("è­¦å‘Š: ç»„åˆæ¨¡æ¿ä¸æ”¯æŒ --stream-num å‚æ•°ï¼Œå°†å¿½ç•¥è¯¥å‚æ•°")
        #     return group_templates[sql_type](**kwargs)
        
        # # å¤„ç†ç‰¹æ®Šç»„åˆ
        # if sql_type == 'all_detailed':
        #     if stream_num > 1:
        #         print("è­¦å‘Š: all_detailed æ¨¡æ¿ä¸æ”¯æŒ --stream-num å‚æ•°ï¼Œå°†å¿½ç•¥è¯¥å‚æ•°")
        #     result = {}
        #     for group_type in ['sliding_detailed', 'session_detailed', 'count_detailed', 
        #                     'event_detailed', 'state_detailed', 'period_detailed']:
        #         result.update(group_templates[group_type](**kwargs))
        #     return result
        
        # å¤„ç†ç»„åˆæ¨¡æ¿ï¼ˆæ”¯æŒ stream_numï¼‰
        if sql_type in group_templates:
            base_sqls = group_templates[sql_type](**kwargs)
            
            # å¦‚æœstream_num > 1ï¼Œä¸ºæ¯ä¸ªç»„åˆä¸­çš„æ¯ä¸ªæµç”Ÿæˆå¤šä¸ªå‰¯æœ¬
            if stream_num > 1:
                print(f"ç”Ÿæˆ {sql_type} ç»„åˆï¼Œæ¯ç§æµç±»å‹åˆ›å»º {stream_num} ä¸ªæµ")
                result = {}
                
                for base_name, base_sql in base_sqls.items():
                    # ä¸ºæ¯ä¸ªåŸºç¡€æµç”Ÿæˆå¤šä¸ªç¼–å·å‰¯æœ¬
                    multiple_streams = instance.generate_multiple_streams(base_sql, stream_num)
                    
                    # é‡å‘½åé”®å€¼ä»¥é¿å…å†²çª
                    for stream_key, stream_sql in multiple_streams.items():
                        # ä¾‹å¦‚: sliding_stb_1, sliding_stb_2, sliding_stb_partition_by_tbname_1, etc.
                        new_key = f"{base_name}_{stream_key.split('_')[-1]}"  # æå–ç¼–å·
                        result[new_key] = stream_sql
                
                return result
            else:
                return base_sqls
        
        # å¤„ç†ç‰¹æ®Šç»„åˆ
        if sql_type == 'all_detailed':
            result = {}
            for group_type in ['sliding_detailed', 'session_detailed', 'count_detailed', 
                            'event_detailed', 'state_detailed', 'period_detailed']:
                group_sqls = group_templates[group_type](**kwargs)
                
                # å¦‚æœstream_num > 1ï¼Œä¸ºæ¯ä¸ªæµç”Ÿæˆå¤šä¸ªå‰¯æœ¬
                if stream_num > 1:
                    for base_name, base_sql in group_sqls.items():
                        multiple_streams = instance.generate_multiple_streams(base_sql, stream_num)
                        
                        for stream_key, stream_sql in multiple_streams.items():
                            new_key = f"{base_name}_{stream_key.split('_')[-1]}"
                            result[new_key] = stream_sql
                else:
                    result.update(group_sqls)
                    
            if stream_num > 1:
                print(f"ç”Ÿæˆ all_detailed ç»„åˆï¼Œæ¯ç§æµç±»å‹åˆ›å»º {stream_num} ä¸ªæµ")
                
            return result
        
        # é»˜è®¤è¿”å› select_stream
        print(f"è­¦å‘Š: æœªæ‰¾åˆ°æ¨¡æ¿ '{sql_type}'ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿ 'select_stream'")
        return instance.get_select_stream(**kwargs)

            
def format_delay_time(delay_ms):
    """å°†å»¶è¿Ÿæ¯«ç§’æ•°æ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„æ—¶é—´æ ¼å¼
    
    Args:
        delay_ms: å»¶è¿Ÿæ¯«ç§’æ•°
        
    Returns:
        str: æ ¼å¼åŒ–åçš„æ—¶é—´å­—ç¬¦ä¸²
    """
    if delay_ms is None:
        return "N/A"
    
    if delay_ms < 1000:  # å°äº1ç§’ï¼Œæ˜¾ç¤ºæ¯«ç§’
        return f"{delay_ms}ms"
    elif delay_ms < 60000:  # å°äº1åˆ†é’Ÿï¼Œæ˜¾ç¤ºç§’
        seconds = delay_ms / 1000.0
        return f"{seconds:.1f}s"
    elif delay_ms < 3600000:  # å°äº1å°æ—¶ï¼Œæ˜¾ç¤ºåˆ†é’Ÿå’Œç§’
        minutes = delay_ms // 60000
        seconds = (delay_ms % 60000) / 1000.0
        if seconds >= 1:
            return f"{minutes}m{seconds:.1f}s"
        else:
            return f"{minutes}m"
    else:  # 1å°æ—¶ä»¥ä¸Šï¼Œæ˜¾ç¤ºå°æ—¶ã€åˆ†é’Ÿã€ç§’
        hours = delay_ms // 3600000
        minutes = (delay_ms % 3600000) // 60000
        seconds = (delay_ms % 60000) / 1000.0
        
        parts = [f"{hours}h"]
        if minutes > 0:
            parts.append(f"{minutes}m")
        if seconds >= 1:
            parts.append(f"{seconds:.1f}s")
        
        return "".join(parts)
        
        
class StreamStarter:
    def __init__(self, runtime=None, perf_file=None, table_count=500, 
                histroy_rows=1, real_time_batch_rows=200, disorder_ratio=0, vgroups=10,
                stream_sql=None, sql_type='select_stream', stream_num=1, stream_perf_test_dir=None, monitor_interval=1,
                create_data=False, restore_data=False, deployment_mode='single',
                debug_flag=131, num_of_log_lines=500000, 
                agg_or_select='agg', tbname_or_trows='tbname', custom_columns=None,
                check_stream_delay=False, max_delay_threshold=30000, delay_check_interval=10,
                real_time_batch_sleep=0) -> None:
        
        self.stream_perf_test_dir = stream_perf_test_dir if stream_perf_test_dir else '/home/taos_stream_cluster'        
        self.table_count = table_count      
        self.histroy_rows = histroy_rows      
        self.real_time_batch_rows = real_time_batch_rows    
        self.real_time_batch_sleep = real_time_batch_sleep  
        self.disorder_ratio = disorder_ratio 
        self.vgroups = vgroups 
        self.monitor_interval = monitor_interval
        self.taosd_processes = [] 
        self.create_data = create_data
        self.restore_data = restore_data
        self.backup_dir = os.path.join(self.stream_perf_test_dir, 'data_bak')
        self.deployment_mode = deployment_mode
        self.debug_flag = debug_flag
        self.num_of_log_lines = num_of_log_lines
        
        self.sql_type = sql_type
        self.stream_num = stream_num
        self.agg_or_select = agg_or_select
        self.tbname_or_trows = tbname_or_trows
        self.custom_columns = custom_columns
        print(f"è°ƒè¯•ä¿¡æ¯: tbname_or_trows = {tbname_or_trows}")
        print(f"è°ƒè¯•ä¿¡æ¯: sql_type = {sql_type}")
        print(f"è°ƒè¯•ä¿¡æ¯: agg_or_select = {agg_or_select}")
        print(f"è°ƒè¯•ä¿¡æ¯:æ•°æ®å†™å…¥é—´éš”: {real_time_batch_sleep}ç§’")
        self.stream_sql = stream_sql if stream_sql else StreamSQLTemplates.get_sql(
            sql_type, 
            stream_num=stream_num, 
            agg_or_select=agg_or_select,
            tbname_or_trows=tbname_or_trows,
            custom_columns=custom_columns
        )
        #print(f"ç”Ÿæˆçš„SQL:\n{self.stream_sql}")
        
        self.check_stream_delay = check_stream_delay
        self.max_delay_threshold = max_delay_threshold  # æ¯«ç§’
        self.delay_check_interval = delay_check_interval  # ç§’
        self.delay_log_file = f"{os.path.splitext(perf_file)[0]}-stream-delay.log"
        
        print(f"æµå»¶è¿Ÿæ£€æŸ¥: {'å¯ç”¨' if check_stream_delay else 'ç¦ç”¨'}")
        if check_stream_delay:
            print(f"æœ€å¤§å»¶è¿Ÿé˜ˆå€¼: {max_delay_threshold}ms")
            print(f"å»¶è¿Ÿæ£€æŸ¥é—´éš”: {delay_check_interval}ç§’")
            print(f"å»¶è¿Ÿæ—¥å¿—æ–‡ä»¶: {self.delay_log_file}")
        
        # æ ¹æ®éƒ¨ç½²æ¨¡å¼è°ƒæ•´å®ä¾‹é…ç½®
        if self.deployment_mode == 'single':
            # å•èŠ‚ç‚¹æ¨¡å¼åªä½¿ç”¨ç¬¬ä¸€ä¸ªå®ä¾‹
            self.instances = [
                {
                    'name': 'dnode1',
                    'host': 'localhost',
                    'port': 6030,
                    'user': 'root',
                    'passwd': 'taosdata',
                    'data_dir': f'{self.stream_perf_test_dir}/dnode1/data',
                    'log_dir': f'{self.stream_perf_test_dir}/dnode1/log',
                }
            ]
            
            # å•èŠ‚ç‚¹æ¨¡å¼çš„æ•°æ®åº“é…ç½®
            self.db_config = {
                'stream_from': {
                    'name': 'stream_from',
                    'vgroups': self.vgroups
                },
                'stream_to': {
                    'name': 'stream_to', 
                    'vgroups': self.vgroups
                }
            }
        else:
            # å®šä¹‰3ä¸ªå®ä¾‹çš„é…ç½®
            self.instances = [
                {
                    'name': 'dnode1',
                    'host': 'localhost',
                    'port': 6030,
                    'user': 'root',
                    'passwd': 'taosdata',
                    'data_dir': f'{self.stream_perf_test_dir}/dnode1/data',
                    'log_dir': f'{self.stream_perf_test_dir}/dnode1/log',
                },
                {
                    'name': 'dnode2',
                    'host': 'localhost',
                    'port': 7030,
                    'user': 'root',
                    'passwd': 'taosdata',
                    'data_dir': f'{self.stream_perf_test_dir}/dnode2/data',
                    'log_dir': f'{self.stream_perf_test_dir}/dnode2/log',
                },
                {
                    'name': 'dnode3',
                    'host': 'localhost',
                    'port': 8030,
                    'user': 'root',
                    'passwd': 'taosdata',
                    'data_dir': f'{self.stream_perf_test_dir}/dnode3/data/',
                    'log_dir': f'{self.stream_perf_test_dir}/dnode3/log',
                }
            ]
            
            self.db_config = {
                'stream_from': {
                    'name': 'stream_from',
                    'vgroups': self.vgroups,
                    'dnodes': '1'  # é»˜è®¤åœ¨dnode1ä¸Š
                },
                'stream_to': {
                    'name': 'stream_to', 
                    'vgroups': self.vgroups,
                    'dnodes': '2'  # é»˜è®¤åœ¨dnode2ä¸Š
                }
            }
            
        self.sql = None
        self.host = self.instances[0]['host']
        self.user = self.instances[0]['user']
        self.passwd = self.instances[0]['passwd']
        self.conf = f"{self.stream_perf_test_dir}/dnode1/conf/taos.cfg"
        self.tz = 'Asia/Shanghai'
        # è®¾ç½®è¿è¡Œæ—¶é—´å’Œæ€§èƒ½æ–‡ä»¶è·¯å¾„
        self.runtime = runtime if runtime else 600  # é»˜è®¤è¿è¡Œ10åˆ†é’Ÿ
        self.perf_file = perf_file if perf_file else '/tmp/perf.log'
        

    def get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥
        
        Returns:
            tuple: (connection, cursor)
        """
        try:
            conn = taos.connect(
                host=self.host,
                user=self.user,
                password=self.passwd,
                config=self.conf,
                timezone=self.tz
            )
            cursor = conn.cursor()
            return conn, cursor
        except Exception as e:
            print(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
            raise
        
    def stop_taosd(self):
        """åœæ­¢æ‰€æœ‰taosdè¿›ç¨‹"""
        try:
            # å…ˆå°è¯•æ­£å¸¸åœæ­¢
            subprocess.run('pkill taosd', shell=True)
            time.sleep(10)
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰è¿›ç¨‹å­˜åœ¨
            result = subprocess.run('ps -ef | grep "taosd -c" | grep -v grep', 
                                shell=True, capture_output=True, text=True)
            if result.stdout:
                print("å‘ç°é¡½å›ºè¿›ç¨‹ï¼Œå¼ºåˆ¶åœæ­¢...")
                for line in result.stdout.splitlines():
                    try:
                        pid = int(line.split()[1])
                        subprocess.run(f'kill -9 {pid}', shell=True)
                        print(f"å¼ºåˆ¶ç»ˆæ­¢è¿›ç¨‹ PID: {pid}")
                    except:
                        continue
                
            print("æ‰€æœ‰taosdè¿›ç¨‹å·²åœæ­¢")
            
        except Exception as e:
            print(f"åœæ­¢è¿›ç¨‹å‡ºé”™: {str(e)}")
            raise

    def check_taosd_status(self):
        """æ£€æŸ¥taosdè¿›ç¨‹çŠ¶æ€"""
        try:
            result = subprocess.run('ps -ef | grep "taosd -c" | grep -v grep', 
                                shell=True, capture_output=True, text=True)
            if result.stdout:
                print("\nå½“å‰è¿è¡Œçš„taosdè¿›ç¨‹:")
                for line in result.stdout.splitlines():
                    print(line)
                return True
            else:
                print("è­¦å‘Š: æœªå‘ç°è¿è¡Œä¸­çš„taosdè¿›ç¨‹")
                return False
                
        except Exception as e:
            print(f"æ£€æŸ¥è¿›ç¨‹çŠ¶æ€å‡ºé”™: {str(e)}")
            return False

        
    def create_database(self, db_name, vgroups=None, dnodes=None):
        """åˆ›å»ºæ•°æ®åº“
        
        Args:
            db_name: æ•°æ®åº“åç§°
            vgroups: vgroupsæ•°é‡,å¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            dnodes: æŒ‡å®šæ•°æ®åº“æ‰€åœ¨çš„dnode,å¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼
        """
        try:
            # è·å–æ•°æ®åº“é…ç½®
            db_config = self.db_config.get(db_name, {})
            vgroups = vgroups or db_config.get('vgroups', self.vgroups)
            dnodes = dnodes or db_config.get('dnodes', '1')
            
            # åˆ›å»ºæ•°æ®åº“
            conn, cursor = self.get_connection()
            create_db_sql = f"create database {db_name} vgroups {vgroups}"
            if dnodes:
                create_db_sql += f" dnodes '{dnodes}'"
                
            print(f"\nåˆ›å»ºæ•°æ®åº“: {create_db_sql}")
            cursor.execute(create_db_sql)
            
            # å…³é—­è¿æ¥
            cursor.close()
            conn.close()
            
            print(f"æ•°æ®åº“ {db_name} åˆ›å»ºæˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"åˆ›å»ºæ•°æ®åº“ {db_name} å¤±è´¥: {str(e)}")
            return False
        
    def start_taosd_processes(self):
        """å¯åŠ¨æ‰€æœ‰ taosd è¿›ç¨‹ - æ”¯æŒå•èŠ‚ç‚¹å’Œé›†ç¾¤æ¨¡å¼"""
        try:
            mode_desc = "å•èŠ‚ç‚¹" if self.deployment_mode == 'single' else "é›†ç¾¤"
            print(f"\n=== å¼€å§‹å¯åŠ¨ {mode_desc} taosd è¿›ç¨‹ ===")
            
            # æ ¹æ®éƒ¨ç½²æ¨¡å¼ç¡®å®šè¦å¯åŠ¨çš„èŠ‚ç‚¹
            nodes_to_start = []
            if self.deployment_mode == 'single':
                nodes_to_start = ['dnode1']
            else:
                nodes_to_start = ['dnode1', 'dnode2', 'dnode3']
            
            for dnode in nodes_to_start:
                cfg_file = os.path.join(self.stream_perf_test_dir, dnode, 'conf', 'taos.cfg')
                cmd = f'nohup taosd -c {cfg_file} > /dev/null 2>&1 &'
                print(f"æ‰§è¡Œå¯åŠ¨å‘½ä»¤: {cmd}")
                
                # æ‰§è¡Œå¯åŠ¨å‘½ä»¤
                result = subprocess.run(cmd, shell=True)
                if result.returncode == 0:
                    print(f"å·²æ‰§è¡Œ {dnode} çš„å¯åŠ¨å‘½ä»¤")
                else:
                    print(f"è­¦å‘Š: {dnode} å¯åŠ¨å‘½ä»¤æ‰§è¡Œå¤±è´¥")
                    
                # éªŒè¯è¿›ç¨‹æ˜¯å¦å¯åŠ¨
                time.sleep(2)
                check_cmd = f"pgrep -f 'taosd -c {cfg_file}'"
                if subprocess.run(check_cmd, shell=True, stdout=subprocess.PIPE).stdout:
                    print(f"{dnode} è¿›ç¨‹å·²æˆåŠŸå¯åŠ¨")
                else:
                    print(f"è­¦å‘Š: {dnode} è¿›ç¨‹å¯èƒ½æœªæ­£å¸¸å¯åŠ¨")
            
            # ç­‰å¾…æœåŠ¡å®Œå…¨å¯åŠ¨
            wait_time = 5 if self.deployment_mode == 'single' else 10
            print(f"\nç­‰å¾…æœåŠ¡å¯åŠ¨ ({wait_time}ç§’)...")
            time.sleep(wait_time)             
            self.check_taosd_status()
            
            # é…ç½®é›†ç¾¤ï¼ˆå¦‚æœæ˜¯é›†ç¾¤æ¨¡å¼ï¼‰
            if self.deployment_mode == 'cluster':
                try:
                    # è¿æ¥åˆ°ç¬¬ä¸€ä¸ªèŠ‚ç‚¹
                    conn = taos.connect(
                        host=self.host,
                        user=self.user,
                        password=self.passwd,
                        config=self.conf
                    )
                    cursor = conn.cursor()
                    
                    print("\n=== æ£€æŸ¥é›†ç¾¤é…ç½® ===")
                    
                    # æŸ¥è¯¢å¹¶æ˜¾ç¤ºèŠ‚ç‚¹çŠ¶æ€
                    try:
                        # æŸ¥è¯¢ dnodes ä¿¡æ¯
                        print("\nDNode ä¿¡æ¯:")
                        cursor.execute("show dnodes")
                        result = cursor.fetchall()
                        for row in result:
                            print(f"ID: {row[0]}, endpoint: {row[1]}, status: {row[4]}")
                        
                        # æŸ¥è¯¢ mnodes ä¿¡æ¯
                        print("\nMNode ä¿¡æ¯:")
                        cursor.execute("show mnodes")
                        result = cursor.fetchall()
                        if result:
                            for row in result:
                                print(f"ID: {row[0]}, endpoint: {row[1]}, role: {row[2]}, status: {row[3]}")
                        else:
                            print("å½“å‰ç³»ç»Ÿä¸­æ²¡æœ‰é¢å¤–çš„mnode")
                        
                        # æŸ¥è¯¢ snodes ä¿¡æ¯
                        print("\nSNode ä¿¡æ¯:")
                        cursor.execute("show snodes")
                        result = cursor.fetchall()
                        if result:
                            for row in result:
                                print(f"ID: {row[0]}, endpoint: {row[1]}, create_time: {row[2]}")
                        else:
                            print("å½“å‰ç³»ç»Ÿä¸­æ²¡æœ‰snode")
                            
                    except Exception as e:
                        print(f"æŸ¥è¯¢èŠ‚ç‚¹ä¿¡æ¯å¤±è´¥: {str(e)}")
                    
                    # å…³é—­è¿æ¥
                    cursor.close()
                    conn.close()
                    
                except Exception as e:
                    print(f"æ£€æŸ¥é›†ç¾¤é…ç½®å¤±è´¥: {str(e)}")
                    
            print(f"\n{mode_desc}æ¨¡å¼ taosd è¿›ç¨‹å¯åŠ¨å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"å¯åŠ¨ taosd è¿›ç¨‹æ—¶å‡ºé”™: {str(e)}")
            return False
        return True
        
    def backup_cluster_data(self):
        """å¤‡ä»½é›†ç¾¤æ•°æ® - æ”¯æŒå•èŠ‚ç‚¹å’Œé›†ç¾¤æ¨¡å¼"""
        try:
            mode_desc = "å•èŠ‚ç‚¹" if self.deployment_mode == 'single' else "é›†ç¾¤"
            print(f"å¼€å§‹å¤‡ä»½{mode_desc}æ•°æ®...")
            
            # å…ˆåœæ­¢æ‰€æœ‰ taosd è¿›ç¨‹
            print("åœæ­¢æ‰€æœ‰ taosd è¿›ç¨‹...")
            subprocess.run('pkill -15 taosd', shell=True)
            time.sleep(5)  # ç­‰å¾…è¿›ç¨‹å®Œå…¨åœæ­¢
            
            # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å®Œå…¨åœæ­¢
            if subprocess.run('pgrep -x taosd', shell=True, stdout=subprocess.PIPE).stdout:
                print("ç­‰å¾… taosd è¿›ç¨‹åœæ­¢...")
                time.sleep(5)
                # å†æ¬¡æ£€æŸ¥ï¼Œå¦‚æœè¿˜æœ‰è¿›ç¨‹åˆ™å¼ºåˆ¶ç»ˆæ­¢
                if subprocess.run('pgrep -x taosd', shell=True, stdout=subprocess.PIPE).stdout:
                    print("å¼ºåˆ¶ç»ˆæ­¢ taosd è¿›ç¨‹...")
                    subprocess.run('pkill -9 taosd', shell=True)
                    time.sleep(2)
                    
            # åˆ›å»ºå¤‡ä»½ç›®å½•
            if os.path.exists(self.backup_dir):
                print(f"æ¸…ç†å·²å­˜åœ¨çš„å¤‡ä»½ç›®å½•: {self.backup_dir}")
                shutil.rmtree(self.backup_dir)
            os.makedirs(self.backup_dir)
            print(f"åˆ›å»ºå¤‡ä»½ç›®å½•: {self.backup_dir}")
            
            # æ ¹æ®éƒ¨ç½²æ¨¡å¼ç¡®å®šéœ€è¦å¤‡ä»½çš„èŠ‚ç‚¹
            nodes_to_backup = []
            if self.deployment_mode == 'single':
                nodes_to_backup = ['dnode1']
                print("å•èŠ‚ç‚¹æ¨¡å¼: ä»…å¤‡ä»½ dnode1 æ•°æ®")
            else:
                nodes_to_backup = ['dnode1', 'dnode2', 'dnode3']
                print("é›†ç¾¤æ¨¡å¼: å¤‡ä»½ dnode1, dnode2, dnode3 æ•°æ®")
                
            # åªå¤‡ä»½dataç›®å½•ï¼Œä¸å¤‡ä»½logå’Œconf
            for dnode in nodes_to_backup:
                src_data_dir = os.path.join(self.stream_perf_test_dir, dnode, 'data')
                dst_data_dir = os.path.join(self.backup_dir, dnode, 'data')
                
                if os.path.exists(src_data_dir):
                    print(f"\nå¤‡ä»½ {dnode}/data ç›®å½•...")
                    try:
                        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
                        os.makedirs(os.path.dirname(dst_data_dir), exist_ok=True)
                        
                        # ä½¿ç”¨ rsync æ’é™¤ä¸´æ—¶æ–‡ä»¶å’Œsocketæ–‡ä»¶
                        cmd = f'rsync -av --exclude="*.sock*" --exclude="*.tmp" --exclude="*.lock" {src_data_dir}/ {dst_data_dir}/'
                        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
                        
                        if result.returncode == 0:
                            print(f"å®Œæˆå¤‡ä»½: {dst_data_dir}")
                            
                            # æ˜¾ç¤ºå¤‡ä»½æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
                            try:
                                # è·å–å¤‡ä»½ç›®å½•å¤§å°
                                du_result = subprocess.run(f'du -sh {dst_data_dir}', shell=True, 
                                                        capture_output=True, text=True)
                                if du_result.returncode == 0:
                                    size_info = du_result.stdout.strip().split()[0]
                                    print(f"  å¤‡ä»½å¤§å°: {size_info}")
                                    
                                # ç»Ÿè®¡æ–‡ä»¶æ•°é‡
                                find_result = subprocess.run(f'find {dst_data_dir} -type f | wc -l', 
                                                            shell=True, capture_output=True, text=True)
                                if find_result.returncode == 0:
                                    file_count = find_result.stdout.strip()
                                    print(f"  æ–‡ä»¶æ•°é‡: {file_count}")
                            except Exception as e:
                                print(f"  ç»Ÿè®¡å¤‡ä»½ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
                        else:
                            # rsync å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ cp å‘½ä»¤
                            print(f"rsync å¤±è´¥ï¼Œä½¿ç”¨ cp å‘½ä»¤å¤‡ä»½...")
                            cmd = f'cp -r {src_data_dir} {dst_data_dir}'
                            subprocess.run(cmd, shell=True, check=True)
                            print(f"å®Œæˆå¤‡ä»½: {dst_data_dir}")
                            
                    except subprocess.CalledProcessError as e:
                        print(f"å¤‡ä»½ {dnode}/data å¤±è´¥: {str(e)}")
                        return False
                    except Exception as e:
                        print(f"å¤‡ä»½ {dnode}/data æ—¶å‡ºé”™: {str(e)}")
                        return False
                else:
                    print(f"è­¦å‘Š: {dnode}/data ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡å¤‡ä»½")
            
            # åˆ›å»ºå¤‡ä»½ä¿¡æ¯æ–‡ä»¶
            backup_info_file = os.path.join(self.backup_dir, 'backup_info.txt')
            try:
                with open(backup_info_file, 'w') as f:
                    f.write(f"TDengine æ•°æ®å¤‡ä»½ä¿¡æ¯\n")
                    f.write(f"=" * 50 + "\n")
                    f.write(f"å¤‡ä»½æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"éƒ¨ç½²æ¨¡å¼: {self.deployment_mode}\n")
                    f.write(f"å¤‡ä»½èŠ‚ç‚¹: {', '.join(nodes_to_backup)}\n")
                    f.write(f"å¤‡ä»½å†…å®¹: ä»…æ•°æ®ç›®å½• (data)\n")
                    f.write(f"å­è¡¨æ•°é‡: {self.table_count}\n")
                    f.write(f"æ¯è¡¨è®°å½•æ•°: {self.histroy_rows}\n")
                    f.write(f"æ•°æ®ä¹±åºç‡: {self.disorder_ratio}\n")
                    f.write(f"vgroupsæ•°: {self.vgroups}\n")
                    f.write(f"=" * 50 + "\n")
                    
                    # è®°å½•æ¯ä¸ªèŠ‚ç‚¹çš„å¤‡ä»½çŠ¶æ€
                    for dnode in nodes_to_backup:
                        dst_data_dir = os.path.join(self.backup_dir, dnode, 'data')
                        if os.path.exists(dst_data_dir):
                            f.write(f"{dnode}: å¤‡ä»½æˆåŠŸ\n")
                        else:
                            f.write(f"{dnode}: å¤‡ä»½å¤±è´¥\n")
                            
                print(f"å¤‡ä»½ä¿¡æ¯å·²ä¿å­˜åˆ°: {backup_info_file}")
            except Exception as e:
                print(f"åˆ›å»ºå¤‡ä»½ä¿¡æ¯æ–‡ä»¶å¤±è´¥: {str(e)}")
            
            print(f"\n=== {mode_desc}æ•°æ®å¤‡ä»½å®Œæˆ! ===")
            print(f"å¤‡ä»½ç›®å½•: {self.backup_dir}")
            return True
            
            # # ç›´æ¥å¤åˆ¶æ•´ä¸ªèŠ‚ç‚¹ç›®å½•
            # for dnode in nodes_to_backup:
            #     src_dir = os.path.join(self.stream_perf_test_dir, dnode, 'data')
            #     dst_dir = os.path.join(self.backup_dir, dnode, 'data')
                
            #     print(f"\nå¤‡ä»½ {dnode} ç›®å½•...")
            #     try:
            #         # ä½¿ç”¨ rsync æ’é™¤ socket æ–‡ä»¶
            #         cmd = f'rsync -av --exclude="*.sock*" {src_dir}/ {dst_dir}/'
            #         subprocess.run(cmd, shell=True, check=True)
            #         print(f"å®Œæˆå¤‡ä»½: {dst_dir}")
            #     except subprocess.CalledProcessError:
            #         # å¦‚æœ rsync å¤±è´¥,ä½¿ç”¨ cp å‘½ä»¤
            #         print(f"rsync å¤±è´¥,ä½¿ç”¨ cp å‘½ä»¤å¤‡ä»½...")
            #         shutil.copytree(src_dir, dst_dir, symlinks=True, 
            #                     ignore=shutil.ignore_patterns('*.sock*'))
            #         print(f"å®Œæˆå¤‡ä»½: {dst_dir}")
            
            # print("\n=== é›†ç¾¤æ•°æ®å¤‡ä»½å®Œæˆ! ===")
            # print(f"å¤‡ä»½ç›®å½•: {self.backup_dir}")
            # return True
    
        except Exception as e:
            print(f"å¤‡ä»½æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return False
        finally:
            # é‡æ–°å¯åŠ¨ taosd è¿›ç¨‹
            if not self.start_taosd_processes():
                print("è­¦å‘Š: taosd è¿›ç¨‹å¯åŠ¨å¤±è´¥")
        
    def restore_cluster_data(self):
        """ä»å¤‡ä»½æ¢å¤é›†ç¾¤æ•°æ® - æ”¯æŒå•èŠ‚ç‚¹å’Œé›†ç¾¤æ¨¡å¼"""
        try:
            if not os.path.exists(self.backup_dir):
                raise Exception(f"é”™è¯¯: å¤‡ä»½ç›®å½•ä¸å­˜åœ¨: {self.backup_dir}")
                
            # è¯»å–å¤‡ä»½ä¿¡æ¯
            backup_info_file = os.path.join(self.backup_dir, 'backup_info.txt')
            backup_mode = None
            if os.path.exists(backup_info_file):
                try:
                    with open(backup_info_file, 'r') as f:
                        content = f.read()
                        # ä»å¤‡ä»½ä¿¡æ¯ä¸­æå–éƒ¨ç½²æ¨¡å¼
                        for line in content.split('\n'):
                            if line.startswith('éƒ¨ç½²æ¨¡å¼:'):
                                backup_mode = line.split(':')[1].strip()
                                break
                except Exception as e:
                    print(f"è¯»å–å¤‡ä»½ä¿¡æ¯å¤±è´¥: {str(e)}")
            
            current_mode_desc = "å•èŠ‚ç‚¹" if self.deployment_mode == 'single' else "é›†ç¾¤"
            backup_mode_desc = "å•èŠ‚ç‚¹" if backup_mode == 'single' else "é›†ç¾¤" if backup_mode == 'cluster' else "æœªçŸ¥"
            
            print(f"å¼€å§‹æ¢å¤æ•°æ®...")
            print(f"å½“å‰æ¨¡å¼: {current_mode_desc}")
            if backup_mode:
                print(f"å¤‡ä»½æ¨¡å¼: {backup_mode_desc}")
                if backup_mode != self.deployment_mode:
                    print(f"è­¦å‘Š: å½“å‰æ¨¡å¼({self.deployment_mode})ä¸å¤‡ä»½æ¨¡å¼({backup_mode})ä¸åŒ¹é…!")
                    response = input("æ˜¯å¦ç»§ç»­æ¢å¤? (y/N): ")
                    if response.lower() != 'y':
                        print("æ¢å¤æ“ä½œå·²å–æ¶ˆ")
                        return False
            
            # åœæ­¢ç°æœ‰taosdè¿›ç¨‹
            subprocess.run('pkill -15 taosd', shell=True)
            time.sleep(5)
            # ç¡®ä¿è¿›ç¨‹å®Œå…¨åœæ­¢
            while subprocess.run('pgrep -x taosd', shell=True, stdout=subprocess.PIPE).stdout:
                print("ç­‰å¾… taosd è¿›ç¨‹åœæ­¢...")
                time.sleep(2)
                subprocess.run('pkill -9 taosd', shell=True)
                
            # æ ¹æ®å½“å‰éƒ¨ç½²æ¨¡å¼ç¡®å®šéœ€è¦æ¢å¤çš„èŠ‚ç‚¹
            nodes_to_restore = []
            if self.deployment_mode == 'single':
                nodes_to_restore = ['dnode1']
                print("å•èŠ‚ç‚¹æ¨¡å¼: ä»…æ¢å¤ dnode1 æ•°æ®")
            else:
                nodes_to_restore = ['dnode1', 'dnode2', 'dnode3']
                print("é›†ç¾¤æ¨¡å¼: æ¢å¤ dnode1, dnode2, dnode3 æ•°æ®")
                
            # æ¢å¤æ¯ä¸ªèŠ‚ç‚¹çš„æ•°æ®
            restored_count = 0
            for dnode in nodes_to_restore:
                backup_data_dir = os.path.join(self.backup_dir, dnode, 'data')
                cluster_data_dir = os.path.join(self.stream_perf_test_dir, dnode, 'data')
                
                if not os.path.exists(backup_data_dir):
                    print(f"è­¦å‘Š: å¤‡ä»½ç›®å½•ä¸­æœªæ‰¾åˆ° {dnode}/data")
                    # å¯¹äºé›†ç¾¤æ¨¡å¼ï¼Œå¦‚æœæŸäº›èŠ‚ç‚¹çš„å¤‡ä»½ä¸å­˜åœ¨ï¼Œè·³è¿‡ä½†ç»§ç»­
                    if self.deployment_mode == 'cluster':
                        continue
                    else:
                        # å¯¹äºå•èŠ‚ç‚¹æ¨¡å¼ï¼Œå¦‚æœdnode1çš„å¤‡ä»½ä¸å­˜åœ¨ï¼Œåˆ™å¤±è´¥
                        if dnode == 'dnode1':
                            raise Exception(f"å•èŠ‚ç‚¹æ¨¡å¼ä¸‹å¿…é¡»çš„ {dnode}/data å¤‡ä»½ä¸å­˜åœ¨")
                        continue
                        
                print(f"\nè¿˜åŸ {dnode}/data ...")
                
                # æ˜¾ç¤ºå¤‡ä»½ç›®å½•ä¿¡æ¯
                try:
                    du_result = subprocess.run(f'du -sh {backup_data_dir}', shell=True, 
                                            capture_output=True, text=True)
                    if du_result.returncode == 0:
                        backup_size = du_result.stdout.strip().split()[0]
                        print(f"å¤‡ä»½æ•°æ®å¤§å°: {backup_size}")
                except:
                    pass
                
                # æ¸…ç†ç°æœ‰æ•°æ®ç›®å½•
                if os.path.exists(cluster_data_dir):
                    print(f"æ¸…ç†ç°æœ‰æ•°æ®ç›®å½•: {cluster_data_dir}")
                    shutil.rmtree(cluster_data_dir)
                
                # ç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(cluster_data_dir), exist_ok=True)
                
                # æ¢å¤æ•°æ®ç›®å½•
                try:
                    shutil.copytree(backup_data_dir, cluster_data_dir, symlinks=True)
                    print(f"å®Œæˆè¿˜åŸ: {cluster_data_dir}")
                    restored_count += 1
                    
                    # æ˜¾ç¤ºæ¢å¤åçš„ä¿¡æ¯
                    try:
                        du_result = subprocess.run(f'du -sh {cluster_data_dir}', shell=True, 
                                                capture_output=True, text=True)
                        if du_result.returncode == 0:
                            restored_size = du_result.stdout.strip().split()[0]
                            print(f"æ¢å¤æ•°æ®å¤§å°: {restored_size}")
                    except:
                        pass
                        
                except Exception as e:
                    print(f"è¿˜åŸ {dnode}/data æ—¶å‡ºé”™: {str(e)}")
                    return False
            
            if restored_count == 0:
                print("é”™è¯¯: æ²¡æœ‰æˆåŠŸæ¢å¤ä»»ä½•èŠ‚ç‚¹çš„æ•°æ®")
                return False
                
            print(f"\n=== æ•°æ®è¿˜åŸå®Œæˆ! ===")
            print(f"æˆåŠŸæ¢å¤ {restored_count} ä¸ªèŠ‚ç‚¹çš„æ•°æ®")
            
            # é‡å¯ taosd è¿›ç¨‹
            return self.start_taosd_processes()
            
        except Exception as e:
            print(f"\nè¿˜åŸæ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return False
        
    def do_test_stream_with_restored_data(self):
        """æ¢å¤æ•°æ®åæµ‹è¯•æŒ‡å®šçš„æµè®¡ç®—SQL"""
        
        loader = None
        monitor_thread = None
        delay_monitor_thread = None
        conn = None
        cursor = None
    
        try:
            print("=== æ¢å¤æ•°æ®åæµ‹è¯•æµè®¡ç®— ===")
            
            # å…ˆæ¢å¤æ•°æ®
            print("1. æ¢å¤å†å²æ•°æ®...")
            if not self.restore_cluster_data():
                raise Exception("æ¢å¤é›†ç¾¤æ•°æ®å¤±è´¥")
            
            print("2. åˆ›å»ºç›®æ ‡æ•°æ®åº“...")
            # åˆ›å»ºstream_toæ•°æ®åº“
            if not self.create_database('stream_to'):
                raise Exception("åˆ›å»ºstream_toæ•°æ®åº“å¤±è´¥")
            
            print("3. å¼€å§‹æµè®¡ç®—æµ‹è¯•...")
            print(f"SQLç±»å‹: {self.sql_type}")
            print(f"æµæ•°é‡: {self.stream_num}")
            print(f"è‡ªå®šä¹‰SQL: {'æ˜¯' if self.stream_sql else 'å¦'}")
            
            # è·å–æ•°æ®åº“è¿æ¥
            conn, cursor = self.get_connection()
            
            
            # è¿æ¥åˆ°æºæ•°æ®åº“
            print("è¿æ¥åˆ°æºæ•°æ®åº“ stream_from...")
            cursor.execute('use stream_from')
            
            # æ£€æŸ¥æºæ•°æ®çŠ¶æ€
            print("æ£€æŸ¥æºæ•°æ®çŠ¶æ€...")
            cursor.execute("select count(*) from information_schema.ins_tables where db_name='stream_from'")
            table_count = cursor.fetchall()[0][0]
            
            cursor.execute("select count(*) from stream_from.stb")
            record_count = cursor.fetchall()[0][0]
            
            print(f"æºæ•°æ®ç»Ÿè®¡: {table_count} å¼ è¡¨, {record_count:,} æ¡è®°å½•")
            
            # è·å– SQL æ¨¡æ¿å¹¶åˆ›å»ºæµ
            sql_templates = self.stream_sql 
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºæ‰¹é‡æ‰§è¡Œ
            if isinstance(sql_templates, dict):
                print("\n=== å¼€å§‹æ‰¹é‡åˆ›å»ºæµ ===")
                for sql_name, sql_template in sql_templates.items():
                    try:
                        print(f"\nåˆ›å»ºæµ {sql_name}:")
                        print(sql_template)
                        cursor.execute(sql_template)
                        print(f"æµ {sql_name} åˆ›å»ºæˆåŠŸ")
                    except Exception as e:
                        print(f"åˆ›å»ºæµ {sql_name} å¤±è´¥: {str(e)}")
            else:
                # å•ä¸ªæµçš„åˆ›å»º
                print("\n=== å¼€å§‹åˆ›å»ºæµ ===")
                print("æ‰§è¡Œæµè®¡ç®—SQL:")
                print("-" * 60)
                print(sql_templates)
                print("-" * 60)
                
                start_time = time.time()
                cursor.execute(sql_templates)
                create_time = time.time() - start_time
                
                print(f"æµåˆ›å»ºå®Œæˆ! è€—æ—¶: {create_time:.2f}ç§’")
            
            # å¯åŠ¨ç³»ç»Ÿç›‘æ§
            print("\nå¼€å§‹ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ...")
            loader = MonitorSystemLoad(
                name_pattern='taosd -c', 
                count=self.runtime * 60,
                perf_file='/tmp/perf-stream-test.log',
                interval=self.monitor_interval,
                deployment_mode=self.deployment_mode
            )
            
            # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œç›‘æ§
            monitor_thread = threading.Thread(
                target=loader.get_proc_status,
                name="StreamTestMonitor"
            )
            monitor_thread.daemon = True
            monitor_thread.start()
            
            # å¯åŠ¨æµå»¶è¿Ÿç›‘æ§
            delay_monitor_thread = self.start_stream_delay_monitor()
            
            # ç­‰å¾…æµè®¡ç®—å®Œæˆ
            print(f"ç­‰å¾…æµè®¡ç®—å®Œæˆ... (ç›‘æ§æ—¶é—´: {self.runtime}åˆ†é’Ÿ)")
            
            # å®šæœŸæ£€æŸ¥æµè®¡ç®—è¿›åº¦
            check_interval = 30  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
            total_checks = (self.runtime * 60) // check_interval
            
            for i in range(total_checks):
                time.sleep(check_interval)
                
                try:
                    # æ£€æŸ¥ç›®æ ‡è¡¨æ•°æ®é‡
                    cursor.execute("use stream_to")
                    
                    # æŸ¥è¯¢ç›®æ ‡è¡¨æ•°é‡å’Œè®°å½•æ•°
                    cursor.execute("select count(*) from information_schema.ins_tables where db_name='stream_to'")
                    target_table_count = cursor.fetchall()[0][0]
                    
                    if target_table_count > 0:
                        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®å†™å…¥
                        cursor.execute("show tables")
                        tables = cursor.fetchall()
                        
                        total_target_records = 0
                        for table in tables:
                            table_name = table[0]
                            try:
                                cursor.execute(f"select count(*) from stream_to.{table_name}")
                                count = cursor.fetchall()[0][0]
                                total_target_records += count
                            except:
                                continue
                        
                        progress = (i + 1) / total_checks * 100
                        print(f"è¿›åº¦: {progress:.1f}% - ç›®æ ‡è¡¨: {target_table_count}, ç›®æ ‡è®°å½•: {total_target_records:,}")
                    else:
                        progress = (i + 1) / total_checks * 100
                        print(f"è¿›åº¦: {progress:.1f}% - ç­‰å¾…æµè®¡ç®—å¼€å§‹...")
                        
                    cursor.execute('use stream_from')  # åˆ‡æ¢å›æºæ•°æ®åº“
                    
                except Exception as e:
                    print(f"æ£€æŸ¥è¿›åº¦æ—¶å‡ºé”™: {str(e)}")
                    
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è¿è¡Œæ—¶é—´é™åˆ¶
                if i >= total_checks - 1:
                    print(f"\nå·²è¾¾åˆ°è¿è¡Œæ—¶é—´é™åˆ¶ ({self.runtime} åˆ†é’Ÿ)")
                    break
            
            print("\næµè®¡ç®—æµ‹è¯•å®Œæˆ!")
            
            # æ˜¾ç¤ºæœ€ç»ˆç»“æœç»Ÿè®¡
            print("\n=== æµè®¡ç®—ç»“æœç»Ÿè®¡ ===")
            try:
                cursor.execute("use stream_to")
                cursor.execute("select count(*) from information_schema.ins_tables where db_name='stream_to'")
                final_table_count = cursor.fetchall()[0][0]
                
                cursor.execute("show tables")
                tables = cursor.fetchall()
                
                final_total_records = 0
                table_details = []
                for table in tables:
                    table_name = table[0]
                    try:
                        cursor.execute(f"select count(*) from stream_to.{table_name}")
                        count = cursor.fetchall()[0][0]
                        final_total_records += count
                        table_details.append(f"è¡¨ {table_name}: {count:,} æ¡è®°å½•")
                    except Exception as e:
                        table_details.append(f"è¡¨ {table_name}: æŸ¥è¯¢å¤±è´¥ - {str(e)}")
                
                print(f"æ€»è®¡: {final_table_count} å¼ ç›®æ ‡è¡¨, {final_total_records:,} æ¡ç»“æœè®°å½•")
                
                # æ˜¾ç¤ºå‰å‡ å¼ è¡¨çš„è¯¦æƒ…
                for detail in table_details[:5]:
                    print(f"  {detail}")
                if len(table_details) > 5:
                    print(f"  ... è¿˜æœ‰ {len(table_details) - 5} å¼ è¡¨")
                
                # è®¡ç®—å¤„ç†æ•ˆç‡
                if record_count > 0:
                    processing_ratio = final_total_records / record_count * 100
                    print(f"å¤„ç†æ•ˆç‡: {processing_ratio:.2f}% ({final_total_records:,}/{record_count:,})")
                
            except Exception as e:
                print(f"ç»Ÿè®¡ç»“æœæ—¶å‡ºé”™: {str(e)}")
                
            print("\n=== æµè®¡ç®—æµ‹è¯•å®Œæˆ ===")
            print("æ€§èƒ½ç›‘æ§æ•°æ®å·²ä¿å­˜åˆ°: /tmp/perf-stream-test-*.log")
            print("å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹ç›‘æ§æ•°æ®:")
            print("cat /tmp/perf-stream-test-all.log")
            
        except Exception as e:
            print(f"æµè®¡ç®—æµ‹è¯•å¤±è´¥: {str(e)}")
            raise
        finally:
            # å®‰å…¨åœ°å…³é—­æ•°æ®åº“è¿æ¥
            try:
                if cursor:
                    cursor.close()
                if conn:
                    conn.close()
            except Exception as e:
                print(f"å…³é—­æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {str(e)}")
            
            # å®‰å…¨åœ°åœæ­¢ç›‘æ§
            try:
                if loader:
                    print("ä¸»åŠ¨åœæ­¢ç³»ç»Ÿç›‘æ§...")
                    loader.stop()
            except Exception as e:
                print(f"åœæ­¢ç³»ç»Ÿç›‘æ§æ—¶å‡ºé”™: {str(e)}")
            
            # å®‰å…¨åœ°ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
            try:
                if monitor_thread:
                    print("ç­‰å¾…ç›‘æ§æ•°æ®æ”¶é›†å®Œæˆ...")
                    monitor_thread.join(timeout=15)
                    
                    if monitor_thread.is_alive():
                        print("ç›‘æ§çº¿ç¨‹æœªåœ¨é¢„æœŸæ—¶é—´å†…ç»“æŸï¼Œå¼ºåˆ¶ç»§ç»­...")
                    else:
                        print("ç›‘æ§çº¿ç¨‹å·²æ­£å¸¸ç»“æŸ")
            except Exception as e:
                print(f"ç­‰å¾…ç›‘æ§çº¿ç¨‹æ—¶å‡ºé”™: {str(e)}")
                
            # å®‰å…¨åœ°ç­‰å¾…å»¶è¿Ÿç›‘æ§çº¿ç¨‹ç»“æŸ
            try:
                if delay_monitor_thread:
                    print("ç­‰å¾…å»¶è¿Ÿç›‘æ§å®Œæˆ...")
                    delay_monitor_thread.join(timeout=10)
                    if delay_monitor_thread.is_alive():
                        print("å»¶è¿Ÿç›‘æ§çº¿ç¨‹æœªåœ¨é¢„æœŸæ—¶é—´å†…ç»“æŸï¼Œå¼ºåˆ¶ç»§ç»­...")
                    else:
                        print("å»¶è¿Ÿç›‘æ§çº¿ç¨‹å·²æ­£å¸¸ç»“æŸ")
            except Exception as e:
                print(f"ç­‰å¾…å»¶è¿Ÿç›‘æ§çº¿ç¨‹æ—¶å‡ºé”™: {str(e)}")
            
            # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
            try:
                if self.check_stream_delay:
                    print(f"\næµå»¶è¿Ÿç›‘æ§æŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.delay_log_file}")
                    self.print_final_delay_summary()
            except Exception as e:
                print(f"æ‰“å°æœ€ç»ˆæŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}")

        
    def create_test_data(self):
        """åˆ›å»ºå¹¶å¤‡ä»½æµ‹è¯•æ•°æ®"""
        try:
            print("å¼€å§‹ç”Ÿæˆæµ‹è¯•æ•°æ®...")
            self.prepare_env()
            self.prepare_source_from_data()
            # è¿è¡Œæ•°æ®ç”Ÿæˆ
            proc = subprocess.Popen('taosBenchmark --f /tmp/stream_from.json',
                                    stdout=subprocess.PIPE, shell=True, text=True)
            
            # ç­‰å¾…æ•°æ®å†™å…¥å®Œæˆ
            conn = taos.connect(host=self.host, user=self.user, 
                                password=self.passwd, config=self.conf)
            cursor = conn.cursor()
            
            if not self.wait_for_data_ready(cursor,self.table_count, self.histroy_rows):
                print("æ•°æ®ç”Ÿæˆå¤±è´¥")
                return False
                
            # å¤‡ä»½æ•°æ®
            if self.backup_cluster_data():
                print("æµ‹è¯•æ•°æ®å·²ç”Ÿæˆå¹¶å¤‡ä»½")
                return True
                
            print("æµ‹è¯•æ•°æ®åˆ›å»ºå®Œæˆ")
            return True
            
        except Exception as e:
            print(f"åˆ›å»ºæµ‹è¯•æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return False
        
    def prepare_env(self):
        """
        æ¸…ç†ç¯å¢ƒå¹¶å¯åŠ¨TDengineæœåŠ¡
        æ”¯æŒå•èŠ‚ç‚¹(single)å’Œé›†ç¾¤(cluster)ä¸¤ç§æ¨¡å¼
        """
        try:
            print_title(f"\n=== å¼€å§‹å‡†å¤‡ç¯å¢ƒ (æ¨¡å¼: {self.deployment_mode}) ===")
            # åœæ­¢å·²å­˜åœ¨çš„taosdè¿›ç¨‹
            print_info("åœæ­¢ç°æœ‰taosdè¿›ç¨‹")
            self.stop_taosd()
            
            # æ£€æŸ¥å¹¶å¤„ç†é›†ç¾¤æ ¹ç›®å½•
            if os.path.exists(self.stream_perf_test_dir):
                print_info(f"æ¸…ç†å·²å­˜åœ¨çš„é›†ç¾¤ç›®å½•: {self.stream_perf_test_dir}")
                subprocess.run(f'rm -rf {self.stream_perf_test_dir}/*', shell=True)
            else:
                print_info(f"åˆ›å»ºæ–°çš„é›†ç¾¤ç›®å½•: {self.stream_perf_test_dir}")
                subprocess.run(f'mkdir -p {self.stream_perf_test_dir}', shell=True)
            
                
            for instance in self.instances:
                # åˆ›å»ºå¿…è¦çš„ç›®å½•
                for dir_type in ['data', 'log', 'conf']:
                    dir_path = f"{self.stream_perf_test_dir}/{instance['name']}/{dir_type}"
                    subprocess.run(f'mkdir -p {dir_path}', shell=True)
                
                # æ¸…ç†æ•°æ®ç›®å½•
                data_dir = f"{self.stream_perf_test_dir}/{instance['name']}/data"
                subprocess.run(f'rm -rf {data_dir}', shell=True)
                print(f"åˆ›å»ºç›®å½•: {dir_path}")
                
                # æ ¹æ®éƒ¨ç½²æ¨¡å¼ç”Ÿæˆä¸åŒçš„é…ç½®æ–‡ä»¶
                if self.deployment_mode == 'single':
                    # å•èŠ‚ç‚¹é…ç½®ç”Ÿæˆé…ç½®æ–‡ä»¶
                    cfg_content = f"""
firstEP         localhost:6030
fqdn            localhost
serverPort      {instance['port']}
supportVnodes   100
dataDir         {instance['data_dir']}
logDir          {instance['log_dir']}
asyncLog        1
debugFlag       {self.debug_flag}
numOfLogLines   {self.num_of_log_lines}
"""
                else:
                    # é›†ç¾¤é…ç½®ç”Ÿæˆé…ç½®æ–‡ä»¶
                    cfg_content = f"""
firstEP         localhost:6030
secondEP        localhost:7030
fqdn            localhost
serverPort      {instance['port']}
supportVnodes   50
dataDir         {instance['data_dir']}
logDir          {instance['log_dir']}
asyncLog        1
debugFlag       {self.debug_flag}
numOfLogLines   {self.num_of_log_lines}
"""
                cfg_file = f"{self.stream_perf_test_dir}/{instance['name']}/conf/taos.cfg"
                
                # ä½¿ç”¨ EOF æ–¹å¼å†™å…¥é…ç½®æ–‡ä»¶
                subprocess.run(f"""
cat << 'EOF' > {cfg_file}
{cfg_content}
EOF
""", shell=True)
            print_success("ç¯å¢ƒå‡†å¤‡å®Œæˆï¼Œé…ç½®æ–‡ä»¶å·²ç”Ÿæˆ")
            
            
            # å¯åŠ¨æ‰€æœ‰taosdå®ä¾‹
            self.taosd_processes = []  
            for instance in self.instances:
                cfg_file = f"{self.stream_perf_test_dir}/{instance['name']}/conf/taos.cfg"
                cmd = f'nohup taosd -c {cfg_file} > /dev/null 2>&1 &'
                
                try:
                    process = subprocess.Popen(cmd, shell=True)
                    self.taosd_processes.append({
                        'name': instance,
                        'pid': process.pid,
                        'cfg': cfg_file
                    })
                    print(f"å¯åŠ¨taosdè¿›ç¨‹: {instance}, PID: {process.pid}")
                except Exception as e:
                    print(f"å¯åŠ¨ {instance} å¤±è´¥: {str(e)}")
                    raise
            
            # ç­‰å¾…æœåŠ¡å®Œå…¨å¯åŠ¨
            wait_time = 5 if self.deployment_mode == 'single' else 10
            print(f"ç­‰å¾…æœåŠ¡å¯åŠ¨ ({wait_time}ç§’)...")
            time.sleep(wait_time)             
            self.check_taosd_status()
            
            # é…ç½®é›†ç¾¤
            try:
                # è¿æ¥åˆ°ç¬¬ä¸€ä¸ªèŠ‚ç‚¹
                conn = taos.connect(
                    host=self.host,
                    user=self.user,
                    password=self.passwd,
                    config=self.conf
                )
                cursor = conn.cursor()
                
                if self.deployment_mode == 'cluster':
                    # é›†ç¾¤æ¨¡å¼é…ç½®
                    print("\n=== å¼€å§‹é…ç½®é›†ç¾¤ ===")
                    cluster_cmds = [
                        'create dnode "localhost:7030"',
                        'create dnode "localhost:8030"',
                        'create mnode on dnode 2',
                        'create mnode on dnode 3',
                        'create snode on dnode 3',
                        
                        'create snode on dnode 2',
                        'create snode on dnode 1'
                    ]
                
                    for cmd in cluster_cmds:
                        try:
                            cursor.execute(cmd)
                            time.sleep(1)
                            print(f"æ‰§è¡ŒæˆåŠŸ: {cmd}")
                        except Exception as e:
                            print(f"æ‰§è¡Œå¤±è´¥: {cmd}")
                            print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
                
                    # æŸ¥è¯¢å¹¶æ˜¾ç¤ºé›†ç¾¤çŠ¶æ€
                    print("\né›†ç¾¤èŠ‚ç‚¹ä¿¡æ¯:")
                    print("-" * 50)
                    
                else:
                    # å•èŠ‚ç‚¹æ¨¡å¼é…ç½®
                    print("\n=== é…ç½®å•èŠ‚ç‚¹ç¯å¢ƒ ===")
                    try:
                        cursor.execute('create snode on dnode 1')
                        print("æ‰§è¡ŒæˆåŠŸ: create snode on dnode 1")
                        time.sleep(1)
                    except Exception as e:
                        print(f"åˆ›å»ºsnodeå¤±è´¥: {str(e)}")
                
                # æŸ¥è¯¢å¹¶æ˜¾ç¤ºèŠ‚ç‚¹çŠ¶æ€ï¼ˆå•èŠ‚ç‚¹å’Œé›†ç¾¤éƒ½æ˜¾ç¤ºï¼‰
                print(f"\n=== {self.deployment_mode.upper()}æ¨¡å¼èŠ‚ç‚¹ä¿¡æ¯ ===")
                print("-" * 60)
                
                try:
                    # æŸ¥è¯¢ dnodes ä¿¡æ¯
                    print("\nDNode ä¿¡æ¯:")
                    cursor.execute("show dnodes")
                    result = cursor.fetchall()
                    for row in result:
                        print(f"ID: {row[0]}, endpoint: {row[1]}, status: {row[4]}")
                    
                    # æŸ¥è¯¢ mnodes ä¿¡æ¯
                    print("\nMNode ä¿¡æ¯:")
                    cursor.execute("show mnodes")
                    result = cursor.fetchall()
                    if result:
                        for row in result:
                            print(f"ID: {row[0]}, endpoint: {row[1]}, role: {row[2]}, status: {row[3]}")
                    else:
                        print("å½“å‰ç³»ç»Ÿä¸­æ²¡æœ‰é¢å¤–çš„mnode")
                    
                    # æŸ¥è¯¢ snodes ä¿¡æ¯
                    print("\nSNode ä¿¡æ¯:")
                    cursor.execute("show snodes")
                    result = cursor.fetchall()
                    if result:
                        for row in result:
                            print(f"ID: {row[0]}, endpoint: {row[1]}, create_time: {row[2]}")
                    else:
                        print("å½“å‰ç³»ç»Ÿä¸­æ²¡æœ‰snode")
                        
                except Exception as e:
                    print(f"æŸ¥è¯¢èŠ‚ç‚¹ä¿¡æ¯å¤±è´¥: {str(e)}")
                
                print("-" * 50)
                
                # å…³é—­è¿æ¥
                cursor.close()
                conn.close()
                
                print(f"{self.deployment_mode.upper()}æ¨¡å¼é…ç½®å®Œæˆ")
                
            except Exception as e:
                print_error(f"èŠ‚ç‚¹é…ç½®å¤±è´¥: {str(e)}")
                raise
                
        except Exception as e:
            print_error(f"ç¯å¢ƒå‡†å¤‡å¤±è´¥: {str(e)}")
            raise
        
    def prepare_source_from_data(self) -> dict:
        json_data = {
            "filetype": "insert",
            "cfgdir": f"{self.stream_perf_test_dir}/dnode1/conf",
            "host": "localhost",
            "port": 6030,
            "rest_port": 6041,
            "user": "root",
            "password": "taosdata",
            "thread_count": 50,
            "create_table_thread_count": 5,
            "result_file": "/tmp/taosBenchmark_result.log",
            "confirm_parameter_prompt": "no",
            "insert_interval": 10,
            "num_of_records_per_req": 1000,
            "max_sql_len": 102400,
            "databases": [
                {
                    "dbinfo": {
                        "name": "stream_from",
                        "drop": "yes",
                        "replica": 1,
                        "duration": 10,
                        "precision": "ms",
                        "keep": 3650,
                        "minRows": 100,
                        "maxRows": 4096,
                        "comp": 2,
                        "dnodes": "1",
                        "vgroups": self.vgroups,
                        "stt_trigger": 2,
                        "WAL_RETENTION_PERIOD": 86400
                    },
                    "super_tables": [
                        {
                            "name": "stb",
                            "child_table_exists": "yes",
                            "childtable_count": self.table_count,
                            "childtable_prefix": "ctb0_",
                            "escape_character": "no",
                            "auto_create_table": "yes",
                            "batch_create_tbl_num": 1000,
                            "data_source": "rand",
                            "insert_mode": "taosc",
                            "interlace_rows": 1,
                            "tcp_transfer": "no",
                            "insert_rows": self.histroy_rows,
                            "partial_col_num": 0,
                            "childtable_limit": 0,
                            "childtable_offset": 0,
                            "rows_per_tbl": 0,
                            "max_sql_len": 1024000,
                            "disorder_ratio": self.disorder_ratio,
                            "disorder_range": 1000,
                            "keep_trying": -1,
                            "timestamp_step": 50,
                            "trying_interval": 10,
                            "start_timestamp": "2025-06-01 00:00:00",
                            "sample_format": "csv",
                            "sample_file": "./sample.csv",
                            "tags_file": "",
                            "columns": [
                                {
                                    "type": "INT",
                                    "count": 1
                                },
                                {
                                    "type": "BIGINT",
                                    "count": 1
                                },
                                {
                                    "type": "DOUBLE",
                                    "count": 1
                                },
                                {
                                    "type": "FLOAT",
                                    "count": 1
                                }
                            ],
                            "tags": [
                                {
                                    "type": "INT",
                                    "count": 1
                                },
                                {
                                    "type": "VARCHAR",
                                    "count": 1,
                                    "len": 32
                                }
                            ]
                        }
                    ]
                }
            ],
            "prepare_rand": 10000,
            "chinese": "no",
            "test_log": "/tmp/testlog/"
        }

        with open('/tmp/stream_from.json', 'w+') as f:
            json.dump(json_data, f, indent=4)
            

    def insert_source_from_data(self) -> dict:
        json_data = {
            "filetype": "insert",
            "cfgdir": f"{self.stream_perf_test_dir}/dnode1/conf",
            "host": "localhost",
            "port": 6030,
            "rest_port": 6041,
            "user": "root",
            "password": "taosdata",
            "thread_count": 50,
            "create_table_thread_count": 5,
            "result_file": "/tmp/taosBenchmark_result.log",
            "confirm_parameter_prompt": "no",
            "insert_interval": 1,
            "num_of_records_per_req": 1000,
            "max_sql_len": 102400,
            "databases": [
                {
                    "dbinfo": {
                        "name": "stream_from",
                        "drop": "no",
                        "replica": 1,
                        "duration": 10,
                        "precision": "ms",
                        "keep": 3650,
                        "minRows": 100,
                        "maxRows": 4096,
                        "comp": 2,
                        "dnodes": "1",
                        "vgroups": self.vgroups,
                        "stt_trigger": 2,
                        "WAL_RETENTION_PERIOD": 86400
                    },
                    "super_tables": [
                        {
                            "name": "stb",
                            "child_table_exists": "yes",
                            "childtable_count": self.table_count,
                            "childtable_prefix": "ctb0_",
                            "escape_character": "no",
                            "auto_create_table": "yes",
                            "batch_create_tbl_num": 1000,
                            "data_source": "rand",
                            "insert_mode": "taosc",
                            "interlace_rows": 1,
                            "tcp_transfer": "no",
                            "insert_rows": self.real_time_batch_rows,
                            "partial_col_num": 0,
                            "childtable_limit": 0,
                            "childtable_offset": 0,
                            "rows_per_tbl": 0,
                            "max_sql_len": 1024000,
                            "disorder_ratio": self.disorder_ratio,
                            "disorder_range": 1000,
                            "keep_trying": -1,
                            "timestamp_step": 50,
                            "trying_interval": 10,
                            "start_timestamp": "2025-06-01 00:00:00",
                            "sample_format": "csv",
                            "sample_file": "./sample.csv",
                            "tags_file": "",
                            "columns": [
                                {
                                    "type": "INT",
                                    "count": 1
                                },
                                {
                                    "type": "BIGINT",
                                    "count": 1
                                },
                                {
                                    "type": "DOUBLE",
                                    "count": 1
                                },
                                {
                                    "type": "FLOAT",
                                    "count": 1
                                }
                            ],
                            "tags": [
                                {
                                    "type": "INT",
                                    "count": 1
                                },
                                {
                                    "type": "VARCHAR",
                                    "count": 1,
                                    "len": 32
                                }
                            ]
                        }
                    ]
                }
            ],
            "prepare_rand": 10000,
            "chinese": "no",
            "test_log": "/tmp/testlog/"
        }

        with open('/tmp/stream_from_insertdata.json', 'w+') as f:
            json.dump(json_data, f, indent=4)
            
    def update_insert_config(self):
        """
        æ›´æ–°æ•°æ®æ’å…¥é…ç½®
        Args:
            real_time_batch_rows: ä¸‹ä¸€è½®è¦æ’å…¥çš„æ•°æ®è¡Œæ•°
        """
        try:
            print("\n=== æ›´æ–°æ•°æ®æ’å…¥é…ç½® ===")
            #print(f"å½“å‰SQLç±»å‹: {self.sql_type}")
            
            # è·å–æœ€æ–°æ—¶é—´æˆ³
            conn = taos.connect(
                host=self.host,
                user=self.user,
                password=self.passwd,
                config=self.conf
            )
            cursor = conn.cursor()
            
            try:
                # æ ¹æ® SQL ç±»å‹å†³å®šæ—¶é—´æˆ³æ›´æ–°æ–¹å¼
                if self.sql_type == 's2_5' or self.sql_type == 's2_11':
                    next_start_time = time.strftime("%Y-%m-%d %H:%M:%S")
                    print(f"æµç±»å‹ä¸º{self.sql_type}ï¼Œä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºèµ·å§‹æ—¶é—´: {next_start_time}")
                
                else:
                    # æŸ¥è¯¢æœ€æ–°æ—¶é—´æˆ³
                    cursor.execute("select last(ts) from stream_from.stb")
                    last_ts = cursor.fetchall()[0][0]
                
                    if not last_ts:
                        raise Exception("æœªèƒ½è·å–åˆ°æœ€æ–°æ—¶é—´æˆ³")
                        
                    # å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼Œå¹¶åŠ ä¸Š1ç§’
                    next_start_time = (last_ts + datetime.timedelta(seconds=1)).strftime("%Y-%m-%d %H:%M:%S")
                    print(f"å½“å‰æœ€æ–°æ—¶é—´æˆ³: {last_ts}")
                    print(f"æ›´æ–°èµ·å§‹æ—¶é—´ä¸º: {next_start_time}")
                
                # è¯»å–ç°æœ‰é…ç½®
                config_file = '/tmp/stream_from_insertdata.json'
                with open(config_file, 'r') as f:
                    content = f.read()
                
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢æ—¶é—´æˆ³
                import re
                new_content = re.sub(
                    r'"start_timestamp":\s*"[^"]*"',
                    f'"start_timestamp": "{next_start_time}"',
                    content,
                    count=1  # åªæ›¿æ¢ç¬¬ä¸€æ¬¡å‡ºç°çš„æ—¶é—´æˆ³
                )
                
                # æ ¼å¼åŒ–å†™å…¥ä»¥ç¡®ä¿ JSON æ ¼å¼æ­£ç¡®
                with open(config_file, 'w') as f:
                    f.write(new_content)
                
                print("é…ç½®æ—¶é—´æˆ³å·²æ›´æ–°")
                return True
                
            except Exception as e:
                print(f"æ›´æ–°é…ç½®æ—¶å‡ºé”™: {str(e)}")
                return False
            finally:
                cursor.close()
                conn.close()
                
        except Exception as e:
            print(f"æ‰§è¡Œæ›´æ–°é…ç½®æ—¶å‡ºé”™: {str(e)}")
            return False

    def wait_for_data_ready(self, cursor, expected_tables, expected_records):
        """ç­‰å¾…æ•°æ®å†™å…¥å®Œæˆ   
        Args:
            cursor: æ•°æ®åº“æ¸¸æ ‡
            expected_tables: é¢„æœŸçš„å­è¡¨æ•°é‡
            expected_records: æ¯ä¸ªå­è¡¨çš„è®°å½•æ•°
            
        Returns:
            bool: æ•°æ®æ˜¯å¦å‡†å¤‡å°±ç»ª
        """
        max_wait_time = 3600  # æœ€å¤§ç­‰å¾…æ—¶é—´(ç§’)
        check_interval = 2   # æ£€æŸ¥é—´éš”(ç§’)
        start_time = time.time()
        
        while True:
            try:
                # æ£€æŸ¥å­è¡¨æ•°é‡
                cursor.execute("select count(*) from information_schema.ins_tables "
                            "where db_name='stream_from'")
                table_count = cursor.fetchall()[0][0]
                
                if table_count < expected_tables:
                    print(f"\rç­‰å¾…å­è¡¨åˆ›å»ºå®Œæˆ... å½“å‰: {table_count}/{expected_tables}", end='')
                    if time.time() - start_time > max_wait_time:
                        print(f"\nç­‰å¾…è¶…æ—¶! å­è¡¨æ•°é‡ä¸è¶³: {table_count}/{expected_tables}")
                        return False
                    time.sleep(check_interval)
                    continue
                
                # ä½¿ç”¨è¶…çº§è¡¨æŸ¥è¯¢æ€»è®°å½•æ•°
                cursor.execute("select count(*) from stream_from.stb")
                total_records = cursor.fetchall()[0][0]
                expected_total = expected_tables * expected_records
                
                if total_records < expected_total:
                    print(f"\rç­‰å¾…æ•°æ®å†™å…¥å®Œæˆ... å½“å‰: {total_records}/{expected_total}", end='')
                    
                    # å¦‚æœæ¥è¿‘è¶…æ—¶ï¼Œæ£€æŸ¥æ¯ä¸ªå­è¡¨çš„è®°å½•æ•°
                    if time.time() - start_time > max_wait_time - 30:  # ç•™å‡º30ç§’ç”¨äºè¯¦ç»†æ£€æŸ¥
                        print("\nå³å°†è¶…æ—¶ï¼Œæ£€æŸ¥å„å­è¡¨æ•°æ®æƒ…å†µ:")
                        insufficient_tables = []
                        
                        for i in range(expected_tables):
                            cursor.execute(f"select count(*) from stream_from.ctb0_{i}")
                            count = cursor.fetchall()[0][0]
                            if count < expected_records:
                                insufficient_tables.append({
                                    'table': f'ctb0_{i}',
                                    'current': count,
                                    'expected': expected_records,
                                    'missing': expected_records - count
                                })
                        
                        if insufficient_tables:
                            print("\nä»¥ä¸‹å­è¡¨æ•°æ®ä¸è¶³:")
                            for table in insufficient_tables:
                                print(f"è¡¨ {table['table']}: "
                                    f"å½“å‰ {table['current']}/{table['expected']}, "
                                    f"ç¼ºå°‘ {table['missing']} æ¡è®°å½•")
                        return False
                    
                    time.sleep(check_interval)
                    continue
                
                print(f"\næ•°æ®å‡†å¤‡å°±ç»ª! å…± {table_count} å¼ å­è¡¨ï¼Œ{total_records} æ¡è®°å½•")
                return True
                
            except Exception as e:
                print(f"\næ£€æŸ¥æ•°æ®æ—¶å‡ºé”™: {str(e)}")
                if time.time() - start_time > max_wait_time:
                    print("ç­‰å¾…è¶…æ—¶!")
                    return False
                time.sleep(check_interval)
                
    def _parse_target_table_from_sql(self, sql):
        """ä»æµSQLä¸­è§£æç›®æ ‡è¡¨å
        
        Args:
            sql: æµçš„SQLè¯­å¥
            
        Returns:
            str: ç›®æ ‡è¡¨åï¼Œå¦‚æœè§£æå¤±è´¥åˆ™è¿”å›None
            
        Example SQL:
            create stream qdb.s18  interval(58s) sliding(11m, 4m)  
                    from qdb.v1   stream_options(watermark(43m)) 
                    into qdb.st18 
                    as select _twstart ts, c2 as c2_val, c1 as c1_val 
                    from stream_trigger;
            
            åº”è¯¥è¿”å›: qdb.st18
        """
        try:
            import re
            
            # æ¸…ç†SQLï¼šç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦å’Œæ¢è¡Œç¬¦
            cleaned_sql = ' '.join(sql.split())
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… into å…³é”®å­—åé¢çš„è¡¨å
            # æ¨¡å¼è¯´æ˜:
            # \binto\s+ : åŒ¹é…å•è¯è¾¹ç•Œçš„intoï¼Œåè·Ÿä¸€ä¸ªæˆ–å¤šä¸ªç©ºç™½å­—ç¬¦
            # ([^\s]+)  : æ•è·ä¸€ä¸ªæˆ–å¤šä¸ªéç©ºç™½å­—ç¬¦ï¼ˆè¡¨åï¼‰
            # \s+as\s+  : åŒ¹é…ç©ºç™½å­—ç¬¦ + as + ç©ºç™½å­—ç¬¦
            pattern = r'\binto\s+([^\s]+)\s+as\s+'
            
            match = re.search(pattern, cleaned_sql, re.IGNORECASE)
            
            if match:
                target_table = match.group(1)
                # ç§»é™¤å¯èƒ½çš„åˆ†å·æˆ–å…¶ä»–æ ‡ç‚¹ç¬¦å·
                target_table = target_table.rstrip(';').rstrip(',')
                #print(f"ä»SQLä¸­è§£æå‡ºç›®æ ‡è¡¨: {target_table}")
                return target_table
            else:
                # å¦‚æœç¬¬ä¸€ä¸ªæ¨¡å¼æ²¡åŒ¹é…åˆ°ï¼Œå°è¯•æ›´å®½æ¾çš„æ¨¡å¼
                # æœ‰äº›SQLå¯èƒ½æ ¼å¼ä¸åŒï¼Œå°è¯•åªåŒ¹é…intoåé¢çš„ç¬¬ä¸€ä¸ªæ ‡è¯†ç¬¦
                pattern2 = r'\binto\s+([^\s\n]+)'
                match2 = re.search(pattern2, cleaned_sql, re.IGNORECASE)
                
                if match2:
                    target_table = match2.group(1)
                    target_table = target_table.rstrip(';').rstrip(',')
                    print(f"ä½¿ç”¨å¤‡ç”¨æ¨¡å¼è§£æå‡ºç›®æ ‡è¡¨: {target_table}")
                    return target_table
                else:
                    print(f"æ— æ³•ä»SQLä¸­è§£æç›®æ ‡è¡¨: {cleaned_sql[:200]}...")
                    return None
                    
        except Exception as e:
            print(f"è§£æSQLæ—¶å‡ºé”™: {str(e)}")
            return None
                
    def get_stream_target_tables(self):
        """è·å–æµè®¡ç®—çš„ç›®æ ‡è¡¨åˆ—è¡¨
        
        Returns:
            list: ç›®æ ‡è¡¨ååˆ—è¡¨
        """
        try:
            conn, cursor = self.get_connection()
            
            # æŸ¥è¯¢æ‰€æœ‰æµä¿¡æ¯
            cursor.execute("select stream_name, sql from information_schema.ins_streams")
            streams = cursor.fetchall()
            print(f"ä» information_schema.ins_streams æŸ¥è¯¢åˆ° {len(streams)} ä¸ªæµ")
        
            
            target_tables = []
            for stream in streams:
                stream_name = stream[0]
                sql = stream[1]
            
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æSQLä¸­çš„ç›®æ ‡è¡¨
                target_table = self._parse_target_table_from_sql(sql)
                
                if target_table:
                    target_tables.append({
                        'stream_name': stream_name,
                        'target_table': target_table,
                        'target_db': target_table.split('.')[0] if '.' in target_table else 'stream_to',
                        'target_table_name': target_table.split('.')[-1],
                        'sql': sql  # ä¿å­˜åŸå§‹SQLç”¨äºè°ƒè¯•
                    })
                    #print(f"  æµ: {stream_name} -> ç›®æ ‡è¡¨: {target_table}")
                else:
                    print(f"  è­¦å‘Š: æ— æ³•è§£ææµ {stream_name} çš„ç›®æ ‡è¡¨ï¼ŒSQL: {sql[:100]}...")
            
            cursor.close()
            conn.close()
            
            if len(target_tables) == 0:
                print("è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•æµæˆ–æ— æ³•è§£æç›®æ ‡è¡¨")
            else:
                print(f"æˆåŠŸè§£æ {len(target_tables)} ä¸ªæµçš„ç›®æ ‡è¡¨")
                
            return target_tables
            
        except Exception as e:
            print(f"è·å–æµç›®æ ‡è¡¨å¤±è´¥: {str(e)}")
            return []

    def check_stream_computation_delay(self):
        """æ£€æŸ¥æµè®¡ç®—å»¶è¿Ÿ
        
        Returns:
            dict: å»¶è¿Ÿæ£€æŸ¥ç»“æœ
        """
        try:
            conn, cursor = self.get_connection()
            
            # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨
            try:
                cursor.execute("show databases")
                databases = cursor.fetchall()
                db_names = [db[0] for db in databases]
                #print(f"å‘ç°æ•°æ®åº“: {db_names}")
                
                if 'stream_from' not in db_names:
                    print("é”™è¯¯: æºæ•°æ®åº“ stream_from ä¸å­˜åœ¨")
                    cursor.close()
                    conn.close()
                    return None
            except Exception as e:
                print(f"æŸ¥è¯¢æ•°æ®åº“åˆ—è¡¨å¤±è´¥: {str(e)}")
                cursor.close()
                conn.close()
                return None
        
            # è·å–æºè¡¨æœ€æ–°æ—¶é—´æˆ³
            try:
                cursor.execute("select last(ts) from stream_from.stb")
                source_result = cursor.fetchall()
                #print(f"æºè¡¨æŸ¥è¯¢ç»“æœ: {source_result}")
            except Exception as e:
                print(f"æŸ¥è¯¢æºè¡¨å¤±è´¥: {str(e)}")
                cursor.close()
                conn.close()
                return None
            
            if not source_result or not source_result[0][0]:
                print("è­¦å‘Š: æ— æ³•è·å–æºè¡¨æœ€æ–°æ—¶é—´æˆ³")
                cursor.close()
                conn.close()
                return None
                
            source_last_ts = source_result[0][0]
            source_ts_ms = int(source_last_ts.timestamp() * 1000)
            print(f"æºè¡¨æœ€æ–°æ—¶é—´æˆ³: {source_last_ts} ({source_ts_ms}ms)")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æµå­˜åœ¨
            print("æŸ¥è¯¢æµçš„è¯¦ç»†ä¿¡æ¯...")
            try:
                cursor.execute("select stream_name, sql from information_schema.ins_streams")
                streams_info = cursor.fetchall()
                print(f"å‘ç° {len(streams_info)} ä¸ªæµ:")
                for stream in streams_info:
                    print(f"  æµåç§°: {stream[0]}")
                    #print(f"  SQL: {stream[1][:100]}...")  # åªæ˜¾ç¤ºå‰100ä¸ªå­—ç¬¦
            except Exception as e:
                print(f"æŸ¥è¯¢æµè¯¦ç»†ä¿¡æ¯å¤±è´¥: {str(e)}")
            
            # è·å–æµç›®æ ‡è¡¨
            target_tables = self.get_stream_target_tables()
            if not target_tables:
                print("è­¦å‘Š: æœªæ‰¾åˆ°æµç›®æ ‡è¡¨")
                cursor.close()
                conn.close()
                return None
            
            delay_results = {
                'check_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'source_last_ts': source_last_ts,
                'source_ts_ms': source_ts_ms,
                'streams': []
            }
            
            # æ£€æŸ¥æ¯ä¸ªæµçš„å»¶è¿Ÿ
            for table_info in target_tables:
                try:
                    target_table = table_info['target_table']
                    stream_name = table_info['stream_name']
                    
                    #print(f"æ£€æŸ¥æµ {stream_name} çš„ç›®æ ‡è¡¨ {target_table}")
                    
                    # å…ˆæ£€æŸ¥ç›®æ ‡è¡¨æ˜¯å¦å­˜åœ¨
                    try:
                        cursor.execute(f"describe {target_table}")
                        #print(f"ç›®æ ‡è¡¨ {target_table} ç»“æ„æ­£å¸¸")
                    except Exception as e:
                        print(f"ç›®æ ‡è¡¨ {target_table} ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®")
                        
                        # # å°è¯•æŸ¥çœ‹ç›®æ ‡æ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨
                        # target_db = table_info['target_db']
                        # try:
                        #     cursor.execute(f"show tables in {target_db}")
                        #     tables_in_db = cursor.fetchall()
                        #     print(f"æ•°æ®åº“ {target_db} ä¸­çš„è¡¨: {[t[0] for t in tables_in_db]}")
                        # except Exception as db_e:
                        #     print(f"æ— æ³•æŸ¥çœ‹æ•°æ®åº“ {target_db}: {str(db_e)}")
                        
                        stream_result = {
                            'stream_name': stream_name,
                            'target_table': target_table,
                            'error': f"ç›®æ ‡è¡¨ä¸å­˜åœ¨: {str(e)}",
                            'status': 'ERROR'
                        }
                        delay_results['streams'].append(stream_result)
                        continue
                    
                    # æŸ¥è¯¢ç›®æ ‡è¡¨æœ€æ–°æ—¶é—´æˆ³
                    cursor.execute(f"select last(ts) from {target_table}")
                    target_result = cursor.fetchall()
                    #print(f"ç›®æ ‡è¡¨ {target_table} æŸ¥è¯¢ç»“æœ: {target_result}")
                    
                    if target_result and target_result[0][0]:
                        target_last_ts = target_result[0][0]
                        target_ts_ms = int(target_last_ts.timestamp() * 1000)
                        
                        # è®¡ç®—å»¶è¿Ÿ(æ¯«ç§’)
                        delay_ms = source_ts_ms - target_ts_ms
                        
                        stream_result = {
                            'stream_name': stream_name,
                            'target_table': target_table,
                            'target_last_ts': target_last_ts,
                            'delay_ms': delay_ms,
                            'delay_seconds': delay_ms / 1000.0,
                            'is_lagging': delay_ms > self.max_delay_threshold,
                            'status': 'LAGGING' if delay_ms > self.max_delay_threshold else 'OK'
                        }
                        
                        print(f"æµ {stream_name} å»¶è¿Ÿ: {delay_ms}ms ({delay_ms/1000.0:.2f}ç§’)")
                        
                    else:
                        # ç›®æ ‡è¡¨æ— æ•°æ®
                        stream_result = {
                            'stream_name': stream_name,
                            'target_table': target_table,
                            'target_last_ts': None,
                            'delay_ms': None,
                            'delay_seconds': None,
                            'is_lagging': True,
                            'status': 'NO_DATA'
                        }
                    
                    delay_results['streams'].append(stream_result)
                    
                except Exception as e:
                    error_msg = f"æ£€æŸ¥æµ {stream_name} å»¶è¿Ÿæ—¶å‡ºé”™: {str(e)}"
                    print(error_msg)
                    stream_result = {
                        'stream_name': stream_name,
                        'target_table': target_table,
                        'error': str(e),
                        'status': 'ERROR'
                    }
                    delay_results['streams'].append(stream_result)
            
            cursor.close()
            conn.close()
            
            return delay_results
            
        except Exception as e:
            print(f"æ£€æŸ¥æµè®¡ç®—å»¶è¿Ÿæ—¶å‡ºé”™: {str(e)}")
            return None

    def log_stream_delay_results(self, delay_results):
        """è®°å½•æµå»¶è¿Ÿæ£€æŸ¥ç»“æœåˆ°æ—¥å¿—æ–‡ä»¶"""
        if not delay_results:
            return
            
        try:
            with open(self.delay_log_file, 'a') as f:
                # å†™å…¥æ£€æŸ¥æ—¶é—´å’Œæºè¡¨ä¿¡æ¯
                f.write(f"\n{'='*80}\n")
                f.write(f"æ£€æŸ¥æ—¶é—´: {delay_results['check_time']}\n")
                f.write(f"æºè¡¨æœ€æ–°æ—¶é—´: {delay_results['source_last_ts']}\n")
                f.write(f"æºè¡¨æ—¶é—´æˆ³(ms): {delay_results['source_ts_ms']}\n")
                f.write(f"å»¶è¿Ÿé˜ˆå€¼: {format_delay_time(self.max_delay_threshold)}\n")
                f.write(f"-" * 80 + "\n")
                
                # å†™å…¥æ¯ä¸ªæµçš„å»¶è¿Ÿä¿¡æ¯
                for stream in delay_results['streams']:
                    f.write(f"æµåç§°: {stream['stream_name']}\n")
                    f.write(f"ç›®æ ‡è¡¨: {stream['target_table']}\n")
                    f.write(f"çŠ¶æ€: {stream['status']}\n")
                    
                    if stream['status'] == 'OK' or stream['status'] == 'LAGGING':
                        f.write(f"ç›®æ ‡è¡¨æœ€æ–°æ—¶é—´: {stream['target_last_ts']}\n")
                        f.write(f"å»¶è¿Ÿ: {format_delay_time(stream['delay_ms'])}\n")
                        if stream['is_lagging']:
                            f.write(f"è­¦å‘Š: å»¶è¿Ÿè¶…è¿‡é˜ˆå€¼!\n")
                    elif stream['status'] == 'NO_DATA':
                        f.write(f"è­¦å‘Š: ç›®æ ‡è¡¨æ— æ•°æ®\n")
                    elif stream['status'] == 'ERROR':
                        f.write(f"é”™è¯¯: {stream.get('error', 'æœªçŸ¥é”™è¯¯')}\n")
                    
                    f.write(f"-" * 40 + "\n")
                    
        except Exception as e:
            print(f"å†™å…¥å»¶è¿Ÿæ—¥å¿—å¤±è´¥: {str(e)}")

    def print_stream_delay_summary(self, delay_results):
        """æ‰“å°æµå»¶è¿Ÿæ£€æŸ¥æ‘˜è¦"""
        if not delay_results:
            return
        
        c = Colors.get_colors()
        
        # åŸºäº delay_check_interval è®¡ç®—åŠ¨æ€é˜ˆå€¼
        check_interval_ms = self.delay_check_interval * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        # å®šä¹‰å€æ•°é˜ˆå€¼
        excellent_threshold = check_interval_ms * 0.1    # 0.1å€æ£€æŸ¥é—´éš” - ä¼˜ç§€
        good_threshold = check_interval_ms * 0.5         # 0.5å€æ£€æŸ¥é—´éš” - è‰¯å¥½  
        normal_threshold = check_interval_ms * 1.0       # 1å€æ£€æŸ¥é—´éš” - æ­£å¸¸
        mild_delay_threshold = check_interval_ms * 6.0   # 6å€æ£€æŸ¥é—´éš” - è½»å¾®å»¶è¿Ÿ
        obvious_delay_threshold = check_interval_ms * 30.0  # 30å€æ£€æŸ¥é—´éš” - æ˜æ˜¾å»¶è¿Ÿ
            
        print(f"\n=== æµè®¡ç®—å»¶è¿Ÿæ£€æŸ¥ ({delay_results['check_time']}) ===")
        print(f"æºè¡¨æœ€æ–°æ—¶é—´: {delay_results['source_last_ts']}")
        print(f"é…ç½®ä¿¡æ¯: å­è¡¨æ•°é‡({self.table_count}) | æ¯è½®æ’å…¥è®°å½•æ•°({self.real_time_batch_rows}) | vgroups({self.vgroups}) | æ•°æ®ä¹±åº({self.disorder_ratio})")
        print(f"å»¶è¿Ÿåˆ¤æ–­åŸºå‡†: æ£€æŸ¥é—´éš” {self.delay_check_interval}s | éƒ¨ç½²æ¨¡å¼({self.deployment_mode}) | SQLç±»å‹({self.sql_type})")
            
        ok_count = 0
        lagging_count = 0
        no_data_count = 0
        error_count = 0
        
        # æŒ‰å»¶è¿Ÿçº§åˆ«åˆ†ç±»ç»Ÿè®¡
        excellent_count = 0
        good_count = 0
        normal_count = 0
        mild_delay_count = 0
        obvious_delay_count = 0
        severe_delay_count = 0
    
        for stream in delay_results['streams']:
            status = stream['status']
            stream_name = stream['stream_name']
            
            if status == 'OK':
                ok_count += 1
                target_time = stream['target_last_ts']
                delay_ms = stream['delay_ms']
                # æ ¹æ®å»¶è¿Ÿæ—¶é—´é€‰æ‹©å›¾æ ‡å’Œæè¿°
                if delay_ms < excellent_threshold:  #ä¼˜ç§€
                    excellent_count += 1
                    status_icon = "ğŸŸ¢" if Colors.supports_color() else "âœ“"
                    delay_desc = f"ä¼˜ç§€(<{format_delay_time(excellent_threshold)})"
                    color = c.GREEN
                elif delay_ms < good_threshold:  # è‰¯å¥½
                    good_count += 1
                    status_icon = "ğŸŸ¢" if Colors.supports_color() else "âœ“"
                    delay_desc = f"è‰¯å¥½(<{format_delay_time(good_threshold)})"
                    color = c.GREEN
                else:  # delay_ms < normal_threshold (åœ¨é˜ˆå€¼å†…) - æ­£å¸¸
                    normal_count += 1
                    status_icon = "ğŸŸ¡" if Colors.supports_color() else "âœ“"
                    delay_desc = f"æ­£å¸¸(<{format_delay_time(normal_threshold)})"
                    color = c.YELLOW
                    
                print(f"{color}{status_icon} {stream_name}: å»¶è¿Ÿ {format_delay_time(delay_ms)} - {delay_desc}{c.END}")
                print(f"  {c.BLUE}ç›®æ ‡è¡¨æœ€æ–°æ—¶é—´: {target_time}{c.END}")
            
            elif status == 'LAGGING':
                lagging_count += 1
                target_time = stream['target_last_ts']
                delay_ms = stream['delay_ms']
            
                # æ ¹æ®å»¶è¿Ÿç¨‹åº¦é€‰æ‹©é¢œè‰²å’Œå›¾æ ‡
                if delay_ms < mild_delay_threshold:  # è½»å¾®å»¶è¿Ÿ
                    mild_delay_count += 1
                    status_icon = "ğŸŸ¡" if Colors.supports_color() else "âš "
                    color = c.YELLOW
                    delay_desc = f"è½»å¾®å»¶è¿Ÿ(<{format_delay_time(mild_delay_threshold)})"
                elif delay_ms < obvious_delay_threshold:  # æ˜æ˜¾å»¶è¿Ÿ
                    obvious_delay_count += 1
                    status_icon = "ğŸŸ " if Colors.supports_color() else "âš "
                    color = c.YELLOW
                    delay_desc = f"æ˜æ˜¾å»¶è¿Ÿ(<{format_delay_time(obvious_delay_threshold)})"
                else:  # ä¸¥é‡å»¶è¿Ÿ
                    severe_delay_count += 1
                    status_icon = "ğŸ”´" if Colors.supports_color() else "âš "
                    color = c.RED
                    delay_desc = f"ä¸¥é‡å»¶è¿Ÿ(>{format_delay_time(obvious_delay_threshold)})"
                    
                print(f"{color}{c.BOLD}{status_icon} {stream_name}: å»¶è¿Ÿ {format_delay_time(delay_ms)} - {delay_desc}!{c.END}")
                print(f"  {c.BLUE}ç›®æ ‡è¡¨æœ€æ–°æ—¶é—´: {target_time}{c.END}")
                
            elif status == 'NO_DATA':
                no_data_count += 1
                status_icon = "âŒ" if Colors.supports_color() else "âœ—"
                print(f"{c.RED}{status_icon} {stream_name}: ç›®æ ‡è¡¨æœªç”Ÿæˆæ•°æ®{c.END}")
                
            elif status == 'ERROR':
                error_count += 1
                status_icon = "ğŸ’¥" if Colors.supports_color() else "âœ—"
                print(f"{c.RED}{c.BOLD}{status_icon} {stream_name}: ç›®æ ‡è¡¨ä¸å­˜åœ¨æˆ–æ£€æŸ¥å‡ºé”™{c.END}")
                error_msg = stream.get('error', 'æœªçŸ¥é”™è¯¯')
                print(f"  {c.RED}é”™è¯¯ä¿¡æ¯: {error_msg}{c.END}")
 
    
        # è®¡ç®—æ€»æ•°å’Œç™¾åˆ†æ¯”
        total_streams = len(delay_results['streams'])
        
        def get_percentage(count, total):
            return f"{(count/total*100):.1f}%" if total > 0 else "0.0%"
           
        # æ‘˜è¦ä¿¡æ¯å¸¦é¢œè‰²å’Œå›¾æ ‡
        summary_parts = []
        # æ­£å¸¸æµ
        if ok_count > 0:
            ok_icon = "ğŸŸ¢" if Colors.supports_color() else ""
            ok_percent = get_percentage(ok_count, total_streams)
            summary_parts.append(f"{c.GREEN}{ok_icon}æ­£å¸¸({ok_count}/{ok_percent}){c.END}")
            
        # å»¶è¿Ÿæµ
        if lagging_count > 0:
            lag_icon = "ğŸŸ¡" if Colors.supports_color() else ""
            lag_percent = get_percentage(lagging_count, total_streams)
            summary_parts.append(f"{c.YELLOW}{lag_icon}å»¶è¿Ÿ({lagging_count}/{lag_percent}){c.END}")
            
        # æœªç”Ÿæˆæ•°æ®çš„æµ     
        if no_data_count > 0:
            no_data_icon = "âŒ" if Colors.supports_color() else ""
            no_data_percent = get_percentage(no_data_count, total_streams)
            summary_parts.append(f"{c.RED}{no_data_icon}æœªç”Ÿæˆæ•°æ®({no_data_count}/{no_data_percent}){c.END}")
            
        # é”™è¯¯çŠ¶æ€çš„æµ    
        if error_count > 0:
            error_icon = "ğŸ’¥" if Colors.supports_color() else ""
            error_percent = get_percentage(error_count, total_streams)
            summary_parts.append(f"{c.RED}{error_icon}è¡¨ä¸å­˜åœ¨({error_count}/{error_percent}){c.END}")
        
        print(f"\n{c.BOLD}ğŸ“Š æ‘˜è¦ (æ€»æµæ•°: {total_streams}): {' '.join(summary_parts)}{c.END}")

        # è¯¦ç»†åˆ†å¸ƒç»Ÿè®¡
        if ok_count > 0 or lagging_count > 0:
            print(f"\n{c.CYAN}ğŸ“ˆ å»¶è¿Ÿåˆ†å¸ƒè¯¦æƒ…:{c.END}")
            
            # æ­£å¸¸çŠ¶æ€åˆ†å¸ƒ
            if ok_count > 0:
                print(f"  {c.GREEN}æ­£å¸¸çŠ¶æ€åˆ†å¸ƒ:{c.END}")
                if excellent_count > 0:
                    excellent_percent = get_percentage(excellent_count, total_streams)
                    print(f"    ğŸŸ¢ ä¼˜ç§€: {excellent_count} ({excellent_percent})")
                if good_count > 0:
                    good_percent = get_percentage(good_count, total_streams)
                    print(f"    ğŸŸ¢ è‰¯å¥½: {good_count} ({good_percent})")
                if normal_count > 0:
                    normal_percent = get_percentage(normal_count, total_streams)
                    print(f"    ğŸŸ¡ æ­£å¸¸: {normal_count} ({normal_percent})")
            
            # å»¶è¿ŸçŠ¶æ€åˆ†å¸ƒ
            if lagging_count > 0:
                print(f"  {c.YELLOW}å»¶è¿ŸçŠ¶æ€åˆ†å¸ƒ:{c.END}")
                if mild_delay_count > 0:
                    mild_percent = get_percentage(mild_delay_count, total_streams)
                    print(f"    ğŸŸ¡ è½»å¾®å»¶è¿Ÿ: {mild_delay_count} ({mild_percent})")
                if obvious_delay_count > 0:
                    obvious_percent = get_percentage(obvious_delay_count, total_streams)
                    print(f"    ğŸŸ  æ˜æ˜¾å»¶è¿Ÿ: {obvious_delay_count} ({obvious_percent})")
                if severe_delay_count > 0:
                    severe_percent = get_percentage(severe_delay_count, total_streams)
                    print(f"    ğŸ”´ ä¸¥é‡å»¶è¿Ÿ: {severe_delay_count} ({severe_percent})")
    
        # é—®é¢˜æµç»Ÿè®¡
        if no_data_count > 0 or error_count > 0:
            print(f"\n{c.RED}âš ï¸ é—®é¢˜æµç»Ÿè®¡:{c.END}")
            if no_data_count > 0:
                no_data_percent = get_percentage(no_data_count, total_streams)
                print(f"  âŒ ç›®æ ‡è¡¨æœªç”Ÿæˆæ•°æ®: {no_data_count} ({no_data_percent}) ")
            if error_count > 0:
                error_percent = get_percentage(error_count, total_streams)
                print(f"  ğŸ’¥ ç›®æ ‡è¡¨ä¸å­˜åœ¨: {error_count} ({error_percent}) ")
           
        # æ˜¾ç¤ºé˜ˆå€¼å‚è€ƒä¿¡æ¯
        print(f"\n{c.CYAN}å»¶è¿Ÿç­‰çº§å‚è€ƒ (åŸºäºæ£€æŸ¥é—´éš” {self.delay_check_interval}s):{c.END}")
        print(f"  ğŸŸ¢ ä¼˜ç§€: < {format_delay_time(excellent_threshold)} (0.1å€é—´éš”)")
        print(f"  ğŸŸ¢ è‰¯å¥½: {format_delay_time(excellent_threshold)} - {format_delay_time(good_threshold)} (0.1-0.5å€é—´éš”)")
        print(f"  ğŸŸ¡ æ­£å¸¸: {format_delay_time(good_threshold)} - {format_delay_time(normal_threshold)} (0.5-1å€é—´éš”)")
        print(f"  ğŸŸ¡ è½»å¾®å»¶è¿Ÿ: {format_delay_time(normal_threshold)} - {format_delay_time(mild_delay_threshold)} (1-6å€é—´éš”)")
        print(f"  ğŸŸ  æ˜æ˜¾å»¶è¿Ÿ: {format_delay_time(mild_delay_threshold)} - {format_delay_time(obvious_delay_threshold)} (6-30å€é—´éš”)")
        print(f"  ğŸ”´ ä¸¥é‡å»¶è¿Ÿ: > {format_delay_time(obvious_delay_threshold)} (>30å€é—´éš”)")
        
        # çŠ¶æ€è¯„ä¼°
        healthy_count = ok_count  # åªæœ‰æ­£å¸¸çŠ¶æ€æ‰ç®—å¥åº·
        problem_count = lagging_count + no_data_count + error_count # å»¶è¿Ÿã€æ— æ•°æ®ã€é”™è¯¯éƒ½æ˜¯é—®é¢˜
        
        # è­¦å‘Šå’Œå»ºè®®ä¿¡æ¯
        if problem_count > 0:
            if no_data_count > 0 or error_count > 0:
                # æœ‰ä¸¥é‡é—®é¢˜ï¼ˆæ— æ•°æ®æˆ–é”™è¯¯ï¼‰
                critical_count = no_data_count + error_count
                critical_percent = get_percentage(critical_count, total_streams)
                warning_icon = "ğŸš¨" if Colors.supports_color() else "âš "
                print_error(f"{warning_icon} ä¸¥é‡: {critical_count} ä¸ªæµå­˜åœ¨ä¸¥é‡é—®é¢˜ ({critical_percent})!")
                
            if lagging_count > 0:
                # æœ‰å»¶è¿Ÿé—®é¢˜
                lag_percent = get_percentage(lagging_count, total_streams)
                warning_icon = "âš ï¸" if Colors.supports_color() else "âš "
                print_warning(f"{warning_icon} å»¶è¿Ÿ: {lagging_count} ä¸ªæµè®¡ç®—å‡ºç°å»¶è¿Ÿ ({lag_percent})!")
            
        # æ•´ä½“çŠ¶æ€è¯„ä¼°
        if error_count > 0 or no_data_count > 0:
            # å­˜åœ¨ä¸¥é‡é—®é¢˜
            advice_icon = "ğŸš¨" if Colors.supports_color() else "ğŸ’¡"
            problem_ratio = (error_count + no_data_count) / total_streams * 100
            print_error(f"{advice_icon} å‘Šè­¦: {problem_ratio:.1f}% çš„æµå­˜åœ¨ä¸¥é‡é—®é¢˜ - éœ€è¦ç«‹å³æ£€æŸ¥")
        elif lagging_count > healthy_count:
            # å»¶è¿Ÿé—®é¢˜è¾ƒå¤š
            advice_icon = "ğŸ’¡" if Colors.supports_color() else "ğŸ’¡"
            lag_ratio = lagging_count / total_streams * 100
            print_error(f"{advice_icon} è­¦å‘Š: {lag_ratio:.1f}% çš„æµå‡ºç°å»¶è¿Ÿ - å»ºè®®ä¼˜åŒ–æ€§èƒ½")
        elif lagging_count > 0:
            # å°‘é‡å»¶è¿Ÿ
            tip_icon = "ğŸ’¡" if Colors.supports_color() else "ğŸ’¡"
            lag_ratio = lagging_count / total_streams * 100
            print_warning(f"{tip_icon} æç¤º: {lag_ratio:.1f}% çš„æµå‡ºç°å»¶è¿Ÿ - æŒç»­å…³æ³¨")
        elif healthy_count == total_streams:
            # å…¨éƒ¨æ­£å¸¸
            success_icon = "âœ…" if Colors.supports_color() else "âœ“"
            print_success(f"{success_icon} çŠ¶æ€ä¼˜ç§€: æ‰€æœ‰æµè®¡ç®—éƒ½æ­£å¸¸è¿è¡Œ (100%)")
            
        # è°ƒä¼˜å»ºè®®
        if problem_count > 0:
            print(f"\n{c.CYAN}ğŸ’¡ é—®é¢˜åˆ†æä¸å»ºè®®:{c.END}")
            
            if error_count > 0:
                print(f"  ğŸ”´ ç›®æ ‡è¡¨ä¸å­˜åœ¨é—®é¢˜:")
                print(f"    - {error_count} ä¸ªæµçš„ç›®æ ‡è¡¨æ— æ³•è®¿é—®")
                print(f"    - å¯èƒ½åŸå› : æµåˆ›å»ºå¤±è´¥ã€ç›®æ ‡è¡¨è¿˜æœªç”Ÿæˆ")
                
            if no_data_count > 0:
                print(f"  âŒ ç›®æ ‡è¡¨æ— æ•°æ®é—®é¢˜:")
                print(f"    - {no_data_count} ä¸ªæµçš„ç›®æ ‡è¡¨å­˜åœ¨ä½†æ— æ•°æ®")
                print(f"    - å¯èƒ½åŸå› : æµè®¡ç®—é€»è¾‘é—®é¢˜ã€æºæ•°æ®ä¸åŒ¹é…ã€æµæœªå¯åŠ¨")
                print(f"    - å»ºè®®: æ£€æŸ¥æµçŠ¶æ€ã€éªŒè¯æºæ•°æ®ã€æ£€æŸ¥æµè®¡ç®—é€»è¾‘")
                
            if severe_delay_count > 0:
                severe_ratio = severe_delay_count / total_streams * 100
                print(f"  ğŸ”´ ä¸¥é‡å»¶è¿Ÿé—®é¢˜:")
                print(f"    - {severe_delay_count} ä¸ªæµå­˜åœ¨ä¸¥é‡å»¶è¿Ÿ ({severe_ratio:.1f}%)")
                print(f"    - å»ºè®®: ç«‹å³æ£€æŸ¥ç³»ç»Ÿèµ„æºã€ä¼˜åŒ–æµè®¡ç®—é€»è¾‘")
                
            if obvious_delay_count > 0:
                obvious_ratio = obvious_delay_count / total_streams * 100
                print(f"  ğŸŸ  æ˜æ˜¾å»¶è¿Ÿé—®é¢˜:")
                print(f"    - {obvious_delay_count} ä¸ªæµå­˜åœ¨æ˜æ˜¾å»¶è¿Ÿ ({obvious_ratio:.1f}%)")
                print(f"    - å»ºè®®: æ£€æŸ¥ç³»ç»Ÿè´Ÿè½½ã€è€ƒè™‘å¢åŠ èµ„æº")
                
            print(f"  ğŸ“Š ç³»ç»Ÿè°ƒä¼˜å»ºè®®:")
            if problem_count > total_streams * 0.5:
                print(f"    - è¶…è¿‡ä¸€åŠçš„æµå­˜åœ¨é—®é¢˜ï¼Œå»ºè®®å…¨é¢æ£€æŸ¥ç³»ç»Ÿé…ç½®")
            print(f"    - å½“å‰å†™å…¥é—´éš”: {self.real_time_batch_sleep}sï¼Œå¯é€‚å½“å¢åŠ ä»¥å‡è½»å‹åŠ›")
            print(f"    - å¯è°ƒæ•´ --delay-check-interval å‚æ•°æ”¹å˜ç›‘æ§é¢‘ç‡")
            
                        

    def start_stream_delay_monitor(self):
        """å¯åŠ¨æµå»¶è¿Ÿç›‘æ§çº¿ç¨‹"""
        if not self.check_stream_delay:
            return None
            
        def delay_monitor():
            """å»¶è¿Ÿç›‘æ§çº¿ç¨‹å‡½æ•°"""
            print(f"å¯åŠ¨æµå»¶è¿Ÿç›‘æ§ (é—´éš”: {self.delay_check_interval}ç§’)")
            
            # åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶
            try:
                with open(self.delay_log_file, 'w') as f:
                    f.write(f"TDengine æµè®¡ç®—å»¶è¿Ÿç›‘æ§æ—¥å¿—\n")
                    f.write(f"å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"æœ€å¤§å»¶è¿Ÿé˜ˆå€¼: {self.max_delay_threshold}ms\n")
                    f.write(f"æ£€æŸ¥é—´éš”: {self.delay_check_interval}ç§’\n")
                    f.write(f"æ•°æ®å†™å…¥é—´éš”: {self.real_time_batch_sleep}ç§’\n") 
                    f.write(f"æ¯è½®å†™å…¥è®°å½•æ•°: {self.real_time_batch_rows}\n")
                    f.write(f"å­è¡¨æ•°é‡: {self.table_count}\n")
                    f.write(f"è¿è¡Œæ—¶é—´: {self.runtime}åˆ†é’Ÿ\n")
                    f.write(f"SQLç±»å‹: {self.sql_type}\n")
                    f.write(f"="*80 + "\n")
            except Exception as e:
                print(f"åˆå§‹åŒ–å»¶è¿Ÿæ—¥å¿—æ–‡ä»¶å¤±è´¥: {str(e)}")
            
            start_time = time.time()
            check_count = 0
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©æµåˆ›å»ºå®Œæˆ
            initial_wait = 15  # ç­‰å¾…15ç§’
            print(f"ç­‰å¾… {initial_wait} ç§’è®©æµè®¡ç®—å¯åŠ¨...")
            time.sleep(initial_wait)
        
            while time.time() - start_time < self.runtime * 60:
                check_count += 1
                try:
                    print(f"\n--- ç¬¬ {check_count} æ¬¡å»¶è¿Ÿæ£€æŸ¥ ---")
                    
                    # è®°å½•æ£€æŸ¥å¼€å§‹
                    with open(self.delay_log_file, 'a') as f:
                        f.write(f"\nç¬¬ {check_count} æ¬¡æ£€æŸ¥å¼€å§‹: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    
                    # æ‰§è¡Œå»¶è¿Ÿæ£€æŸ¥
                    delay_results = self.check_stream_computation_delay()
                    
                    if delay_results:
                        # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
                        self.log_stream_delay_results(delay_results)
                        
                        # æ‰“å°æ‘˜è¦
                        self.print_stream_delay_summary(delay_results)
                    else:
                        error_msg = f"ç¬¬ {check_count} æ¬¡æ£€æŸ¥å¤±è´¥: æ— æ³•è·å–å»¶è¿Ÿç»“æœ"
                        print(error_msg)
                        with open(self.delay_log_file, 'a') as f:
                            f.write(f"{error_msg}\n")
                    
                    # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
                    print(f"ç­‰å¾… {self.delay_check_interval} ç§’åè¿›è¡Œä¸‹æ¬¡æ£€æŸ¥...")
                    time.sleep(self.delay_check_interval)
                    
                except Exception as e:
                    error_msg = f"ç¬¬ {check_count} æ¬¡å»¶è¿Ÿç›‘æ§å‡ºé”™: {str(e)}"
                    print(error_msg)
                    with open(self.delay_log_file, 'a') as f:
                        f.write(f"{error_msg}\n")
                    time.sleep(self.delay_check_interval)
            
            final_msg = f"æµå»¶è¿Ÿç›‘æ§ç»“æŸï¼Œå…±æ‰§è¡Œäº† {check_count} æ¬¡æ£€æŸ¥"
            print(final_msg)
            with open(self.delay_log_file, 'a') as f:
                f.write(f"\n{final_msg}\n")
                f.write(f"ç»“æŸæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # åˆ›å»ºå¹¶å¯åŠ¨ç›‘æ§çº¿ç¨‹
        monitor_thread = threading.Thread(target=delay_monitor, name="StreamDelayMonitor")
        monitor_thread.daemon = True
        monitor_thread.start()
        
        return monitor_thread
            
    def do_test_stream_with_realtime_data(self):
        self.prepare_env()
        self.prepare_source_from_data()
        self.insert_source_from_data()
        
        conn = taos.connect(
            host=self.host,
            user=self.user,
            password=self.passwd,
            config=self.conf
        )
        cursor = conn.cursor()

        try:
            # è¿è¡Œsource_fromçš„æ•°æ®ç”Ÿæˆ
            subprocess.Popen('taosBenchmark --f /tmp/stream_from.json', 
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, text=True)
            
            # åˆ›å»ºstream_toæ•°æ®åº“
            if not self.create_database('stream_to'):
                raise Exception("åˆ›å»ºstream_toæ•°æ®åº“å¤±è´¥")
            
            time.sleep(5)
            print("æ•°æ®åº“å·²åˆ›å»º,ç­‰å¾…æ•°æ®å†™å…¥...")
            
            # ç­‰å¾…æ•°æ®å‡†å¤‡å°±ç»ª
            if not self.wait_for_data_ready(cursor, self.table_count, self.histroy_rows):
                print("æ•°æ®å‡†å¤‡å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
                return
            
            # è·å–æ–°è¿æ¥æ‰§è¡Œæµå¼æŸ¥è¯¢
            conn, cursor = self.get_connection()
            
            print("å¼€å§‹è¿æ¥æ•°æ®åº“")
            cursor.execute('use stream_from')
            
            # # æ‰§è¡Œæµå¼æŸ¥è¯¢
            # print(f"æ‰§è¡Œæµå¼æŸ¥è¯¢SQL:\n{self.stream_sql}")
            # cursor.execute(self.stream_sql)
            
            # è·å– SQL æ¨¡æ¿
            #sql_templates = StreamSQLTemplates.get_sql(self.sql_type)
            sql_templates = self.stream_sql 
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºæ‰¹é‡æ‰§è¡Œ
            if isinstance(sql_templates, dict):
                print("\n=== å¼€å§‹æ‰¹é‡åˆ›å»ºæµ ===")
                for sql_name, sql_template in sql_templates.items():
                    try:
                        print(f"\nåˆ›å»ºæµ {sql_name}:")
                        print(sql_template)
                        cursor.execute(sql_template)
                        print(f"æµ {sql_name} åˆ›å»ºæˆåŠŸ")
                    except Exception as e:
                        print(f"åˆ›å»ºæµ {sql_name} å¤±è´¥: {str(e)}")
            else:
                # å•ä¸ªæµçš„åˆ›å»º
                print("\n=== å¼€å§‹åˆ›å»ºæµ ===")
                print("æ‰§è¡Œæµå¼æŸ¥è¯¢SQL:")
                print(sql_templates)
                cursor.execute(sql_templates)
                print("æµåˆ›å»ºæˆåŠŸ")
            
            print("æµå¼æŸ¥è¯¢å·²åˆ›å»º,å¼€å§‹ç›‘æ§ç³»ç»Ÿè´Ÿè½½")
            cursor.close()
            conn.close()
            
            # ç›‘æ§ç³»ç»Ÿè´Ÿè½½ - åŒæ—¶ç›‘æ§ä¸‰ä¸ªèŠ‚ç‚¹
            loader = MonitorSystemLoad(
                name_pattern='taosd -c', 
                count=self.runtime * 60,
                perf_file='/tmp/perf-taosd.log',  # åŸºç¡€æ–‡ä»¶å,ä¼šè‡ªåŠ¨æ·»åŠ dnodeç¼–å·
                interval=self.monitor_interval,
                deployment_mode=self.deployment_mode 
            )
                        
            # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œç›‘æ§
            monitor_thread = threading.Thread(
                target=loader.get_proc_status,
                name="TaosdMonitor"
            )
            monitor_thread.daemon = True
            monitor_thread.start()
            print("å¼€å§‹ç›‘æ§taosdè¿›ç¨‹èµ„æºä½¿ç”¨æƒ…å†µ...")
            
            # å¯åŠ¨æµå»¶è¿Ÿç›‘æ§
            delay_monitor_thread = self.start_stream_delay_monitor()    
          
            try:            
                # å¾ªç¯æ‰§è¡Œå†™å…¥å’Œè®¡ç®—
                cycle_count = 0
                start_time = time.time()
                
                while True:
                    cycle_count += 1
                    print(f"\n=== å¼€å§‹ç¬¬ {cycle_count} è½®å†™å…¥å’Œè®¡ç®— ===")
                    
                    if cycle_count > 1:
                        # ä»ç¬¬äºŒè½®å¼€å§‹ï¼Œå…ˆå†™å…¥æ–°æ•°æ®
                        print(f"\nå†™å…¥æ–°ä¸€æ‰¹æµ‹è¯•æ•°æ® (æ¯è½® {self.real_time_batch_rows} æ¡è®°å½•)...")
                        
                        # åº”ç”¨å†™å…¥é—´éš”æ§åˆ¶
                        if self.real_time_batch_sleep > 0:
                            print(f"ç­‰å¾… {self.real_time_batch_sleep} ç§’åå¼€å§‹å†™å…¥æ•°æ®...")
                            time.sleep(self.real_time_batch_sleep)
                        
                        write_start_time = time.time()
                        
                        if not self.update_insert_config():
                            raise Exception("æ›´æ–°å†™å…¥é…ç½®å¤±è´¥")
                        
                        # cmd = "taosBenchmark -f /tmp/stream_from_insertdata.json"
                        # if subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, text=True).returncode != 0:
                        #     raise Exception("å†™å…¥æ–°æ•°æ®å¤±è´¥")
                        
                        cmd = "taosBenchmark -f /tmp/stream_from_insertdata.json"
                        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, text=True)
                        
                        write_end_time = time.time()
                        write_duration = write_end_time - write_start_time
                        
                        if result.returncode != 0:
                            print(f"å†™å…¥æ•°æ®å¤±è´¥: {result.stderr}")
                            raise Exception("å†™å…¥æ–°æ•°æ®å¤±è´¥")
                        else:
                            total_records = self.table_count * self.real_time_batch_rows
                            write_speed = total_records / write_duration if write_duration > 0 else 0
                            print(f"æ•°æ®å†™å…¥å®Œæˆ: {total_records} æ¡è®°å½•, è€—æ—¶ {write_duration:.2f}ç§’, é€Ÿåº¦ {write_speed:.0f} æ¡/ç§’")
                            
                            # å¦‚æœè®¾ç½®äº†å†™å…¥é—´éš”ï¼Œæ˜¾ç¤ºå†™å…¥æ§åˆ¶ä¿¡æ¯
                            if self.real_time_batch_sleep > 0:
                                print(f"å†™å…¥é—´éš”æ§åˆ¶: {self.real_time_batch_sleep}ç§’ (ç”¨äºæ§åˆ¶æµè®¡ç®—å‹åŠ›)")
                                           
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è¿è¡Œæ—¶é—´é™åˆ¶
                    if time.time() - start_time >= self.runtime * 60:
                        print(f"\n\nå·²è¾¾åˆ°è¿è¡Œæ—¶é—´é™åˆ¶ ({self.runtime} åˆ†é’Ÿ)ï¼Œåœæ­¢æ‰§è¡Œ")
                        break
                        
                    print(f"\n=== ç¬¬ {cycle_count} è½®å¤„ç†å®Œæˆ ===")
             
            except Exception as e:
                print(f"æŸ¥è¯¢å†™å…¥æ“ä½œå‡ºé”™: {str(e)}")
            finally:
                cursor.close()
                conn.close()
                print("æŸ¥è¯¢å†™å…¥æ“ä½œå®Œæˆ")
        
                # ä¸»åŠ¨åœæ­¢ç›‘æ§
                print("ä¸»åŠ¨åœæ­¢ç³»ç»Ÿç›‘æ§...")
                loader.stop()
                
            # ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
            print("ç­‰å¾…ç›‘æ§æ•°æ®æ”¶é›†å®Œæˆ...")
            monitor_thread.join(timeout=15)  # æœ€å¤šç­‰å¾…15ç§’
                
            if monitor_thread.is_alive():
                print("ç›‘æ§çº¿ç¨‹æœªåœ¨é¢„æœŸæ—¶é—´å†…ç»“æŸï¼Œå¼ºåˆ¶ç»§ç»­...")
            else:
                print("ç›‘æ§çº¿ç¨‹å·²æ­£å¸¸ç»“æŸ")
        
            if delay_monitor_thread:
                print("ç­‰å¾…å»¶è¿Ÿç›‘æ§å®Œæˆ...")
                delay_monitor_thread.join(timeout=10)  # æœ€å¤šç­‰å¾…10ç§’
                if delay_monitor_thread.is_alive():
                    print("å»¶è¿Ÿç›‘æ§çº¿ç¨‹æœªåœ¨é¢„æœŸæ—¶é—´å†…ç»“æŸï¼Œå¼ºåˆ¶ç»§ç»­...")
                else:
                    print("å»¶è¿Ÿç›‘æ§çº¿ç¨‹å·²æ­£å¸¸ç»“æŸ")
                
            # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
            if self.check_stream_delay:
                print(f"\næµå»¶è¿Ÿç›‘æ§æŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.delay_log_file}")
                self.print_final_delay_summary()
                
                            
            try:
                loader.get_proc_status()
            except KeyboardInterrupt:
                print("\nç›‘æ§è¢«ä¸­æ–­")
            finally:
                # æ£€æŸ¥taosdè¿›ç¨‹
                result = subprocess.run('ps -ef | grep taosd | grep -v grep', 
                                    shell=True, capture_output=True, text=True)
                if result.stdout:
                    print("\ntaosdè¿›ç¨‹ä»åœ¨è¿è¡Œ")
                    print("å¦‚éœ€åœæ­¢taosdè¿›ç¨‹ï¼Œè¯·æ‰‹åŠ¨æ‰§è¡Œ: pkill taosd")
                
        except Exception as e:
            print(f"æ‰§è¡Œé”™è¯¯: {str(e)}")            


    def print_final_delay_summary(self):
        """æ‰“å°æœ€ç»ˆçš„å»¶è¿Ÿæµ‹è¯•æ‘˜è¦"""
        try:
            c = Colors.get_colors()
            
            print_title("\n=== æµè®¡ç®—å»¶è¿Ÿæµ‹è¯•æ€»ç»“ ===")
            print(f"æµ‹è¯•é…ç½®:")
            print(f"  å­è¡¨æ•°é‡: {self.table_count}")
            print(f"  æ¯è½®æ’å…¥è®°å½•æ•°: {self.real_time_batch_rows}")
            print(f"  æ•°æ®å†™å…¥é—´éš”: {self.real_time_batch_sleep}ç§’")
            print(f"  å»¶è¿Ÿæ£€æŸ¥é—´éš”: {self.delay_check_interval}ç§’")
            print(f"  æœ€å¤§å»¶è¿Ÿé˜ˆå€¼: {format_delay_time(self.max_delay_threshold)}")
            print(f"  è¿è¡Œæ—¶é—´: {self.runtime}åˆ†é’Ÿ")
            print(f"  SQLç±»å‹: {self.sql_type}")
            print(f"  æµæ•°é‡: {self.stream_num}")
            
            # åˆ†æå†™å…¥å‹åŠ›
            total_records_per_round = self.table_count * self.real_time_batch_rows
            estimated_rounds = (self.runtime * 60) / max(self.delay_check_interval, 1)
            total_estimated_records = total_records_per_round * estimated_rounds
            
            print(f"\nå†™å…¥å‹åŠ›åˆ†æ:")
            print(f"  æ¯è½®æ€»è®°å½•æ•°: {total_records_per_round:,}")
            print(f"  é¢„è®¡è½®æ¬¡: {estimated_rounds:.0f}")
            print(f"  é¢„è®¡æ€»è®°å½•æ•°: {total_estimated_records:,.0f}")
            
            if self.real_time_batch_sleep > 0:
                effective_write_interval = self.delay_check_interval + self.real_time_batch_sleep
                print(f"  å®é™…å†™å…¥é—´éš”: {effective_write_interval}ç§’ (æ£€æŸ¥é—´éš” + å†™å…¥ç­‰å¾…)")
                write_rate = total_records_per_round / effective_write_interval
                print(f"  å¹³å‡å†™å…¥é€Ÿç‡: {write_rate:.0f} æ¡/ç§’")
            else:
                print(f"  å†™å…¥æ¨¡å¼: æ— é—´éš”æ§åˆ¶ (æœ€å¤§é€Ÿåº¦å†™å…¥)")
            
            print(f"\nä¼˜åŒ–å»ºè®®:")
            if self.real_time_batch_sleep == 0:
                print_warning("  - å½“å‰æ— å†™å…¥é—´éš”æ§åˆ¶ï¼Œå¦‚æœå»¶è¿Ÿè¾ƒå¤§å¯ä»¥å°è¯•å¢åŠ  --real-time-batch-sleep å‚æ•°")
            else:
                print_info(f"  - å½“å‰å†™å…¥é—´éš”: {self.real_time_batch_sleep}ç§’")
                print_info("  - å¦‚æœå»¶è¿Ÿä»ç„¶è¾ƒå¤§ï¼Œå¯ä»¥è¿›ä¸€æ­¥å¢åŠ å†™å…¥é—´éš”")
                print_info("  - å¦‚æœå»¶è¿Ÿè¾ƒå°ï¼Œå¯ä»¥å‡å°‘å†™å…¥é—´éš”ä»¥å¢åŠ æµ‹è¯•å‹åŠ›")
            
            print(f"  - å¯ä»¥è°ƒæ•´ --real-time-batch-rows å‚æ•°æ”¹å˜æ¯è½®å†™å…¥é‡")
            print(f"  - å¯ä»¥è°ƒæ•´ --delay-check-interval å‚æ•°æ”¹å˜ç›‘æ§é¢‘ç‡")
            
            print(f"\nå»¶è¿Ÿç›‘æ§æ—¥å¿—: {self.delay_log_file}")
            print(f"æ€§èƒ½ç›‘æ§æ—¥å¿—: /tmp/perf-taosd-*.log")
            
        except Exception as e:
            print(f"ç”Ÿæˆæœ€ç»ˆæ‘˜è¦æ—¶å‡ºé”™: {str(e)}")
            
            
    def do_start_bak(self):
        self.prepare_env()
        self.prepare_source_from_data()
        
        conn = taos.connect(
            host=self.host,
            user=self.user,
            password=self.passwd,
            config=self.conf
        )
        cursor = conn.cursor()

        try:
            # è¿è¡Œsource_fromçš„æ•°æ®ç”Ÿæˆ
            subprocess.Popen('taosBenchmark --f /tmp/stream_from.json', 
                            stdout=subprocess.PIPE, shell=True, text=True)
            
            # åˆ›å»ºstream_toæ•°æ®åº“
            if not self.create_database('stream_to'):
                raise Exception("åˆ›å»ºstream_toæ•°æ®åº“å¤±è´¥")
            
            time.sleep(5)
            print("æ•°æ®åº“å·²åˆ›å»º,ç­‰å¾…æ•°æ®å†™å…¥...")
            
            # ç­‰å¾…æ•°æ®å‡†å¤‡å°±ç»ª
            if not self.wait_for_data_ready(cursor, self.table_count, self.histroy_rows):
                print("æ•°æ®å‡†å¤‡å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
                return
            
            # è·å–æ–°è¿æ¥æ‰§è¡Œæµå¼æŸ¥è¯¢
            conn, cursor = self.get_connection()
            
            print("å¼€å§‹è¿æ¥æ•°æ®åº“")
            cursor.execute('use stream_from')
            
            # æ‰§è¡Œæµå¼æŸ¥è¯¢
            print(f"æ‰§è¡Œæµå¼æŸ¥è¯¢SQL:\n{self.stream_sql}")
            cursor.execute(self.stream_sql)
            
            print("æµå¼æŸ¥è¯¢å·²åˆ›å»º,å¼€å§‹ç›‘æ§ç³»ç»Ÿè´Ÿè½½")
            cursor.close()
            conn.close()
            
            # ç›‘æ§ç³»ç»Ÿè´Ÿè½½ - åŒæ—¶ç›‘æ§ä¸‰ä¸ªèŠ‚ç‚¹
            loader = MonitorSystemLoad(
                name_pattern='taosd -c', 
                count=self.runtime * 60,
                perf_file='/tmp/perf-taosd.log',  # åŸºç¡€æ–‡ä»¶å,ä¼šè‡ªåŠ¨æ·»åŠ dnodeç¼–å·
                interval=self.monitor_interval
            )
        
            try:
                loader.get_proc_status()
            except KeyboardInterrupt:
                print("\nç›‘æ§è¢«ä¸­æ–­")
            finally:
                # æ£€æŸ¥taosdè¿›ç¨‹
                result = subprocess.run('ps -ef | grep taosd | grep -v grep', 
                                    shell=True, capture_output=True, text=True)
                if result.stdout:
                    print("\ntaosdè¿›ç¨‹ä»åœ¨è¿è¡Œ")
                    print("å¦‚éœ€åœæ­¢taosdè¿›ç¨‹ï¼Œè¯·æ‰‹åŠ¨æ‰§è¡Œ: pkill taosd")
                
        except Exception as e:
            print(f"æ‰§è¡Œé”™è¯¯: {str(e)}")            
                                    
    def format_timestamp(self, ts):
        """æ ¼å¼åŒ–æ—¶é—´æˆ³ä¸ºå¯è¯»å­—ç¬¦ä¸²
        Args:
            ts: æ¯«ç§’çº§æ—¶é—´æˆ³
        Returns:
            str: æ ¼å¼åŒ–åçš„æ—¶é—´å­—ç¬¦ä¸² (YYYY-MM-DD HH:mm:ss)
        """
        return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(ts/1000))

         
    def do_query_then_insert(self):
        self.prepare_env()
        self.prepare_source_from_data()
        self.insert_source_from_data()
        
        conn = taos.connect(
            host=self.host,
            user=self.user,
            password=self.passwd,
            config=self.conf
        )
        cursor = conn.cursor()

        try:
            # è¿è¡Œsource_fromçš„æ•°æ®ç”Ÿæˆ
            subprocess.Popen('taosBenchmark --f /tmp/stream_from.json', 
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, text=True)
        except subprocess.CalledProcessError as e:
            print(f"Error running Bash command: {e}")

        # åˆ›å»ºstream_toæ•°æ®åº“
        if not self.create_database('stream_to'):
            raise Exception("åˆ›å»ºstream_toæ•°æ®åº“å¤±è´¥")
        
        time.sleep(5)
        print("æ•°æ®åº“å·²åˆ›å»º,ç­‰å¾…æ•°æ®å†™å…¥...")
        # ç­‰å¾…æ•°æ®å‡†å¤‡å°±ç»ª
        if not self.wait_for_data_ready(cursor, self.table_count, self.histroy_rows):
            print("æ•°æ®å‡†å¤‡å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
            return
        
        try:
            # å¯åŠ¨æ€§èƒ½ç›‘æ§çº¿ç¨‹
            loader = MonitorSystemLoad(
                name_pattern='taosd -c', 
                count=self.runtime * 60,
                perf_file='/tmp/perf-taosd-query.log',
                interval=self.monitor_interval,
                deployment_mode=self.deployment_mode 
            )
            
            # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œç›‘æ§
            monitor_thread = threading.Thread(
                target=loader.get_proc_status,
                name="TaosdMonitor"
            )
            monitor_thread.daemon = True
            monitor_thread.start()
            print("å¼€å§‹ç›‘æ§taosdè¿›ç¨‹èµ„æºä½¿ç”¨æƒ…å†µ...")

            # æ•°æ®åº“è¿æ¥å’ŒæŸ¥è¯¢æ“ä½œ
            conn = taos.connect(
                host=self.host, 
                user=self.user, 
                password=self.passwd, 
                config=self.conf, 
                timezone=self.tz
            )
            cursor = conn.cursor()
            cursor.execute('use stream_to')
            
            print("å¼€å§‹æ‰§è¡ŒæŸ¥è¯¢å’Œå†™å…¥æ“ä½œ...")

            cursor.execute("create stable if not exists stream_to.stb_result(wstart timestamp, avg_c0 float, avg_c1 float, avg_c2 float,avg_c3 float, max_c0 float, max_c1 float, max_c2 float,max_c3 float, min_c0 float, min_c1 float, min_c2 float,min_c3 float) tags(gid bigint unsigned)")

            try:
                t = threading.Thread(target=do_monitor, args=(self.runtime * 60, self.perf_file))
                t.daemon = True 
                t.start()
            except Exception as e:
                print("Error: unable to start thread, %s" % e)
            finally:
                print("Execution completed")

            print("start to query")

            list = get_table_list(cursor)
            print("there are %d tables" % len(list))

            try:
                # å¾ªç¯æ‰§è¡Œå†™å…¥å’Œè®¡ç®—
                cycle_count = 0
                start_time = time.time()
                
                while True:
                    cycle_count += 1
                    print(f"\n=== å¼€å§‹ç¬¬ {cycle_count} è½®å†™å…¥å’Œè®¡ç®— ===")
                    
                    if cycle_count > 1:
                        # ä»ç¬¬äºŒè½®å¼€å§‹ï¼Œå…ˆå†™å…¥æ–°æ•°æ®
                        print(f"\nå†™å…¥æ–°ä¸€æ‰¹æµ‹è¯•æ•°æ® (æ¯è½® {self.real_time_batch_rows} æ¡è®°å½•)...")
                        
                        # åº”ç”¨å†™å…¥é—´éš”æ§åˆ¶
                        if self.real_time_batch_sleep > 0:
                            print(f"ç­‰å¾… {self.real_time_batch_sleep} ç§’åå¼€å§‹å†™å…¥æ•°æ®...")
                            time.sleep(self.real_time_batch_sleep)
                
                        if not self.update_insert_config():
                            raise Exception("å†™å…¥æ–°æ•°æ®å¤±è´¥")
                        
                        cmd = "taosBenchmark -f /tmp/stream_from_insertdata.json"
                        if subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, text=True).returncode != 0:
                            raise Exception("å†™å…¥æ–°æ•°æ®å¤±è´¥")
                
                        if self.real_time_batch_sleep > 0:
                            print(f"æ•°æ®å†™å…¥å®Œæˆï¼Œå†™å…¥é—´éš”æ§åˆ¶: {self.real_time_batch_sleep}ç§’")
                
                    for index, table in enumerate(list):
                        table_name = table[0]
                        print(f"\nå¼€å§‹å¤„ç†è¡¨ {table_name} ({index+1}/{len(list)})")
                        window_count = 0
                        
                        count_sql = f"select count(*) from stream_from.{table_name}"
                        cursor.execute(count_sql)
                        count_res = cursor.fetchall()
                        if count_res and count_res[0][0] >= 0:
                            print(f"è¡¨ {table_name} åŒ…å« {count_res[0][0]} æ¡è®°å½•")
                            cursor.execute(f"create table if not exists stream_to.{table_name}_1 using stream_to.stb_result tags(1)")
                            
                            # æŸ¥è¯¢è¡¨çš„æ—¶é—´èŒƒå›´
                            range_sql = f"select first(ts), last(ts) from stream_from.{table_name}"
                            print(f"æŸ¥è¯¢æ—¶é—´èŒƒå›´SQL: {range_sql}")
                            cursor.execute(range_sql)
                            time_range = cursor.fetchall()
                            
                            if time_range and len(time_range) > 0 and time_range[0][0]:
                                start_ts = int(time_range[0][0].timestamp() * 1000)
                                end_ts = int(time_range[0][1].timestamp() * 1000)
                                step = 15 * 1000  # 15ç§’
                                
                                # è®¡ç®—æ€»æ—¶é—´çª—å£æ•°
                                total_windows = ((end_ts - start_ts) // step) + 1
                                print(f"æ•°æ®æ—¶é—´èŒƒå›´: {self.format_timestamp(start_ts)} -> {self.format_timestamp(end_ts)}")
                                print(f"é¢„è®¡å¤„ç† {total_windows} ä¸ªæ—¶é—´çª—å£")
                                
                                # ä½¿ç”¨åˆ—è¡¨ä¿å­˜æ‰€æœ‰æ—¶é—´çª—å£
                                time_windows = []
                                current_ts = start_ts
                                while current_ts < end_ts:
                                    next_ts = min(current_ts + step, end_ts)
                                    time_windows.append((current_ts, next_ts))
                                    current_ts = next_ts
                            
                                for window_idx, (window_start, window_end) in enumerate(time_windows, 1):                                
                                    window_sql = (f"select cast({current_ts} as timestamp), "
                                        f"avg(c0), avg(c1), avg(c2), avg(c3), "
                                        f"max(c0), max(c1), max(c2), max(c3), "
                                        f"min(c0), min(c1), min(c2), min(c3) "
                                        f"from stream_from.{table_name} "
                                        f"where ts >= {window_start} and ts < {window_end}")
                                    
                                    #print(f"æ‰§è¡ŒSQLæŸ¥è¯¢: {window_sql}")
                                    cursor.execute(window_sql)
                                    window_data = cursor.fetchall()
                                    
                                    if window_data and len(window_data) > 0:
                                        # å†™å…¥æ•°æ®
                                        insert_sql = f"insert into stream_to.{table_name}_1 values ({current_ts}, {window_data[0][1]}, {window_data[0][2]}, {window_data[0][3]}, {window_data[0][4]}, {window_data[0][5]}, {window_data[0][6]}, {window_data[0][7]}, {window_data[0][8]}, {window_data[0][9]}, {window_data[0][10]}, {window_data[0][11]}, {window_data[0][12]})"
                                        
                                        cursor.execute(insert_sql)
                                        window_count += 1
                                        
                                        # æ˜¾ç¤ºè¿›åº¦
                                        print(f"\rè¿›åº¦: {(window_count/total_windows)*100:.2f}% - "
                                            f"çª—å£ [{window_count}/{total_windows}]: "
                                            f"{self.format_timestamp(current_ts)} -> "
                                            f"{self.format_timestamp(window_end)}", end='')
                                    else:
                                        print(f" stream_from.{table_name} æ²¡æœ‰æŸ¥è¯¢åˆ°æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(current_ts/1000))} -> {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(query_end/1000))}")
                                        break
                                    
                                    # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´çª—å£
                                    current_ts = window_end
                                    
                                print(f"è¡¨ {table_name} å¤„ç†å®Œæˆ, å…±å†™å…¥ {window_count} ä¸ªæ—¶é—´çª—å£çš„æ•°æ®")
                                
                            else:
                                print(f"è¡¨ {table_name} æ— æ•°æ®è®°å½•ï¼Œè·³è¿‡å¤„ç†")
                
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è¿è¡Œæ—¶é—´é™åˆ¶
                    if time.time() - start_time >= self.runtime * 60:
                        print(f"\n\nå·²è¾¾åˆ°è¿è¡Œæ—¶é—´é™åˆ¶ ({self.runtime} åˆ†é’Ÿ)ï¼Œåœæ­¢æ‰§è¡Œ")
                        break
                        
                    print(f"\n=== ç¬¬ {cycle_count} è½®å¤„ç†å®Œæˆ ===")
                        
            except Exception as e:
                print(f"æŸ¥è¯¢å†™å…¥æ“ä½œå‡ºé”™: {str(e)}")
            finally:
                cursor.close()
                conn.close()
                print("æŸ¥è¯¢å†™å…¥æ“ä½œå®Œæˆ")
        
                # ä¸»åŠ¨åœæ­¢ç›‘æ§
                print("ä¸»åŠ¨åœæ­¢ç³»ç»Ÿç›‘æ§...")
                loader.stop()
                
            # ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
            print("ç­‰å¾…ç›‘æ§æ•°æ®æ”¶é›†å®Œæˆ...")
            monitor_thread.join(timeout=15)  # æœ€å¤šç­‰å¾…15ç§’
    
            if monitor_thread.is_alive():
                print("ç›‘æ§çº¿ç¨‹æœªåœ¨é¢„æœŸæ—¶é—´å†…ç»“æŸï¼Œç¨‹åºå°†ç»§ç»­...")
            else:
                print("ç›‘æ§çº¿ç¨‹å·²æ­£å¸¸ç»“æŸ")
            
        except KeyboardInterrupt:
            print("\næ”¶åˆ°ä¸­æ–­ä¿¡å·")
            print("åœæ­¢ç›‘æ§å’ŒæŸ¥è¯¢æ“ä½œ...")
        except Exception as e:
            print(f"æ‰§è¡Œå‡ºé”™: {str(e)}")
        finally:
            print("\næ‰§è¡Œå®Œæˆ")
            print("ç›‘æ§æ•°æ®å·²ä¿å­˜åˆ°: /tmp/perf-taosd-query-*.log")
            print("å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹ç›‘æ§æ•°æ®:")
            print("cat /tmp/perf-taosd-query-all.log")
                     
    def do_query_then_insert_bak(self):
        self.prepare_env()
        self.prepare_source_from_data()
        
        conn = taos.connect(
            host=self.host,
            user=self.user,
            password=self.passwd,
            config=self.conf
        )
        cursor = conn.cursor()

        try:
            # è¿è¡Œsource_fromçš„æ•°æ®ç”Ÿæˆ
            subprocess.Popen('taosBenchmark --f /tmp/stream_from.json', 
                            stdout=subprocess.PIPE, shell=True, text=True)
        except subprocess.CalledProcessError as e:
            print(f"Error running Bash command: {e}")

        # åˆ›å»ºstream_toæ•°æ®åº“
        if not self.create_database('stream_to'):
            raise Exception("åˆ›å»ºstream_toæ•°æ®åº“å¤±è´¥")
        
        time.sleep(5)
        print("æ•°æ®åº“å·²åˆ›å»º,ç­‰å¾…æ•°æ®å†™å…¥...")
        # ç­‰å¾…æ•°æ®å‡†å¤‡å°±ç»ª
        if not self.wait_for_data_ready(cursor, self.table_count, self.histroy_rows):
            print("æ•°æ®å‡†å¤‡å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
            return
        
        try:
            # å¯åŠ¨æ€§èƒ½ç›‘æ§çº¿ç¨‹
            loader = MonitorSystemLoad(
                name_pattern='taosd -c', 
                count=self.runtime * 60,
                perf_file='/tmp/perf-taosd-query.log',
                interval=self.monitor_interval
            )
            
            # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œç›‘æ§
            monitor_thread = threading.Thread(
                target=loader.get_proc_status,
                name="TaosdMonitor"
            )
            monitor_thread.daemon = True
            monitor_thread.start()
            print("å¼€å§‹ç›‘æ§taosdè¿›ç¨‹èµ„æºä½¿ç”¨æƒ…å†µ...")

            # æ•°æ®åº“è¿æ¥å’ŒæŸ¥è¯¢æ“ä½œ
            conn = taos.connect(
                host=self.host, 
                user=self.user, 
                password=self.passwd, 
                config=self.conf, 
                timezone=self.tz
            )
            cursor = conn.cursor()
            cursor.execute('use stream_to')
            
            print("å¼€å§‹æ‰§è¡ŒæŸ¥è¯¢å’Œå†™å…¥æ“ä½œ...")

            cursor.execute("create stable if not exists stream_to.stb_result(wstart timestamp, avg_c0 float, avg_c1 float, avg_c2 float,avg_c3 float, max_c0 float, max_c1 float, max_c2 float,max_c3 float, min_c0 float, min_c1 float, min_c2 float,min_c3 float) tags(gid bigint unsigned)")

            try:
                t = threading.Thread(target=do_monitor, args=(self.runtime * 60, self.perf_file))
                t.daemon = True 
                t.start()
            except Exception as e:
                print("Error: unable to start thread, %s" % e)
            finally:
                print("Execution completed")

            print("start to query")

            list = get_table_list(cursor)
            print("there are %d tables" % len(list))

            try:
                for index, table in enumerate(list):
                    table_name = table[0]
                    print(f"\nå¼€å§‹å¤„ç†è¡¨ {table_name} ({index+1}/{len(list)})")
                    window_count = 0
                    
                    count_sql = f"select count(*) from stream_from.{table_name}"
                    cursor.execute(count_sql)
                    count_res = cursor.fetchall()
                    if count_res and count_res[0][0] >= 0:
                        print(f"è¡¨ {table_name} åŒ…å« {count_res[0][0]} æ¡è®°å½•")
                        cursor.execute(f"create table if not exists stream_to.{table_name}_1 using stream_to.stb_result tags(1)")
                        
                        # æŸ¥è¯¢è¡¨çš„æ—¶é—´èŒƒå›´
                        range_sql = f"select first(ts), last(ts) from stream_from.{table_name}"
                        print(f"æŸ¥è¯¢æ—¶é—´èŒƒå›´SQL: {range_sql}")
                        cursor.execute(range_sql)
                        time_range = cursor.fetchall()
                        
                        if time_range and len(time_range) > 0 and time_range[0][0]:
                            start_ts = int(time_range[0][0].timestamp() * 1000)
                            end_ts = int(time_range[0][1].timestamp() * 1000)
                            step = 15 * 1000  # 15ç§’
                            
                            # è®¡ç®—æ€»æ—¶é—´çª—å£æ•°
                            total_windows = ((end_ts - start_ts) // step) + 1
                            print(f"æ•°æ®æ—¶é—´èŒƒå›´: {self.format_timestamp(start_ts)} -> {self.format_timestamp(end_ts)}")
                            print(f"é¢„è®¡å¤„ç† {total_windows} ä¸ªæ—¶é—´çª—å£")
                            
                            # ä½¿ç”¨åˆ—è¡¨ä¿å­˜æ‰€æœ‰æ—¶é—´çª—å£
                            time_windows = []
                            current_ts = start_ts
                            while current_ts < end_ts:
                                next_ts = min(current_ts + step, end_ts)
                                time_windows.append((current_ts, next_ts))
                                current_ts = next_ts
                        
                            for window_idx, (window_start, window_end) in enumerate(time_windows, 1):                                
                                window_sql = (f"select cast({current_ts} as timestamp), "
                                    f"avg(c0), avg(c1), avg(c2), avg(c3), "
                                    f"max(c0), max(c1), max(c2), max(c3), "
                                    f"min(c0), min(c1), min(c2), min(c3) "
                                    f"from stream_from.{table_name} "
                                    f"where ts >= {window_start} and ts < {window_end}")
                                
                                #print(f"æ‰§è¡ŒSQLæŸ¥è¯¢: {window_sql}")
                                cursor.execute(window_sql)
                                window_data = cursor.fetchall()
                                
                                if window_data and len(window_data) > 0:
                                    # å†™å…¥æ•°æ®
                                    insert_sql = f"insert into stream_to.{table_name}_1 values ({current_ts}, {window_data[0][1]}, {window_data[0][2]}, {window_data[0][3]}, {window_data[0][4]}, {window_data[0][5]}, {window_data[0][6]}, {window_data[0][7]}, {window_data[0][8]}, {window_data[0][9]}, {window_data[0][10]}, {window_data[0][11]}, {window_data[0][12]})"
                                    
                                    cursor.execute(insert_sql)
                                    window_count += 1
                                    
                                    # æ˜¾ç¤ºè¿›åº¦
                                    print(f"\rè¿›åº¦: {(window_count/total_windows)*100:.2f}% - "
                                        f"çª—å£ [{window_count}/{total_windows}]: "
                                        f"{self.format_timestamp(current_ts)} -> "
                                        f"{self.format_timestamp(window_end)}", end='')
                                else:
                                    print(f" stream_from.{table_name} æ²¡æœ‰æŸ¥è¯¢åˆ°æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(current_ts/1000))} -> {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(query_end/1000))}")
                                    break
                                
                                # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´çª—å£
                                current_ts = window_end
                                
                            print(f"è¡¨ {table_name} å¤„ç†å®Œæˆ, å…±å†™å…¥ {window_count} ä¸ªæ—¶é—´çª—å£çš„æ•°æ®")
                            
                        else:
                            print(f"è¡¨ {table_name} æ— æ•°æ®è®°å½•ï¼Œè·³è¿‡å¤„ç†")
                        
            except Exception as e:
                print(f"æŸ¥è¯¢å†™å…¥æ“ä½œå‡ºé”™: {str(e)}")
            finally:
                cursor.close()
                conn.close()
                print("æŸ¥è¯¢å†™å…¥æ“ä½œå®Œæˆ")
                
            # ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
            print("ç­‰å¾…ç›‘æ§æ•°æ®æ”¶é›†å®Œæˆ...")
            monitor_thread.join()
            
        except KeyboardInterrupt:
            print("\næ”¶åˆ°ä¸­æ–­ä¿¡å·")
            print("åœæ­¢ç›‘æ§å’ŒæŸ¥è¯¢æ“ä½œ...")
        except Exception as e:
            print(f"æ‰§è¡Œå‡ºé”™: {str(e)}")
        finally:
            print("\næ‰§è¡Œå®Œæˆ")
            print("ç›‘æ§æ•°æ®å·²ä¿å­˜åˆ°: /tmp/perf-taosd-query-*.log")
            print("å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹ç›‘æ§æ•°æ®:")
            print("cat /tmp/perf-taosd-query-all.log")
        
    def do_query_then_insert_no_monitor(self):
        self.prepare_env()
        self.prepare_source_from_data()

        try:
            # è¿è¡Œsource_fromçš„æ•°æ®ç”Ÿæˆ
            subprocess.Popen('taosBenchmark --f /tmp/stream_from.json', 
                            stdout=subprocess.PIPE, shell=True, text=True)
        except subprocess.CalledProcessError as e:
            print(f"Error running Bash command: {e}")

        # åˆ›å»ºstream_toæ•°æ®åº“
        if not self.create_database('stream_to'):
            raise Exception("åˆ›å»ºstream_toæ•°æ®åº“å¤±è´¥")
        
        time.sleep(10)
        print("æ•°æ®åº“å·²åˆ›å»º,ç­‰å¾…æ•°æ®å†™å…¥...")

        conn = taos.connect(
            host=self.host, user=self.user, password=self.passwd, config=self.conf, timezone=self.tz
        )

        cursor = conn.cursor()
        cursor.execute('use stream_to')

        start_ts = 1748707200000
        step = 300 # å°†æ­¥é•¿æ”¹ä¸º300ç§’

        cursor.execute("create stable if not exists stream_to.stb_result(wstart timestamp, avg_c0 float, avg_c1 float, avg_c2 float,avg_c3 float, max_c0 float, max_c1 float, max_c2 float,max_c3 float, min_c0 float, min_c1 float, min_c2 float,min_c3 float) tags(gid bigint unsigned)")

        try:
            t = threading.Thread(target=do_monitor, args=(self.runtime * 60, self.perf_file))
            t.daemon = True 
            t.start()
        except Exception as e:
            print("Error: unable to start thread, %s" % e)
        finally:
            print("Execution completed")

        print("start to query")

        list = get_table_list(cursor)
        print("there are %d tables" % len(list))

        for index, n in enumerate(list):
            cursor.execute(f"create table if not exists stream_to.{n[0]}_1 using stream_to.stb_result tags(1)")
            count = 1
            while True:
                sql = (f"select cast({start_ts + step * 1000 * (count - 1)} as timestamp), "
                        f"avg(c0), avg(c1), avg(c2), avg(c3), "
                        f"max(c0), max(c1), max(c2), max(c3), "
                        f"min(c0), min(c1), min(c2), min(c3) "
                        f"from stream_from.{n[0]} "
                        f"where ts >= {start_ts + step * 1000 * (count - 1)} "
                        f"and ts < {start_ts + step * 1000 * count}")
                print(f"æ‰§è¡ŒSQLæŸ¥è¯¢: {sql}")
                cursor.execute(sql)

                res = cursor.fetchall()
                if not res or len(res) == 0:  # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ
                    print(f"æ²¡æœ‰æŸ¥è¯¢åˆ°æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {start_ts + step * 1000 * (count - 1)} -> {start_ts + step * 1000 * count}")
                    break

                insert = f"insert into stream_to.{n[0]}_1 values ({start_ts + step * 1000 * (count - 1)}, {res[0][1]}, {res[0][2]}, {res[0][3]}, {res[0][4]}, {res[0][5]}, {res[0][6]}, {res[0][7]}, {res[0][8]}, {res[0][9]}, {res[0][10]}, {res[0][11]}, {res[0][12]})"
                print(f"Inserting: {insert}")
                cursor.execute(insert)
                count += 1
        conn.close()

    def multi_insert(self):
        self.prepare_env()
        self.prepare_source_from_data()

        try:
            subprocess.Popen('taosBenchmark --f /tmp/stream_from.json', stdout=subprocess.PIPE, shell=True, text=True)
            subprocess.Popen('taosBenchmark --f /tmp/stream_to.json', stdout=subprocess.PIPE, shell=True, text=True)
        except subprocess.CalledProcessError as e:
            print(f"Error running Bash command: {e}")

        time.sleep(10)

        for n in range(5):
            try:
                print(f"start query_insert thread {n}")
                t = threading.Thread(target=do_multi_insert, args=(n, 100, self.host, self.user, self.passwd, self.conf, self.tz))
                t.start()
            except Exception as e:
                print("Error: unable to start thread, %s" % e)

        loader = MonitorSystemLoad('taosd', self.runtime * 60, self.perf_file)
        loader.get_proc_status()

    
def main():
    def signal_handler(signum, frame):
        """ä¸»ç¨‹åºçš„ä¿¡å·å¤„ç†å™¨"""
        print("\næ”¶åˆ°ä¸­æ–­ä¿¡å·")
        print("æ­£åœ¨åœæ­¢ç›‘æ§,ä½†ä¿æŒtaosdè¿›ç¨‹è¿è¡Œ...")
        return  # ç›´æ¥è¿”å›ï¼Œä¸æ‰§è¡Œä»»ä½•åœæ­¢æ“ä½œ

    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # ä½¿ç”¨è‡ªå®šä¹‰æ ¼å¼åŒ–å™¨æ¥æ”¹å–„å¸®åŠ©ä¿¡æ¯æ˜¾ç¤º
    class CustomHelpFormatter(argparse.RawDescriptionHelpFormatter):
        def __init__(self, prog):
            super().__init__(prog, max_help_position=50, width=150)
            
        def _format_action_invocation(self, action):
            # ä¿æŒåŸæœ‰çš„è°ƒç”¨æ ¼å¼
            return super()._format_action_invocation(action)
        
        def _format_usage(self, usage, actions, groups, prefix):
            """é‡å†™ usage æ ¼å¼åŒ–ï¼Œæ”¯æŒå¤šè¡Œæ˜¾ç¤º"""
            if prefix is None:
                prefix = 'usage: '
            
            # è·å–ç¨‹åºå
            prog = usage or self._prog
            
            # æ„å»ºå‚æ•°åˆ—è¡¨
            parts = [prog]
            
            # åˆ†ç»„æ˜¾ç¤ºå‚æ•°
            optional_parts = []
            positional_parts = []
            
            for action in actions:
                if action.option_strings:
                    # å¯é€‰å‚æ•°
                    if action.option_strings:
                        opt_str = '/'.join(action.option_strings)
                        if action.nargs != 0:
                            opt_str += f' {action.dest.upper()}'
                        optional_parts.append(f'[{opt_str}]')
                else:
                    # ä½ç½®å‚æ•°
                    positional_parts.append(action.dest.upper())
            
            # ç»„åˆæ‰€æœ‰éƒ¨åˆ†
            all_parts = parts + positional_parts + optional_parts
            
            # æŒ‰è¡Œåˆ†ç»„æ˜¾ç¤º
            lines = [prefix + prog]
            current_line = ''
            max_line_length = 80
            
            for part in optional_parts:
                if len(current_line + ' ' + part) > max_line_length:
                    if current_line:
                        lines.append(' ' * len(prefix) + current_line)
                        current_line = part
                    else:
                        lines.append(' ' * len(prefix) + part)
                else:
                    current_line = current_line + ' ' + part if current_line else part
            
            if current_line:
                lines.append(' ' * len(prefix) + current_line)
                
            return '\n'.join(lines) + '\n\n'
        
        def _fill_text(self, text, width, indent):
            # ä¿æŒæ–‡æœ¬çš„åŸå§‹æ ¼å¼ï¼Œä¸è¿›è¡Œè‡ªåŠ¨æ¢è¡Œ
            return ''.join(indent + line for line in text.splitlines(keepends=True))
        
        def _split_lines(self, text, width):
            # æŒ‰ç…§ \n åˆ†å‰²è¡Œï¼Œä¿æŒæ‰‹åŠ¨æ¢è¡Œ
            if '\n' in text:
                return text.splitlines()
            else:
                # å¯¹äºæ²¡æœ‰æ¢è¡Œç¬¦çš„æ–‡æœ¬ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†
                return argparse.HelpFormatter._split_lines(self, text, width)
    
    parser = argparse.ArgumentParser(
        prog=' python3 stream_perf_test.py -m 1 --stream-perf-test-dir /home/stream_perf_test_dir',
        description='TDengine Stream Perf Test',
        formatter_class=CustomHelpFormatter,
        epilog="""
ä¸Šé¢å‚æ•°å¯ä»¥è¿›è¡Œæ­é…ç»„åˆè¿›è¡Œæµ‹è¯•
        """
    )
    
    # åŸºç¡€è¿è¡Œå‚æ•°
    basic_group = parser.add_argument_group('åŸºç¡€å‚æ•°ï¼Œæ§åˆ¶æµ‹è¯•æ¨¡å¼ã€è¿è¡Œæ—¶é—´ç­‰æ ¸å¿ƒå‚æ•°')
    # basic_group.add_argument('-m', '--mode', type=int, default=0,
    #                         help='è¿è¡Œæ¨¡å¼:\n'
    #                             '  1: do_test_stream_with_realtime_data (å†™å…¥æ•°æ®å¹¶æ‰§è¡Œå®æ—¶æ•°æ®æµè®¡ç®—)\n'
    #                             '  2: do_query_then_insert (æŒç»­æŸ¥è¯¢å†™å…¥)\n'
    #                             '  3: multi_insert (å¤šçº¿ç¨‹æ’å…¥)')
    basic_group.add_argument('-m', '--mode', type=int, default=0,
                            # help='è¿è¡Œæ¨¡å¼:\n'
                            #     '  1: do_test_stream_with_realtime_data (å†™å…¥æ•°æ®å¹¶æ‰§è¡Œå®æ—¶æ•°æ®æµè®¡ç®—)\n'
                            #     '  2: do_query_then_insert (æŒç»­æŸ¥è¯¢å†™å…¥)(å’Œæµè®¡ç®—æ— å…³)\n'
                            #     '  3: do_test_stream_with_restored_data (æ¢å¤å†å²æ•°æ®å¹¶æµ‹è¯•æŒ‡å®šæµè®¡ç®—)')
                            help='''è¿è¡Œæ¨¡å¼ (å¿…é€‰):
  1 = do_test_stream_with_realtime_data: åˆ›å»ºæ•°æ®å¹¶æ‰§è¡Œå®æ—¶æµè®¡ç®—æµ‹è¯•
      â”œâ”€ è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•æ•°æ®
      â”œâ”€ åˆ›å»ºå¹¶å¯åŠ¨æµè®¡ç®—  
      â”œâ”€ æ¨¡æ‹ŸæŒç»­å®æ—¶å†™å…¥æ•°æ®
      â””â”€ å…¨ç¨‹æ€§èƒ½ç›‘æ§
      
  2 = do_test_stream_with_restored_data: åŠ è½½å†å²æ•°æ®å¹¶æ‰§è¡Œæµè®¡ç®—æµ‹è¯•
      â”œâ”€ ä»å¤‡ä»½æ¢å¤å†å²æ•°æ®
      â”œâ”€ æµ‹è¯•æŒ‡å®šçš„æµè®¡ç®—SQL  
      â””â”€ é¿å…é‡å¤æ•°æ®ç”Ÿæˆï¼ŒèŠ‚çœæ—¶é—´
      
  3 = do_query_then_insert: æŒç»­æŸ¥è¯¢å†™å…¥(å’Œæµè®¡ç®—æ— å…³)
      â”œâ”€ æŒç»­å†™å…¥æ•°æ®
      â”œâ”€ æŒç»­æ‰§è¡ŒèšåˆæŸ¥è¯¢
      â””â”€ ç”¨äºå¯¹æ¯”æµè®¡ç®—ä¸æŸ¥è¯¢æ€§èƒ½å·®å¼‚
      
é»˜è®¤: 0 (æ˜¾ç¤ºå¸®åŠ©)\n''')
    basic_group.add_argument('-t', '--time', type=int, default=10,
                            help='è¿è¡Œæ—¶é—´(åˆ†é’Ÿ), é»˜è®¤10åˆ†é’Ÿ\n')
    basic_group.add_argument('-f', '--file', type=str, default='/tmp/perf.log',
                            help='æ€§èƒ½æ•°æ®è¾“å‡ºæ–‡ä»¶è·¯å¾„, é»˜è®¤/tmp/perf.log')
    
    # æ•°æ®ç›¸å…³å‚æ•°
    data_group = parser.add_argument_group('æ•°æ®å‚æ•°, æ§åˆ¶æ•°æ®è§„æ¨¡å’Œå†™å…¥é€Ÿåº¦')
    data_group.add_argument('--table-count', type=int, default=500,
                            help='å­è¡¨æ•°é‡, é»˜è®¤500, æµ‹è¯•å¤§æ•°æ®æ—¶éœ€è°ƒå¤§æ­¤å‚æ•°')
    data_group.add_argument('--histroy-rows', type=int, default=1,
                            help='æ¯ä¸ªå­è¡¨æ’å…¥å†å²æ•°æ®æ•°, é»˜è®¤1')
    data_group.add_argument('--real-time-batch-rows', type=int, default=200,
                            help='åç»­æ¯ä¸ªå­è¡¨æ¯è½®æ’å…¥å®æ—¶æ•°æ®æ•°, é»˜è®¤200ã€timestamp_step = 50ã€‘')
    data_group.add_argument('--real-time-batch-sleep', type=float, default=0,
                            help='æ•°æ®å†™å…¥é—´éš”æ—¶é—´(ç§’), é»˜è®¤0ç§’\n'
                                '    æ§åˆ¶æ¯è½®æ•°æ®å†™å…¥ä¹‹é—´çš„ç­‰å¾…æ—¶é—´\n'
                                '    ç”¨äºè°ƒèŠ‚å†™å…¥å‹åŠ›ï¼Œè§‚å¯Ÿæµè®¡ç®—å»¶è¿Ÿå˜åŒ–\n'
                                '    å»ºè®®å€¼: 0(æ— æ§åˆ¶), 1-10(è½»åº¦æ§åˆ¶), 10+(é‡åº¦æ§åˆ¶)\n'
                                '    ä¾‹å¦‚: --real-time-batch-sleep 5')
    data_group.add_argument('--disorder-ratio', type=int, default=0,
                            help='æ•°æ®ä¹±åºç‡, é»˜è®¤0æ— ä¹±åº, ä¹±åºè¿‡å¤šå½±å“æµè®¡ç®—é€Ÿåº¦')
    data_group.add_argument('--vgroups', type=int, default=10,
                            help='''vgroupsæ•°é‡, é»˜è®¤10\n
ç¤ºä¾‹ç”¨æ³•:%(prog)s --table-count 10000 --histroy-rows 1000 \n --real-time-batch-rows 500 --real-time-batch-sleep 5 --disorder-ratio 1 --vgroups 20 --sql-type sliding_stb --time 60\n\n''')
    
    # SQLç›¸å…³å‚æ•°
    sql_group = parser.add_argument_group('æµè®¡ç®—SQLé…ç½®')
    
    # old for phase 1
    # sql_group.add_argument('--sql-type', type=str, default='s2_7',
    #                     choices=['s2_2', 's2_3', 's2_4', 's2_5', 's2_6', 
    #                              's2_7', 's2_8', 's2_9', 's2_10', 's2_11', 
    #                              's2_12', 's2_13', 's2_14', 's2_15', 'all'],
    #                      help='å®æ—¶æµè®¡ç®—SQLç±»å‹:\n'
    #                           '  s2_2: trigger at_once\n'
    #                           '  s2_3: trigger window_close\n'
    #                           '  s2_4: trigger max_delay 5s\n'
    #                           '  s2_5: trigger FORCE_WINDOW_CLOSE\n'
    #                           '  s2_6: trigger CONTINUOUS_WINDOW_CLOSE\n'
    #                           '  s2_7: INTERVAL(15s) çª—å£\n'
    #                           '  s2_8: INTERVAL with %%trows\n'
    #                           '  s2_9: INTERVAL with MAX_DELAY\n'
    #                           '  s2_10: INTERVAL with MAX_DELAY and %%trows\n'
    #                           '  s2_11: PERIOD(15s) å®šæ—¶è§¦å‘\n'
    #                           '  s2_12: SESSION(ts,10a) ä¼šè¯çª—å£\n'
    #                           '  s2_13: COUNT_WINDOW(1000) è®¡æ•°çª—å£\n'
    #                           '  s2_14: EVENT_WINDOW äº‹ä»¶çª—å£\n'
    #                           '  s2_15: STATE_WINDOW çŠ¶æ€çª—å£\n'
    #                           '  all: æ‰¹é‡æ‰§è¡Œå¤šç§æµç±»å‹')
    
    def validate_sql_type(value):
        """éªŒè¯ SQL ç±»å‹å‚æ•°"""
        valid_choices = [
            'select_stream', 'all_detailed',
            'tbname_agg', 'tbname_select', 'trows_agg', 'trows_select',
            'sliding_stb', 'sliding_stb_partition_by_tbname', 'sliding_stb_partition_by_tag', 'sliding_tb',
            'session_stb', 'session_stb_partition_by_tbname', 'session_stb_partition_by_tag', 'session_tb',
            'count_stb', 'count_stb_partition_by_tbname', 'count_stb_partition_by_tag', 'count_tb',
            'event_stb', 'event_stb_partition_by_tbname', 'event_stb_partition_by_tag', 'event_tb',
            'state_stb', 'state_stb_partition_by_tbname', 'state_stb_partition_by_tag', 'state_tb',
            'period_stb', 'period_stb_partition_by_tbname', 'period_stb_partition_by_tag', 'period_tb',
            'sliding_detailed', 'session_detailed', 'count_detailed',
            'event_detailed', 'state_detailed', 'period_detailed'
        ]
        
        if value not in valid_choices:
            # æŒ‰ç±»åˆ«æ ¼å¼åŒ–é”™è¯¯ä¿¡æ¯
            error_msg = f"invalid choice: '{value}'\n\næœ‰æ•ˆé€‰æ‹©é¡¹:\n"
            error_msg += "è¯´æ˜: stb(è¶…çº§è¡¨), tb(å­è¡¨), by_tbname(tbnameåˆ†ç»„), by_tag(tagåˆ†ç»„,ç¬¬ä¸€åˆ—tag)\n"
            error_msg += "æŸ¥è¯¢ç±»å‹: select_stream æŸ¥è¯¢æ‰€æœ‰æµä¿¡æ¯\n"
            error_msg += "å›ºå®šå‚æ•°ç»„åˆ: tbname_agg, tbname_select, trows_agg, trows_select\n"
            error_msg += "æ»‘åŠ¨çª—å£: sliding_stb, sliding_stb_partition_by_tbname, sliding_stb_partition_by_tag, sliding_tb\n"
            error_msg += "ä¼šè¯çª—å£: session_stb, session_stb_partition_by_tbname, session_stb_partition_by_tag, session_tb\n"
            error_msg += "è®¡æ•°çª—å£: count_stb, count_stb_partition_by_tbname, count_stb_partition_by_tag, count_tb\n"
            error_msg += "äº‹ä»¶çª—å£: event_stb, event_stb_partition_by_tbname, event_stb_partition_by_tag, event_tb\n"
            error_msg += "çŠ¶æ€çª—å£: state_stb, state_stb_partition_by_tbname, state_stb_partition_by_tag, state_tb\n"
            error_msg += "å®šæ—¶è§¦å‘: period_stb, period_stb_partition_by_tbname, period_stb_partition_by_tag, period_tb\n"
            error_msg += "ç»„åˆæ¨¡æ¿: sliding_detailed, session_detailed, count_detailed, event_detailed, state_detailed, period_detailed, all_detailed"
            
            raise argparse.ArgumentTypeError(error_msg)
        return value

    # ä½¿ç”¨æ—¶ä¸è®¾ç½® choicesï¼Œè€Œæ˜¯ä½¿ç”¨ type å‚æ•°è¿›è¡ŒéªŒè¯
    sql_group.add_argument('--sql-type', type=validate_sql_type, default='select_stream',
                        help='''å®æ—¶æµè®¡ç®—SQLç±»å‹:
    è¯´æ˜:
        stb(è¶…çº§è¡¨), tb(å­è¡¨), by_tbname(tbnameåˆ†ç»„), by_tag(tagåˆ†ç»„,ç¬¬ä¸€åˆ—tag)
    æŸ¥è¯¢ç±»å‹:
        select_stream: æŸ¥è¯¢æ‰€æœ‰æµä¿¡æ¯ (é»˜è®¤æŸ¥è¯¢information_schema.ins_streamsçš„æ•°æ®)
    å›ºå®šå‚æ•°ç»„åˆæ¨¡æ¿(å¿½ç•¥ --tbname-or-trows å’Œ --agg-or-select å‚æ•°):
        tbname_agg:    æ‰€æœ‰çª—å£ç±»å‹ + tbname + èšåˆæŸ¥è¯¢
        tbname_select: æ‰€æœ‰çª—å£ç±»å‹ + tbname + æŠ•å½±æŸ¥è¯¢  
        trows_agg:     æ‰€æœ‰çª—å£ç±»å‹ + trows +  èšåˆæŸ¥è¯¢
        trows_select:  æ‰€æœ‰çª—å£ç±»å‹ + trows +  æŠ•å½±æŸ¥è¯¢
    æ»‘åŠ¨çª—å£æ¨¡æ¿(INTERVAL(15s) SLIDING(15s)):
        sliding_stb, sliding_stb_partition_by_tbname, sliding_stb_partition_by_tag, sliding_tb
    ä¼šè¯çª—å£æ¨¡æ¿(SESSION(ts,10a)):
        session_stb, session_stb_partition_by_tbname, session_stb_partition_by_tag, session_tb
    è®¡æ•°çª—å£æ¨¡æ¿(COUNT_WINDOW(1000)):
        count_stb, count_stb_partition_by_tbname, count_stb_partition_by_tag, count_tb
    äº‹ä»¶çª—å£æ¨¡æ¿(EVENT_WINDOW(START WITH c0 > -10000000 END WITH c0 < 10000000)):
        event_stb, event_stb_partition_by_tbname, event_stb_partition_by_tag, event_tb
    çŠ¶æ€çª—å£æ¨¡æ¿(STATE_WINDOW(c0)):
        state_stb, state_stb_partition_by_tbname, state_stb_partition_by_tag, state_tb
    å®šæ—¶è§¦å‘æ¨¡æ¿(PERIOD(15s)):
        period_stb, period_stb_partition_by_tbname, period_stb_partition_by_tag, period_tb
    ç»„åˆæ¨¡æ¿(æ¯ç»„åŒ…å«ä¸Šè¿°4ä¸ªç»„åˆ,allåŒ…å«ä¸Šè¿°æ‰€æœ‰ç»„åˆ):
        sliding_detailed, session_detailed, count_detailed, event_detailed, 
        state_detailed, period_detailed, all_detailed''')
    
    sql_group.add_argument('--stream-num', type=int, default=1,
                        help='æµæ•°é‡: å½“ sql-type ä¸ºéç»„åˆæ¨¡ç‰ˆæ—¶ï¼Œåˆ›å»ºæŒ‡å®šæ•°é‡çš„ç›¸åŒæµ\n'
                                '    å¹¶å‘æµæ•°é‡, é»˜è®¤1ä¸ªæµ\n'
                                '    è®¾ç½®ä¸ºNæ—¶ï¼Œä¼šåˆ›å»ºNä¸ªç¼–å·ä¸º1åˆ°Nçš„ç›¸åŒæµï¼Œå¦‚æœæ˜¯ä»å­è¡¨è·å–æ•°æ®åˆ™ä»ç¬¬ä¸€ä¸ªå­è¡¨åˆ°ç¬¬Nä¸ªå­è¡¨\n'
                                '    éœ€è¦å’Œ --sql-type æ­é…ä½¿ç”¨ï¼Œé€‚åˆæµ‹è¯•å¤šä¸ªæµçš„å‹åŠ›\n'
                                '    ç¤ºä¾‹: --sql-type sliding_stb --stream-num 100')
    
    sql_group.add_argument('--stream-sql', type=str,
                        help='è‡ªå®šä¹‰æµè®¡ç®—SQL (ä¼˜å…ˆçº§æœ€é«˜)\n'
                                '    ç¤ºä¾‹: "create stream test_stream..."')
    
    sql_group.add_argument('--agg-or-select', type=str, default='agg',
                        choices=['agg', 'select'],
                        help='æŸ¥è¯¢ç±»å‹:\n'
                            '    agg: èšåˆæŸ¥è¯¢ (avg,max,min)\n'
                            '         avg(c0), avg(c1), avg(c2), avg(c3), max(c0), max(c1), max(c2), max(c3), min(c0), min(c1), min(c2), min(c3)\n'
                            '    select: æŠ•å½±æŸ¥è¯¢ (c0,c1,c2,c3)')
    sql_group.add_argument('--custom-columns', type=str,
                        help='è‡ªå®šä¹‰æŸ¥è¯¢åˆ— (é€—å·åˆ†éš”):\n'
                                '    ç¤ºä¾‹: "sum(c0),count(*),avg(c1)"\n'
                                '          "c0,c1,c2" (æŠ•å½±æŸ¥è¯¢)\n'
                                '    æ³¨æ„: ä¼šè¦†ç›– --agg-or-select è®¾ç½®')
    sql_group.add_argument('--tbname-or-trows', type=str, default='tbname',
                        choices=['tbname', 'trows'],
                        help='FROMå­å¥ç±»å‹ï¼Œé»˜è®¤%%tbname:\n'
                            '    tbname: from %%tbname where ts >= _twstart and ts <= _twend\n'
                            '    trows:  from %%trows ')
    
    # todo
    sql_group.add_argument('--sql-file', type=str,
                        help='ä»æ–‡ä»¶è¯»å–æµå¼æŸ¥è¯¢SQLï¼Œtodo\n'
                            '''    ç¤ºä¾‹: --sql-file ./my_stream.sql\n
ç¤ºä¾‹ç”¨æ³•1:%(prog)s --table-count 10000 \n --real-time-batch-rows 500 --real-time-batch-sleep 5 --sql-type sliding_stb --time 60\n
ç¤ºä¾‹ç”¨æ³•2:%(prog)s --table-count 10000 \n --real-time-batch-rows 500 --real-time-batch-sleep 5 --sql-type sliding_detailed --time 60\n
ç¤ºä¾‹ç”¨æ³•3:%(prog)s --table-count 10000 \n --real-time-batch-rows 500 --real-time-batch-sleep 5 --sql-type tbname_agg --time 60\n
ç¤ºä¾‹ç”¨æ³•4:%(prog)s --table-count 10000 \n --real-time-batch-rows 500 --real-time-batch-sleep 5 --sql-type sliding_detailed --tbname-or-trows trows\n
ç¤ºä¾‹ç”¨æ³•5:%(prog)s --table-count 10000 \n --real-time-batch-rows 500 --real-time-batch-sleep 5 --sql-type sliding_detailed --agg-or-select select\n
ç¤ºä¾‹ç”¨æ³•6:%(prog)s --table-count 10000 \n --real-time-batch-rows 500 --real-time-batch-sleep 5 --sql-type sliding_stb--stream-num 100\n\n''')
    
    # æµæ€§èƒ½ç›‘æ§å‚æ•°
    stream_monitor_group = parser.add_argument_group('æµæ€§èƒ½ç›‘æ§å’Œæµè®¡ç®—å»¶è¿Ÿæ£€æµ‹')
    stream_monitor_group.add_argument('--check-stream-delay', action='store_true',
                                    help='''å¯ç”¨æµè®¡ç®—å»¶è¿Ÿç›‘æ§,é»˜è®¤ä¸å¼€å¯,åŠ ä¸Šæ­¤å‚æ•°åä¼šè‡ªåŠ¨ç›‘æ§æµè®¡ç®—å»¶è¿Ÿ:
  
åŠŸèƒ½: å®æ—¶ç›‘æµ‹æºè¡¨ä¸ç›®æ ‡è¡¨çš„æ•°æ®å»¶è¿Ÿ
  â”œâ”€ æ™ºèƒ½å»¶è¿Ÿåˆ†çº§: ä¼˜ç§€/è‰¯å¥½/æ­£å¸¸/è½»å¾®/æ˜æ˜¾/ä¸¥é‡
  â”œâ”€ åŠ¨æ€é˜ˆå€¼è®¡ç®—: åŸºäºæ£€æŸ¥é—´éš”è‡ªåŠ¨è°ƒæ•´  
  â”œâ”€ é—®é¢˜è¯Šæ–­: è‡ªåŠ¨è¯†åˆ«æ— æ•°æ®/è¡¨ä¸å­˜åœ¨ç­‰é—®é¢˜
  â””â”€ ä¼˜åŒ–å»ºè®®: æä¾›å…·ä½“çš„æ€§èƒ½è°ƒä¼˜å»ºè®®

è¾“å‡º: è¯¦ç»†å»¶è¿ŸæŠ¥å‘Šä¿å­˜ä¸º /tmp/*-stream-delay.log''')
    stream_monitor_group.add_argument('--max-delay-threshold', type=int, default=30000,
                                    help='æœ€å¤§å…è®¸å»¶è¿Ÿæ—¶é—´(æ¯«ç§’), é»˜è®¤30000ms(30ç§’)\n'
                                        'è¶…è¿‡æ­¤é˜ˆå€¼è®¤ä¸ºæµè®¡ç®—è·Ÿä¸ä¸Šæ•°æ®å†™å…¥é€Ÿåº¦ï¼Œå¯ä»¥é’ˆå¯¹å…·ä½“çš„æµå’Œé—´éš”è°ƒæ•´')
    stream_monitor_group.add_argument('--delay-check-interval', type=int, default=10,
                                    help='''å»¶è¿Ÿæ£€æŸ¥é—´éš”(ç§’), é»˜è®¤10ç§’, ç›®å‰æ˜¯ç²—ç²’åº¦çš„æ£€æŸ¥ç”Ÿæˆç›®æ ‡è¡¨è¶…çº§è¡¨çš„last(ts), è€Œéæ¯ä¸ªç”Ÿæˆå­è¡¨çš„last(ts)\n
ç¤ºä¾‹ç”¨æ³•:%(prog)s --check-stream-delay \n--max-delay-threshold 60000 --delay-check-interval 5 --sql-type sliding_stb \n\n''')
    
    # ç³»ç»Ÿé…ç½®å‚æ•°
    system_group = parser.add_argument_group('ç³»ç»Ÿé…ç½®ï¼Œé…ç½®TDengineéƒ¨ç½²æ¶æ„å’Œç³»ç»Ÿå‚æ•°')
    system_group.add_argument('--stream-perf-test-dir', type=str, 
                            default='/home/stream_perf_test_dir',
                            help='å•æœºæˆ–è€…é›†ç¾¤æµ‹è¯•æ ¹ç›®å½•, é»˜è®¤/home/stream_perf_test_dir, å°½é‡é…ç½®æ­¤å‚æ•°é¿å…è¯¯åˆ ç›®å½•å¤–çš„æ–‡ä»¶\n'
                                 'å­˜å‚¨é…ç½®æ–‡ä»¶ã€æ•°æ®æ–‡ä»¶ã€æ—¥å¿—æ–‡ä»¶\n'
                                 'ç¡®ä¿ç›®å½•æœ‰è¶³å¤Ÿç©ºé—´å’Œè¯»å†™æƒé™')
    system_group.add_argument('--monitor-interval', type=int, default=5,
                            help='æ€§èƒ½æ•°æ®é‡‡é›†é—´éš”(ç§’), é»˜è®¤5ç§’\n'
                                  'ç›‘æ§CPUã€å†…å­˜ã€ç£ç›˜IOä½¿ç”¨æƒ…å†µ\nè¿™ä¸ªæ˜¯ç›‘æ§taosdå ç”¨èµ„æºçš„ï¼Œdelay-check-intervalæ˜¯æ£€æŸ¥æµæ•°æ®ç”Ÿæˆçš„\n'
                                  'å»ºè®®å€¼: 1(è¯¦ç»†), 5(æ ‡å‡†), 10(ç²—ç•¥)')
    system_group.add_argument('--deployment-mode', type=str, default='single',
                            choices=['single', 'cluster'],
                            help='éƒ¨ç½²æ¨¡å¼:é»˜è®¤single\n'
                                '    single:  å•èŠ‚ç‚¹æ¨¡å¼ï¼Œdnode+mnode+snode éƒ½åœ¨ä¸€ä¸ªèŠ‚ç‚¹\n'
                                '    cluster: ä¸‰èŠ‚ç‚¹é›†ç¾¤æ¨¡å¼ï¼Œ3ä¸ªdnodeï¼Œ3ä¸ªmnodeï¼Œæºæ•°æ®åº“vgroupséƒ½åœ¨dnode1ï¼Œç›®æ ‡æ•°æ®åº“vgroupséƒ½åœ¨dnode2ï¼Œsnodeç‹¬ç«‹åœ¨dnode3')
    system_group.add_argument('--debug-flag', type=int, default=131,
                            help='TDengineè°ƒè¯•çº§åˆ«, é»˜è®¤131\n'
                                '    å¸¸ç”¨å€¼: 131(é»˜è®¤), 135, 143')
    system_group.add_argument('--num-of-log-lines', type=int, default=5000000,
                            help='''TDengineæ—¥å¿—æ–‡ä»¶æœ€å¤§è¡Œæ•°, é»˜è®¤5000000,æœ€å¤§å€¼ä¸èƒ½è¶…è¿‡2000000000\n
ç¤ºä¾‹ç”¨æ³•:%(prog)s --monitor-interval 10 \n--deployment-mode single --debug-flag 135 --num-of-log-lines 1000\n\n''')
    
    # æ•°æ®ç®¡ç†å‚æ•°
    data_mgmt_group = parser.add_argument_group('æ•°æ®ç®¡ç†')
    data_mgmt_group.add_argument('--create-data', action='store_true',
                                help='''åˆ›å»ºæµ‹è¯•æ•°æ®å¹¶è‡ªåŠ¨å¤‡ä»½:  
æ‰§è¡Œæµç¨‹:
  1. å‡†å¤‡TDengineç¯å¢ƒ (å¯åŠ¨æœåŠ¡ã€é…ç½®å•èŠ‚ç‚¹æˆ–è€…é›†ç¾¤)
  2. åˆ›å»ºæŒ‡å®šè§„æ¨¡çš„æµ‹è¯•æ•°æ®  
  3. è‡ªåŠ¨å¤‡ä»½åˆ° --stream-perf-test-dir/data_bak
  4. ä¿ç•™ç¯å¢ƒä¾›åç»­æµ‹è¯•å†å²æ•°æ®æµè®¡ç®—ä½¿ç”¨
å»ºè®®: æµ‹è¯•å¤šç»„å†å²æ•°æ®æµè®¡ç®—å‰å…ˆæ‰§è¡Œæ­¤æ“ä½œ\n
ç¤ºä¾‹ç”¨æ³•:%(prog)s --create-data 
--deployment-mode single --table-count 500 --histroy-rows 10000000\n\n''')
    
    data_mgmt_group.add_argument('--restore-data', action='store_true',
                                help='''ä»å¤‡ä»½æ¢å¤æµ‹è¯•æ•°æ®: 
æ‰§è¡Œæµç¨‹:
  1. æ£€æŸ¥å¤‡ä»½æ•°æ®å®Œæ•´æ€§
  2. åœæ­¢ç°æœ‰TDengineæœåŠ¡
  3. æ¢å¤æ•°æ®æ–‡ä»¶
  4. é‡å¯æœåŠ¡å¹¶éªŒè¯
é€‚ç”¨: å¿«é€Ÿæ¢å¤åˆ°å·²çŸ¥æ•°æ®çŠ¶æ€ï¼Œé¿å…é‡å¤æ•°æ®ç”Ÿæˆï¼ŒèŠ‚çœæ—¶é—´\nå¦‚æœæµ‹è¯•å†å²æ•°æ®è¿›è¡Œæµè®¡ç®—ï¼Œå»ºè®®ç”¨-m 2æ¨¡å¼\n
ç¤ºä¾‹ç”¨æ³•:%(prog)s --restore-data --deployment-mode single''')
    
    args = parser.parse_args()
    
    # æ‰“å°è¿è¡Œå‚æ•°
    print("è¿è¡Œå‚æ•°:")
    print(f"è¿è¡Œæ¨¡å¼: {args.mode}")
    print(f"è¿è¡Œæ—¶é—´: {args.time}åˆ†é’Ÿ")
    print(f"æ€§èƒ½æ–‡ä»¶: {args.file}")
    print(f"å­è¡¨æ•°é‡: {args.table_count}")
    print(f"åˆå§‹æ’å…¥è®°å½•æ•°: {args.histroy_rows}")
    print(f"åç»­æ¯è½®æ’å…¥è®°å½•æ•°: {args.real_time_batch_rows}")
    print(f"æ•°æ®å†™å…¥é—´éš”: {args.real_time_batch_sleep}ç§’")
    print(f"æ•°æ®ä¹±åº: {args.disorder_ratio}")
    print(f"vgroupsæ•°: {args.vgroups}")
    print(f"æµç±»å‹: {args.sql_type}")
    print(f"æµæ•°é‡: {args.stream_num}")
    print(f"é›†ç¾¤ç›®å½•: {args.stream_perf_test_dir}")
    print(f"æ€§èƒ½æ•°æ®é‡‡é›†é—´éš”: {args.monitor_interval}ç§’")
    print(f"éƒ¨ç½²æ¨¡å¼: {args.deployment_mode}")
    print(f"è°ƒè¯•æ ‡å¿—: {args.debug_flag}")
    print(f"æ—¥å¿—è¡Œæ•°: {args.num_of_log_lines}")
    
    print(f"æµå»¶è¿Ÿæ£€æŸ¥: {'å¯ç”¨' if args.check_stream_delay else 'ç¦ç”¨'}")
    if args.check_stream_delay:
        print(f"æœ€å¤§å»¶è¿Ÿé˜ˆå€¼: {args.max_delay_threshold}ms")
        print(f"å»¶è¿Ÿæ£€æŸ¥é—´éš”: {args.delay_check_interval}ç§’")
    
    # å¤„ç†SQLå‚æ•°
    stream_sql = None
    if args.sql_file:
        try:
            with open(args.sql_file, 'r') as f:
                stream_sql = f.read().strip()
                print(f"ä»æ–‡ä»¶åŠ è½½SQL: {args.sql_file}")
        except Exception as e:
            print(f"è¯»å–SQLæ–‡ä»¶å¤±è´¥: {e}")
            return
    elif args.stream_sql:
        stream_sql = args.stream_sql
        print("ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šSQL")
    else:
        print("ä½¿ç”¨é»˜è®¤SQL")
        
    custom_columns = None
    if args.custom_columns:
        custom_columns = [col.strip() for col in args.custom_columns.split(',')]
    
    # åˆ›å»ºStreamStarterå®ä¾‹
    try:
        starter = StreamStarter(
            runtime=args.time,
            perf_file=args.file,
            table_count=args.table_count,
            histroy_rows=args.histroy_rows,
            real_time_batch_rows=args.real_time_batch_rows,
            real_time_batch_sleep=args.real_time_batch_sleep,
            disorder_ratio=args.disorder_ratio,
            vgroups=args.vgroups,
            stream_sql=stream_sql,
            sql_type=args.sql_type,
            stream_num=args.stream_num, 
            stream_perf_test_dir=args.stream_perf_test_dir,
            monitor_interval=args.monitor_interval,
            deployment_mode=args.deployment_mode,
            debug_flag=args.debug_flag, 
            num_of_log_lines=args.num_of_log_lines,            
            agg_or_select=args.agg_or_select,
            tbname_or_trows=args.tbname_or_trows,
            custom_columns=custom_columns,
            check_stream_delay=args.check_stream_delay,
            max_delay_threshold=args.max_delay_threshold,
            delay_check_interval=args.delay_check_interval 
        )
        
        if args.create_data:
            print("\n=== å¼€å§‹åˆ›å»ºæµ‹è¯•æ•°æ® ===")
            print(f"å­è¡¨æ•°é‡: {args.table_count}")
            print(f"æ¯è¡¨è®°å½•æ•°: {args.histroy_rows}")
            print(f"æ•°æ®ä¹±åºç‡: {args.disorder_ratio}")
            print(f"vgroupsæ•°: {args.vgroups}\n")
            
            if starter.create_test_data():
                print("\næµ‹è¯•æ•°æ®åˆ›å»ºå®Œæˆ!")
            return
            
        if args.restore_data:
            print("\n=== å¼€å§‹æ¢å¤æµ‹è¯•æ•°æ® ===")
            if starter.restore_cluster_data():
                print("\næ•°æ®æ¢å¤å®Œæˆ!")
                
                # å¦‚æœæŒ‡å®šäº†SQLç±»å‹ä¸”ä¸æ˜¯é»˜è®¤å€¼ï¼Œæç¤ºå¯ä»¥è¿›è¡Œæµè®¡ç®—æµ‹è¯•
                if args.sql_type != 'select_stream' or args.stream_sql or args.sql_file:
                    print(f"\næç¤º: æ•°æ®å·²æ¢å¤ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æµ‹è¯•æµè®¡ç®—:")
                    print(f"python3 {sys.argv[0]} -m 2 --sql-type {args.sql_type} --time {args.time}")
            return
        
        print("\nå¼€å§‹æ‰§è¡Œ...")
        if args.mode == 1:
            print("æ‰§è¡Œæ¨¡å¼: do_test_stream_with_realtime_data")
            starter.do_test_stream_with_realtime_data()
        elif args.mode == 3:
            print("æ‰§è¡Œæ¨¡å¼: do_query_then_insert")
            starter.do_query_then_insert()
        elif args.mode == 2:
            print("æ‰§è¡Œæ¨¡å¼: do_test_stream_with_restored_data (æ¢å¤æ•°æ®å¹¶æµ‹è¯•æŒ‡å®šæµè®¡ç®—)")
            starter.do_test_stream_with_restored_data()
        else:
            print(f"é”™è¯¯: ä¸æ”¯æŒçš„æ‰§è¡Œæ¨¡å¼ {args.mode}")
            
    except KeyboardInterrupt:
        print("\nç¨‹åºé€€å‡º")
        # æ£€æŸ¥taosdè¿›ç¨‹çŠ¶æ€
        result = subprocess.run('ps -ef | grep "taosd -c" | grep -v grep', 
                                shell=True, capture_output=True, text=True)
        if result.stdout:
            print("\ntaosdè¿›ç¨‹ä»åœ¨è¿è¡Œ:")
            print(result.stdout)
        else:
            print("\nè­¦å‘Š: æœªæ‰¾åˆ°è¿è¡Œä¸­çš„taosdè¿›ç¨‹")
    except Exception as e:
        print(f"\nç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
    finally:
        print("\nå¦‚éœ€æ‰‹åŠ¨åœæ­¢taosdè¿›ç¨‹ï¼Œè¯·æ‰§è¡Œ:")
        print("pkill taosd")

if __name__ == "__main__":
    main()
