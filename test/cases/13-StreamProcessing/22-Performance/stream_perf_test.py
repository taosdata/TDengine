import json
import subprocess
import threading
import psutil
import time
import taos
import shutil
import argparse
import os
import signal
import sys
import datetime
import itertools

"""TDengine New Stream Computing Performance Test
TDengine æ–°æ•°æ®æµè®¡ç®—æ€§èƒ½æµ‹è¯•å·¥å…·

Purpose/ç”¨é€”:
    TDengine æµè®¡ç®—æ€§èƒ½æµ‹è¯•å·¥å…·ï¼Œæ”¯æŒå¤šç§æµ‹è¯•æ¨¡å¼å’Œæµè®¡ç®—ç±»å‹ã€‚
    æä¾›å®Œæ•´çš„æ•°æ®ç”Ÿæˆã€æµè®¡ç®—åˆ›å»ºã€æ€§èƒ½ç›‘æ§ã€å»¶è¿Ÿæ£€æµ‹ç­‰åŠŸèƒ½ã€‚
    é€‚ç”¨äºæµè®¡ç®—æ€§èƒ½åŸºå‡†æµ‹è¯•ã€ç³»ç»Ÿè°ƒä¼˜ã€å‹åŠ›æµ‹è¯•ç­‰åœºæ™¯ã€‚

Catalog/ç›®å½•:
    - Performance Testing / æ€§èƒ½æµ‹è¯•
    - Stream Computing / æµè®¡ç®—  
    - Real-time Analytics / å®æ—¶åˆ†æ
    - Benchmarking / åŸºå‡†æµ‹è¯•

Features/åŠŸèƒ½:
    âœ¨ æµè®¡ç®—æ€§èƒ½æµ‹è¯•
        - æ”¯æŒæ—¶é—´çª—å£ã€æ»‘åŠ¨çª—å£ã€ä¼šè¯çª—å£ã€è®¡æ•°çª—å£ã€äº‹ä»¶çª—å£ã€çŠ¶æ€çª—å£ã€å®šæ—¶è§¦å‘ç­‰7ç§çª—å£ç±»å‹
        - æ”¯æŒè¶…çº§è¡¨å’Œå­è¡¨æ•°æ®æºï¼Œæ”¯æŒæŒ‰å­è¡¨åå’Œtagåˆ†ç»„
        - æ”¯æŒèšåˆæŸ¥è¯¢å’ŒæŠ•å½±æŸ¥è¯¢ä¸¤ç§è®¡ç®—æ¨¡å¼
        - æ”¯æŒå•æµå’Œå¤šæµå¹¶å‘æµ‹è¯•
        
    ğŸ“Š å®æ—¶æ€§èƒ½ç›‘æ§
        - ç³»ç»Ÿèµ„æºç›‘æ§ï¼šCPUã€å†…å­˜ã€ç£ç›˜I/Oå®æ—¶ç›‘æ§
        - æµè®¡ç®—å»¶è¿Ÿç›‘æ§ï¼šæºè¡¨ä¸ç›®æ ‡è¡¨æ•°æ®å»¶è¿Ÿæ£€æµ‹
        - åŠ¨æ€é˜ˆå€¼å‘Šè­¦ï¼šåŸºäºæ£€æŸ¥é—´éš”çš„æ™ºèƒ½å»¶è¿Ÿåˆ†çº§
        - è¯¦ç»†æ€§èƒ½æŠ¥å‘Šï¼šå›¾è¡¨åŒ–å±•ç¤ºå’Œé—®é¢˜åˆ†æå»ºè®®
        
    ğŸ—„ï¸ æ•°æ®ç®¡ç†
        - è‡ªåŠ¨æ•°æ®ç”Ÿæˆï¼šæ”¯æŒå¯é…ç½®çš„è¡¨æ•°é‡ã€è®°å½•æ•°ã€ä¹±åºç‡
        - æ•°æ®å¤‡ä»½æ¢å¤ï¼šæ”¯æŒå®Œæ•´çš„æ•°æ®å¤‡ä»½å’Œå¿«é€Ÿæ¢å¤
        - å†å²æ•°æ®æµ‹è¯•ï¼šåŸºäºå·²æœ‰æ•°æ®è¿›è¡Œæµè®¡ç®—æ€§èƒ½æµ‹è¯•
        - å¢é‡æ•°æ®å†™å…¥ï¼šæ¨¡æ‹Ÿå®æ—¶æ•°æ®æµåœºæ™¯
        
    ğŸ—ï¸ éƒ¨ç½²æ¶æ„
        - å•èŠ‚ç‚¹æ¨¡å¼ï¼šé€‚ç”¨äºå¼€å‘æµ‹è¯•å’Œå°è§„æ¨¡éªŒè¯
        - é›†ç¾¤æ¨¡å¼ï¼šæ”¯æŒ3èŠ‚ç‚¹é›†ç¾¤çš„é«˜å¯ç”¨æµ‹è¯•
        - è‡ªåŠ¨ç¯å¢ƒå‡†å¤‡ï¼šè‡ªåŠ¨é…ç½®TDengineé›†ç¾¤ç¯å¢ƒ
        - çµæ´»å‚æ•°è°ƒä¼˜ï¼šæ”¯æŒvgroupsã€è°ƒè¯•çº§åˆ«ç­‰å‚æ•°è°ƒæ•´
        
Test Scenarios/æµ‹è¯•åœºæ™¯:
    ğŸš€ åŸºç¡€æµè®¡ç®—æµ‹è¯•
        - åˆ›å»ºæµ‹è¯•æ•°æ®å¹¶æ‰§è¡Œå®æ—¶æµè®¡ç®—
        - æŒç»­æ•°æ®å†™å…¥ï¼Œè§‚å¯Ÿæµè®¡ç®—å»¶è¿Ÿå˜åŒ–
        - é€‚ç”¨äºéªŒè¯æµè®¡ç®—åŸºæœ¬åŠŸèƒ½å’Œæ€§èƒ½
        
    ğŸ“ˆ å‹åŠ›æµ‹è¯•
        - å¤§æ•°æ®é‡ä¸‹çš„æµè®¡ç®—æ€§èƒ½æµ‹è¯•
        - å¤šæµå¹¶å‘å¤„ç†èƒ½åŠ›æµ‹è¯•  
        - ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µåˆ†æ
        
    ğŸ” å»¶è¿Ÿæµ‹è¯•
        - æµè®¡ç®—å®æ—¶æ€§éªŒè¯
        - å»¶è¿Ÿåˆ†å¸ƒç»Ÿè®¡å’Œåˆ†æ
        - å»¶è¿Ÿé˜ˆå€¼å‘Šè­¦æµ‹è¯•
        
    ğŸ‹ï¸ å†å²æ•°æ®æµ‹è¯•
        - åŸºäºå·²æœ‰å¤§æ•°æ®é›†çš„æµè®¡ç®—æµ‹è¯•
        - é¿å…é‡å¤æ•°æ®ç”Ÿæˆï¼Œå¿«é€ŸéªŒè¯ä¸åŒæµSQL
        - é€‚ç”¨äºç®—æ³•éªŒè¯å’Œæ€§èƒ½å¯¹æ¯”

Window Types/çª—å£ç±»å‹:
    ğŸ“Š æ—¶é—´çª—å£ (INTERVAL SLIDING WINDOW)
        - å›ºå®šæ—¶é—´é—´éš”çš„è®¡ç®—
        - é€‚ç”¨äºè¿ç»­æ•°æ®çš„å¹³æ»‘ç»Ÿè®¡
        
    ğŸ“Š æ»‘åŠ¨çª—å£ (SLIDING WINDOW)
        - å›ºå®šæ—¶é—´é—´éš”çš„æ»‘åŠ¨è®¡ç®—
        - é€‚ç”¨äºè¿ç»­æ•°æ®çš„å¹³æ»‘ç»Ÿè®¡
        
    ğŸ”— ä¼šè¯çª—å£ (SESSION WINDOW)  
        - åŸºäºæ•°æ®é—´éš”çš„åŠ¨æ€çª—å£
        - é€‚ç”¨äºç”¨æˆ·ä¼šè¯åˆ†æ
        
    ğŸ“ è®¡æ•°çª—å£ (COUNT WINDOW)
        - åŸºäºè®°å½•æ•°é‡çš„å›ºå®šçª—å£
        - é€‚ç”¨äºæ‰¹é‡æ•°æ®å¤„ç†
        
    âš¡ äº‹ä»¶çª—å£ (EVENT WINDOW)
        - åŸºäºäº‹ä»¶æ¡ä»¶çš„åŠ¨æ€çª—å£
        - é€‚ç”¨äºå¼‚å¸¸æ£€æµ‹å’Œæ¨¡å¼è¯†åˆ«
        
    ğŸ“ çŠ¶æ€çª—å£ (STATE WINDOW)
        - åŸºäºçŠ¶æ€å˜åŒ–çš„çª—å£åˆ’åˆ†
        - é€‚ç”¨äºçŠ¶æ€æœºåˆ†æ
        
    â° å®šæ—¶è§¦å‘ (PERIOD TRIGGER)
        - å®šæ—¶æ‰§è¡Œçš„è®¡ç®—è§¦å‘
        - é€‚ç”¨äºå®šæœŸæŠ¥è¡¨ç”Ÿæˆ

Query Types/æŸ¥è¯¢ç±»å‹:
    ğŸ“Š èšåˆæŸ¥è¯¢ (AGG): avg(), max(), min() ç­‰ç»Ÿè®¡å‡½æ•°
    ğŸ“‹ æŠ•å½±æŸ¥è¯¢ (SELECT): å­—æ®µé€‰æ‹©å’ŒæŠ•å½±æ“ä½œ

Data Sources/æ•°æ®æº:
    ğŸ¢ è¶…çº§è¡¨ (STB): stream_from.stb - é€‚ç”¨äºå¤§è§„æ¨¡æ•°æ®æŸ¥è¯¢
    ğŸ“„ å­è¡¨ (TB): stream_from.ctb0_X - é€‚ç”¨äºå•è¡¨æ€§èƒ½æµ‹è¯•

Partitioning/åˆ†ç»„æ–¹å¼:
    ğŸ“‚ æ— åˆ†ç»„ (NONE): å…¨è¡¨ç»Ÿè®¡
    ğŸ·ï¸ æŒ‰å­è¡¨å (BY_TBNAME): partition by tbname
    ğŸ”– æŒ‰æ ‡ç­¾ (BY_TAG): partition by tag

Performance Monitoring/æ€§èƒ½ç›‘æ§:
    ğŸ’» ç³»ç»Ÿèµ„æº: CPUä½¿ç”¨ç‡ã€å†…å­˜å ç”¨ã€ç£ç›˜I/O
    â±ï¸ å»¶è¿Ÿç›‘æ§: æºè¡¨ä¸ç›®æ ‡è¡¨çš„æ—¶é—´å·®åˆ†æ
    ğŸ“ˆ å®æ—¶ç»Ÿè®¡: æµè®¡ç®—å¤„ç†é€Ÿåº¦å’Œæ•ˆç‡
    ğŸš¨ æ™ºèƒ½å‘Šè­¦: å»¶è¿Ÿé˜ˆå€¼å‘Šè­¦å’Œæ€§èƒ½å»ºè®®

Requirements/ç³»ç»Ÿè¦æ±‚:
    TDengine: v3.3.7.0+
    Python: 3.7+
    ä¾èµ–åŒ…: taos, psutil, json

Authors/ä½œè€…:
    Guo Xiangyang / éƒ­å‘é˜³

Labels/æ ‡ç­¾: 
    performance, stream, testing

History/å†å²:
    - 2025-08-15 Initial commit
                 é¦–æ¬¡æäº¤

Usage Examples/ä½¿ç”¨ç¤ºä¾‹:

    # å¿«é€Ÿå¼€å§‹ - åŸºç¡€æµè®¡ç®—æµ‹è¯•
    python3 stream_perf_test.py --stream-perf-test-dir /home/stream_perf_test_dir -m 1 --table-count 1000 --time 30
    
    # åˆ›å»ºå¹¶å¤‡ä»½æµ‹è¯•æ•°æ®  
    python3 stream_perf_test.py --stream-perf-test-dir /home/stream_perf_test_dir --create-data --table-count 5000 --histroy-rows 1000
    
    # æ¢å¤æ•°æ®å¹¶æµ‹è¯•ç‰¹å®šæµè®¡ç®—
    python3 stream_perf_test.py --stream-perf-test-dir /home/stream_perf_test_dir -m 3 --sql-type intervalsliding_stb_partition_by_tbname --time 60
    
    # å¤šæµå¹¶å‘æµ‹è¯•
    python3 stream_perf_test.py --stream-perf-test-dir /home/stream_perf_test_dir -m 1 --sql-type intervalsliding_stb --stream-num 100 --time 30
    
    # å»¶è¿Ÿç›‘æ§æµ‹è¯•
    python3 stream_perf_test.py --stream-perf-test-dir /home/stream_perf_test_dir -m 1 --check-stream-delay --delay-check-interval 5 --time 60
    
    # é›†ç¾¤æ¨¡å¼æµ‹è¯•
    python3 stream_perf_test.py --stream-perf-test-dir /home/stream_perf_test_dir -m 1 --deployment-mode cluster --vgroups 20 --time 30
    
    # å‹åŠ›æ§åˆ¶æµ‹è¯•
    python3 stream_perf_test.py --stream-perf-test-dir /home/stream_perf_test_dir -m 1 --real-time-batch-sleep 10 --real-time-batch-rows 500


"""

# ========== å…¨å±€é¢œè‰²å®šä¹‰ ==========
class Colors:
    """ç»ˆç«¯é¢œè‰²å¸¸é‡å®šä¹‰"""
    # åŸºç¡€é¢œè‰²
    RED = '\033[91m'        # çº¢è‰² - é”™è¯¯ã€ä¸¥é‡è­¦å‘Š
    GREEN = '\033[92m'      # ç»¿è‰² - æˆåŠŸã€æ­£å¸¸çŠ¶æ€
    YELLOW = '\033[93m'     # é»„è‰² - è­¦å‘Šã€æ³¨æ„
    BLUE = '\033[94m'       # è“è‰² - ä¿¡æ¯ã€è¯¦æƒ…
    PURPLE = '\033[95m'     # ç´«è‰² - æ ‡é¢˜ã€é‡è¦ä¿¡æ¯
    CYAN = '\033[96m'       # é’è‰² - æ—¶é—´æˆ³ã€æ•°æ®
    WHITE = '\033[97m'      # ç™½è‰² - æ™®é€šæ–‡æœ¬
    
    # ç‰¹æ®Šæ ¼å¼
    BOLD = '\033[1m'        # ç²—ä½“
    UNDERLINE = '\033[4m'   # ä¸‹åˆ’çº¿
    BLINK = '\033[5m'       # é—ªçƒ
    REVERSE = '\033[7m'     # åè‰²
    
    # èƒŒæ™¯è‰²
    BG_RED = '\033[41m'     # çº¢è‰²èƒŒæ™¯
    BG_GREEN = '\033[42m'   # ç»¿è‰²èƒŒæ™¯
    BG_YELLOW = '\033[43m'  # é»„è‰²èƒŒæ™¯
    BG_BLUE = '\033[44m'    # è“è‰²èƒŒæ™¯
    
    # ç»“æŸç¬¦
    END = '\033[0m'         # é‡ç½®æ‰€æœ‰æ ¼å¼
    
    @staticmethod
    def supports_color():
        """æ£€æµ‹ç»ˆç«¯æ˜¯å¦æ”¯æŒé¢œè‰²è¾“å‡º"""
        return (
            hasattr(os.sys.stdout, 'isatty') and os.sys.stdout.isatty() and
            os.environ.get('TERM') != 'dumb' and
            os.environ.get('NO_COLOR') is None
        )
    
    @staticmethod
    def get_colors():
        """è·å–é¢œè‰²å¯¹è±¡ï¼Œå¦‚æœä¸æ”¯æŒé¢œè‰²åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²"""
        if Colors.supports_color():
            return Colors
        else:
            # åˆ›å»ºä¸€ä¸ªæ‰€æœ‰é¢œè‰²éƒ½ä¸ºç©ºå­—ç¬¦ä¸²çš„ç±»
            class NoColors:
                RED = GREEN = YELLOW = BLUE = PURPLE = CYAN = WHITE = ''
                BOLD = UNDERLINE = BLINK = REVERSE = ''
                BG_RED = BG_GREEN = BG_YELLOW = BG_BLUE = ''
                END = ''
            return NoColors


# ========== é¢œè‰²æ‰“å°è¾…åŠ©å‡½æ•° ==========
def print_success(message):
    """æ‰“å°æˆåŠŸæ¶ˆæ¯"""
    c = Colors.get_colors()
    print(f"{c.GREEN}âœ“ {message}{c.END}")

def print_warning(message):
    """æ‰“å°è­¦å‘Šæ¶ˆæ¯"""
    c = Colors.get_colors()
    print(f"{c.YELLOW}âš  {message}{c.END}")

def print_error(message):
    """æ‰“å°é”™è¯¯æ¶ˆæ¯"""
    c = Colors.get_colors()
    print(f"{c.RED}âœ— {message}{c.END}")

def print_info(message):
    """æ‰“å°ä¿¡æ¯æ¶ˆæ¯"""
    c = Colors.get_colors()
    print(f"{c.BLUE}â„¹ {message}{c.END}")

def print_title(message):
    """æ‰“å°æ ‡é¢˜æ¶ˆæ¯"""
    c = Colors.get_colors()
    print(f"{c.BOLD}{c.PURPLE}{message}{c.END}")

def print_subtitle(message):
    """æ‰“å°å‰¯æ ‡é¢˜æ¶ˆæ¯"""
    c = Colors.get_colors()
    print(f"{c.CYAN}{message}{c.END}")

def print_timestamp(message):
    """æ‰“å°å¸¦æ—¶é—´æˆ³çš„æ¶ˆæ¯"""
    c = Colors.get_colors()
    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
    print(f"{c.CYAN}[{timestamp}]{c.END} {message}")

def print_separator(char='-', length=80, color=None):
    """æ‰“å°åˆ†éš”çº¿"""
    c = Colors.get_colors()
    if color:
        print(f"{getattr(c, color.upper(), '')}{char * length}{c.END}")
    else:
        print(char * length)
        
        
class MonitorSystemLoad:

    def __init__(self, name_pattern, count, perf_file='/tmp/perf.log', use_signal=True, interval=1, deployment_mode='cluster', warm_up_time=0) -> None:
        """åˆå§‹åŒ–ç³»ç»Ÿè´Ÿè½½ç›‘æ§
        
        Args:
            name_pattern: è¿›ç¨‹åæ¨¡å¼,ä¾‹å¦‚ 'taosd.*dnode1/conf'
            count: ç›‘æ§æ¬¡æ•°
            perf_file: æ€§èƒ½æ•°æ®è¾“å‡ºæ–‡ä»¶
            interval: æ€§èƒ½é‡‡é›†é—´éš”(ç§’),é»˜è®¤1ç§’
            deployment_mode: éƒ¨ç½²æ¨¡å¼ 'single' æˆ– 'cluster'
            warm_up_time: é¢„çƒ­æ—¶é—´(ç§’),åœ¨æ­¤æœŸé—´æ”¶é›†æ•°æ®ä½†ä¸æ‰“å°,é»˜è®¤0ç§’
        """
        self.name_pattern = name_pattern  # ä¿å­˜è¿›ç¨‹åæ¨¡å¼
        self.count = count
        self.perf_file = perf_file
        self.interval = interval
        self.deployment_mode = deployment_mode
        self.warm_up_time = warm_up_time
        
        # æ·»åŠ å”¯ä¸€æ ‡è¯†ç¬¦
        import uuid
        self.instance_id = str(uuid.uuid4())[:8]
        self.thread_name = f"MonitorSystemLoad-{self.instance_id}"
        
        print(f"è°ƒè¯•: MonitorSystemLoad åˆå§‹åŒ–ï¼Œå®ä¾‹ID = {self.instance_id}ï¼Œperf_file = {perf_file}")
        print(f"è°ƒè¯•: é¢„çƒ­ç­‰å¾…æ—¶é—´ = {warm_up_time}ç§’")        
        
        self.stop_monitoring = False
        self._should_stop = False
        self._is_running = False
        self._force_stop = False
        self._stop_event = threading.Event()
        
        # æ ¹æ®éƒ¨ç½²æ¨¡å¼ç¡®å®šéœ€è¦ç›‘æ§çš„èŠ‚ç‚¹
        if deployment_mode == 'single':
            self.monitor_nodes = ['dnode1']
            print(f"å•èŠ‚ç‚¹æ¨¡å¼: ä»…ç›‘æ§ dnode1")
        else:
            self.monitor_nodes = ['dnode1', 'dnode2', 'dnode3']
            print(f"é›†ç¾¤æ¨¡å¼: ç›‘æ§ dnode1, dnode2, dnode3")
        
        # ä¸ºæ¯ä¸ªdnodeåˆ›å»ºå¯¹åº”çš„æ€§èƒ½æ–‡ä»¶å¥æŸ„
        self.perf_files = {}
        for dnode in self.monitor_nodes:
            # åŸºäºä¼ å…¥çš„perf_fileè·¯å¾„ç”Ÿæˆæ¯ä¸ªèŠ‚ç‚¹çš„æ–‡ä»¶è·¯å¾„
            base_dir = os.path.dirname(perf_file)
            base_name = os.path.splitext(os.path.basename(perf_file))[0]
            file_path = os.path.join(base_dir, f"{base_name}-{dnode}.log")
            
            try:
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                os.makedirs(base_dir, exist_ok=True)
                self.perf_files[dnode] = open(file_path, 'w+')
                print(f"åˆ›å»ºæ€§èƒ½æ—¥å¿—æ–‡ä»¶: {file_path}")
            except Exception as e:
                print(f"åˆ›å»ºæ—¥å¿—æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
                
        # åˆ›å»ºæ±‡æ€»æ—¥å¿—æ–‡ä»¶
        try:
            base_dir = os.path.dirname(perf_file)
            base_name = os.path.splitext(os.path.basename(perf_file))[0]
            all_file = os.path.join(base_dir, f"{base_name}-all.log")
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(base_dir, exist_ok=True)
            self.perf_files['all'] = open(all_file, 'w+')
            print(f"åˆ›å»ºæ±‡æ€»æ—¥å¿—æ–‡ä»¶: {all_file}")
        except Exception as e:
            print(f"åˆ›å»ºæ±‡æ€»æ—¥å¿—æ–‡ä»¶å¤±è´¥: {str(e)}")
            
        # è·å–è¿›ç¨‹ID
        self.pids = self.get_pids_by_pattern()
        self.processes = {}
        
        # ä¿®å¤CPUç›‘æ§é—®é¢˜ï¼šæ­£ç¡®åˆå§‹åŒ–è¿›ç¨‹å¯¹è±¡å¹¶è¿›è¡Œç¬¬ä¸€æ¬¡CPUé‡‡æ ·
        for dnode, pid in self.pids.items():
            if pid:
                try:
                    process = psutil.Process(pid)
                    process.cpu_percent()
                    self.processes[dnode] = process
                    print(f"åˆå§‹åŒ–è¿›ç¨‹å¯¹è±¡: {dnode}, PID: {pid}")
                except Exception as e:
                    print(f"åˆå§‹åŒ–è¿›ç¨‹å¯¹è±¡å¤±è´¥ {dnode} (PID: {pid}): {str(e)}")
                    self.processes[dnode] = None
            else:
                self.processes[dnode] = None
                
        self.stop_monitoring = False
        self._should_stop = False
        if use_signal and threading.current_thread() is threading.main_thread():
            signal.signal(signal.SIGINT, self.signal_handler)
            signal.signal(signal.SIGTERM, self.signal_handler)

    def __del__(self):
        """ç¡®ä¿æ‰€æœ‰æ–‡ä»¶éƒ½è¢«æ­£ç¡®å…³é—­"""
        try:
            if hasattr(self, '_is_running') and self._is_running:
                print(f"è°ƒè¯•: MonitorSystemLoadå®ä¾‹ {getattr(self, 'instance_id', 'unknown')} æ­£åœ¨ææ„ï¼Œå¼ºåˆ¶åœæ­¢")
                self.stop()
        except:
            pass
        
        # å…³é—­æ–‡ä»¶å¥æŸ„
        if hasattr(self, 'perf_files'):
            for f in self.perf_files.values():
                try:
                    if not f.closed:
                        f.close()
                except:
                    pass
            
    def stop(self):
        """æä¾›å¤–éƒ¨åœæ­¢ç›‘æ§çš„æ–¹æ³•"""
        print(f"è°ƒè¯•: åœæ­¢ç›‘æ§å®ä¾‹ {getattr(self, 'instance_id', 'unknown')}")
        self.stop_monitoring = True
        self._should_stop = True
        self._force_stop = True 
        self._is_running = False
        self._stop_event.set() 
        
        print("\nåœæ­¢æ€§èƒ½ç›‘æ§...")
        # ç­‰å¾…æ‰€æœ‰æ–‡ä»¶å†™å…¥å®Œæˆ
        if hasattr(self, 'perf_files'):
            for f in self.perf_files.values():
                try:
                    if not f.closed:
                        f.flush()
                except:
                    pass
            

    def _should_continue_monitoring(self):
        """ç»Ÿä¸€çš„åœæ­¢æ£€æŸ¥æ–¹æ³•"""
        return (not self.stop_monitoring and 
                not self._should_stop and 
                not self._force_stop and 
                not self._stop_event.is_set())
        
    def signal_handler(self, signum, frame):
        """å¤„ç†ä¸­æ–­ä¿¡å·"""
        print("\næ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢ç›‘æ§...")
        self.stop_monitoring = True
        time.sleep(1)
        # å…³é—­æ‰€æœ‰æ–‡ä»¶
        for f in self.perf_files.values():
            try:
                f.close()
            except:
                pass
        # é€€å‡ºç›‘æ§ä½†ä¸å½±å“taosdè¿›ç¨‹
        print("\nç›‘æ§å·²åœæ­¢ï¼Œtaosdè¿›ç¨‹ç»§ç»­è¿è¡Œ")
        os._exit(0)
            
    def get_pid_by_name(self, name):
        for proc in psutil.process_iter(['pid', 'name']):
            if proc.info['name'] == name:
                return proc.info['pid']
        return None
    
    def write_metrics(self, dnode, status, timestamp=None, print_to_console=True):
        """å†™å…¥æ€§èƒ½æŒ‡æ ‡
        
        Args:
            dnode: èŠ‚ç‚¹åç§°
            status: æ€§èƒ½æ•°æ®
            timestamp: æ—¶é—´æˆ³(å¯é€‰)
            print_to_console: æ˜¯å¦æ‰“å°åˆ°æ§åˆ¶å°
        """
        # å†™å…¥å•ä¸ªèŠ‚ç‚¹çš„æ—¥å¿—æ–‡ä»¶
        self.perf_files[dnode].write(status + '\n')
        
        # åŒæ—¶å†™å…¥æ±‡æ€»æ—¥å¿—æ–‡ä»¶
        self.perf_files['all'].write(status + '\n')
        
        # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦è¾“å‡ºåˆ°æ§åˆ¶å°
        if print_to_console:
            print(status)

    def get_pids_by_pattern(self):
        """æ ¹æ®è¿›ç¨‹åæ¨¡å¼è·å–æ‰€æœ‰åŒ¹é…çš„è¿›ç¨‹ID"""
        pids = {}
        # ä½¿ç”¨ ps å‘½ä»¤è·å–è¯¦ç»†è¿›ç¨‹ä¿¡æ¯
        result = subprocess.run(
            'ps -ef | grep taosd | grep -v grep', 
            shell=True, 
            capture_output=True, 
            text=True
        )
        
        for line in result.stdout.splitlines():
            if self.name_pattern in line:
                parts = line.split()
                pid = int(parts[1])
                # ä»é…ç½®æ–‡ä»¶è·¯å¾„ä¸­æå–dnodeä¿¡æ¯
                cfg_path = next((p for p in parts if '/conf/taos.cfg' in p), None)
                if cfg_path:
                    # ä»è·¯å¾„ä¸­æå–dnodeåç§°
                    dnode = next((part for part in cfg_path.split('/') if part.startswith('dnode')), None)
                    if dnode and dnode in self.monitor_nodes:
                        pids[dnode] = pid
                        print(f"æ‰¾åˆ° {dnode} è¿›ç¨‹, PID: {pid}, é…ç½®æ–‡ä»¶: {cfg_path}")
        
        if not pids:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ°åŒ¹é…æ¨¡å¼ '{self.name_pattern}' çš„è¿›ç¨‹")
        else:
            print(f"å…±æ‰¾åˆ° {len(pids)} ä¸ªtaosdè¿›ç¨‹")
        
        # æ— è®ºæ˜¯å¦æ‰¾åˆ°è¿›ç¨‹,éƒ½åˆå§‹åŒ–æ‰€æœ‰dnodeçš„æ–‡ä»¶å¥æŸ„
        for dnode in self.monitor_nodes:
            if dnode not in self.perf_files:
                file_path = f"{os.path.splitext(self.perf_file)[0]}-{dnode}.log"
                try:
                    self.perf_files[dnode] = open(file_path, 'w+')
                    print(f"åˆ›å»ºæ€§èƒ½æ—¥å¿—æ–‡ä»¶: {file_path}")
                except Exception as e:
                    print(f"åˆ›å»ºæ—¥å¿—æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
        
        return pids
    
    def write_zero_metrics(self, dnode, timestamp, is_warm_up=False, instance_id=None):
        """å†™å…¥é›¶å€¼æŒ‡æ ‡
        
        Args:
            dnode: èŠ‚ç‚¹åç§°
            timestamp: æ—¶é—´æˆ³
            is_warm_up: æ˜¯å¦ä¸ºé¢„çƒ­æœŸ
        """
        status = (
            f"{timestamp} [{dnode}] "
            f"CPU: 0.0%, "
            f"Memory: 0.00MB (0.00%), "
            f"Read: 0.00MB (0), "
            f"Write: 0.00MB (0)"
        )
        
        if is_warm_up:
            status += " [é¢„çƒ­æœŸæ•°æ®]"
        if instance_id:
            status += f" (å®ä¾‹ID: {instance_id})"
        
        try:
            if hasattr(self, 'perf_files') and dnode in self.perf_files:
                if not self.perf_files[dnode].closed:
                    self.perf_files[dnode].write(status + '\n')
        except:
            pass
        
        # é¢„çƒ­æœŸä¸æ‰“å°åˆ°æ§åˆ¶å°
        if not is_warm_up:
            print(status)
        
    def get_proc_status_old(self):
        """ç›‘æ§æ‰€æœ‰åŒ¹é…è¿›ç¨‹çš„çŠ¶æ€"""
        try:
            processes = {
                dnode: psutil.Process(pid) if pid else None
                for dnode, pid in self.pids.items()
            }
            
            while self.count > 0 and not self.stop_monitoring:
                try:
                    sys_load = psutil.getloadavg()
                    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
                    
                    # è®°å½•ç³»ç»Ÿæ•´ä½“è´Ÿè½½
                    load_info = f"\n{timestamp} System Load: {sys_load[0]:.2f}\n"
                    for dnode in self.perf_files.keys():
                        self.perf_files[dnode].write(load_info)
                    print(load_info)
                    
                    # è®°å½•æ¯ä¸ªèŠ‚ç‚¹çš„çŠ¶æ€
                    for dnode in ['dnode1', 'dnode2', 'dnode3']:
                        process = processes.get(dnode)
                        try:
                            if process and process.is_running():
                                cpu_percent = process.cpu_percent(interval=self.interval)
                                memory_info = process.memory_info()
                                memory_percent = process.memory_percent()
                                io_counters = process.io_counters()

                                status = (
                                    f"{timestamp} [{dnode}] "
                                    f"CPU: {cpu_percent:.1f}%, "
                                    f"Memory: {memory_info.rss/1048576.0:.2f}MB ({memory_percent:.2f}%), "
                                    f"Read: {io_counters.read_bytes/1048576.0:.2f}MB ({io_counters.read_count}), "
                                    f"Write: {io_counters.write_bytes/1048576.0:.2f}MB ({io_counters.write_count})"
                                )
                                self.write_metrics(dnode, status)
                            else:
                                # è¿›ç¨‹ä¸å­˜åœ¨æ—¶å†™å…¥é›¶å€¼
                                self.write_zero_metrics(dnode, timestamp)
                                
                        except (psutil.NoSuchProcess, psutil.AccessDenied):
                            # è¿›ç¨‹å·²ç»ˆæ­¢æˆ–æ— æ³•è®¿é—®æ—¶å†™å…¥é›¶å€¼
                            self.write_zero_metrics(dnode, timestamp)
                            processes[dnode] = None
                        except Exception as e:
                            print(f"ç›‘æ§ {dnode} å‡ºé”™: {str(e)}")
                            self.write_zero_metrics(dnode, timestamp)
                    
                    # æ·»åŠ åˆ†éš”çº¿
                    separator = "-" * 80 + "\n"
                    for f in self.perf_files.values():
                        f.write(separator)
                        f.flush()
                    print(separator.strip())
                        
                    time.sleep(self.interval)
                    self.count -= 1
                    
                    if self.stop_monitoring:
                        print("æ­£åœ¨å®Œæˆæœ€åçš„ç›‘æ§è®°å½•...")
                        break
                    
                except Exception as e:
                    print(f"ç›‘æ§å‡ºé”™: {str(e)}")
                    if not self.stop_monitoring:  # åªæœ‰åœ¨éä¸»åŠ¨åœæ­¢æ—¶æ‰è·³å‡º
                            break
                
            print("\nç›‘æ§å·²åœæ­¢")
        
        finally:
            # å…³é—­æ‰€æœ‰æ–‡ä»¶
            for f in self.perf_files.values():
                try:
                    f.close()
                except:
                    pass

    def get_proc_status(self):
        """ç›‘æ§æ‰€æœ‰åŒ¹é…è¿›ç¨‹çš„çŠ¶æ€"""
        # æ ‡è®°å®ä¾‹æ­£åœ¨è¿è¡Œ
        self._is_running = True
        instance_id = getattr(self, 'instance_id', 'unknown')
        
        print(f"è°ƒè¯•: ç›‘æ§çº¿ç¨‹å¼€å§‹è¿è¡Œï¼Œå®ä¾‹ID = {instance_id}")
        
        try:
            # è®¡ç®—é¢„çƒ­æœŸé—´çš„æ•°æ®ç‚¹æ•°é‡
            warm_up_cycles = int(self.warm_up_time / self.interval) if self.warm_up_time > 0 else 0
            
            if warm_up_cycles > 0:
                print(f"\n=== å¼€å§‹æ€§èƒ½ç›‘æ§é¢„çƒ­æœŸ ===")
                print(f"é¢„çƒ­æ—¶é—´: {self.warm_up_time}ç§’ ({warm_up_cycles}ä¸ªå‘¨æœŸ)")
                print(f"é¢„çƒ­æœŸé—´æ”¶é›†æ•°æ®ä½†ä¸æ‰“å°ï¼Œç­‰å¾…ç³»ç»Ÿç¨³å®š...")
                
                # å†™å…¥é¢„çƒ­è¯´æ˜åˆ°æ—¥å¿—æ–‡ä»¶
                for f in self.perf_files.values():
                    f.write(f"=== æ€§èƒ½ç›‘æ§é¢„çƒ­æœŸå¼€å§‹ ===\n")
                    f.write(f"é¢„çƒ­æ—¶é—´: {self.warm_up_time}ç§’\n")
                    f.write(f"é¢„çƒ­æœŸé—´çš„æ•°æ®ä»…ç”¨äºç³»ç»Ÿç¨³å®šï¼Œä¸ä½œä¸ºæ€§èƒ½è¯„ä¼°ä¾æ®\n")
                    f.write(f"{'='*60}\n")
            
            cycle_count = 0
            
            while self.count > 0 and self._should_continue_monitoring():
                start_time = time.time()
                cycle_count += 1
                
                # åœ¨æ¯ä¸ªå¾ªç¯å¼€å§‹æ—¶æ£€æŸ¥åœæ­¢æ ‡å¿—
                if not self._should_continue_monitoring():
                    print(f"è°ƒè¯•: ç›‘æ§å®ä¾‹ {instance_id} æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç¬¬{cycle_count}æ¬¡å¾ªç¯")
                    break
                
                sys_load = psutil.getloadavg()
                timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
                
                # åˆ¤æ–­æ˜¯å¦åœ¨é¢„çƒ­æœŸ
                is_warm_up = cycle_count <= warm_up_cycles
                
                # è®°å½•ç³»ç»Ÿè´Ÿè½½
                load_info = f"\n{timestamp} System Load: {sys_load[0]:.2f}\n"
                if is_warm_up:
                    load_info += f" [é¢„çƒ­æœŸ {cycle_count}/{warm_up_cycles}] (å®ä¾‹ID: {instance_id})"
                else:
                    load_info += f" [æ­£å¼ç›‘æ§] (å®ä¾‹ID: {instance_id})"
                load_info += "\n"
                
                # å†™å…¥æ–‡ä»¶ï¼ˆåŒ…æ‹¬é¢„çƒ­æœŸæ•°æ®ï¼‰
                for f in self.perf_files.values():
                    try:
                        if not f.closed:
                            f.write(load_info)
                    except:
                        pass
                                
                # æ§åˆ¶å°è¾“å‡ºï¼ˆé¢„çƒ­æœŸé™é»˜ï¼‰
                if not is_warm_up:
                    print(load_info.strip()) 
                elif cycle_count == 1:
                    print(f"é¢„çƒ­å¼€å§‹: {timestamp}(å®ä¾‹ID: {instance_id})")
                elif cycle_count % 10 == 0:  # æ¯10ä¸ªå‘¨æœŸæ˜¾ç¤ºä¸€æ¬¡é¢„çƒ­è¿›åº¦
                    print(f"é¢„çƒ­è¿›åº¦: {cycle_count}/{warm_up_cycles} ({(cycle_count/warm_up_cycles)*100:.1f}%)(å®ä¾‹ID: {instance_id})")
                
                current_pids = self.get_pids_by_pattern()
                
                # æ›´æ–°è¿›ç¨‹å¯¹è±¡ï¼šæ£€æŸ¥PIDæ˜¯å¦å˜åŒ–ï¼Œå¦‚æœå˜åŒ–åˆ™é‡æ–°åˆå§‹åŒ–
                for dnode in self.monitor_nodes:
                    current_pid = current_pids.get(dnode)
                    existing_process = self.processes.get(dnode)
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°è¿›ç¨‹å¯¹è±¡
                    need_update = False
                    if current_pid is None:
                        # è¿›ç¨‹ä¸å­˜åœ¨äº†
                        if existing_process is not None:
                            need_update = True
                            self.processes[dnode] = None
                    elif existing_process is None:
                        # æ–°å‘ç°çš„è¿›ç¨‹
                        need_update = True
                    elif existing_process.pid != current_pid:
                        # PIDå˜åŒ–äº†ï¼Œè¿›ç¨‹é‡å¯äº†
                        need_update = True
                    
                    if need_update and current_pid:
                        try:
                            new_process = psutil.Process(current_pid)
                            new_process.cpu_percent()
                            self.processes[dnode] = new_process
                            print(f"è°ƒè¯•: æ›´æ–°è¿›ç¨‹å¯¹è±¡ {dnode}, æ–°PID: {current_pid}")
                        except Exception as e:
                            print(f"è°ƒè¯•: åˆå§‹åŒ–æ–°è¿›ç¨‹å¯¹è±¡å¤±è´¥ {dnode} (PID: {current_pid}): {str(e)}")
                            self.processes[dnode] = None
                            
                # æ”¶é›†è¿›ç¨‹æŒ‡æ ‡
                for dnode in self.monitor_nodes:
                    try:
                        process = self.processes.get(dnode)
                        
                        if process and process.is_running():
                            try:
                                cpu_percent = process.cpu_percent(interval=None)
                                memory_info = process.memory_info()
                                memory_percent = process.memory_percent()
                                io_counters = process.io_counters()

                                status = (
                                    f"{timestamp} [{dnode}] "
                                    f"CPU: {cpu_percent:.1f}%, "
                                    f"Memory: {memory_info.rss/1048576.0:.2f}MB ({memory_percent:.2f}%), "
                                    f"Read: {io_counters.read_bytes/1048576.0:.2f}MB ({io_counters.read_count}), "
                                    f"Write: {io_counters.write_bytes/1048576.0:.2f}MB ({io_counters.write_count})"
                                )
                                
                                if is_warm_up:
                                    status += f" [é¢„çƒ­æœŸæ•°æ®] (å®ä¾‹ID: {instance_id})"
                                else:
                                    status += f" (å®ä¾‹ID: {instance_id})"
                                
                                # å†™å…¥æ—¥å¿—æ–‡ä»¶
                                self.write_metrics(dnode, status, print_to_console=(not is_warm_up))
                                # âœ… è°ƒè¯•è¾“å‡ºCPUé‡‡æ ·ä¿¡æ¯
                                if cycle_count <= 3 or (cycle_count % 20 == 0):  # å‰3æ¬¡æˆ–æ¯20æ¬¡æ‰“å°è°ƒè¯•ä¿¡æ¯
                                    print(f"è°ƒè¯•: {dnode} CPUé‡‡æ · - å‘¨æœŸ{cycle_count}, CPU: {cpu_percent:.1f}%")
                                    
                            except (psutil.NoSuchProcess, psutil.AccessDenied) as e:
                                print(f"è°ƒè¯•: è¿›ç¨‹{dnode}è®¿é—®å¤±è´¥: {str(e)}")
                                self.write_zero_metrics(dnode, timestamp, is_warm_up, instance_id)
                                self.processes[dnode] = None  # æ¸…ç†æ— æ•ˆçš„è¿›ç¨‹å¯¹è±¡
                        else:
                            # å¦‚æœé…ç½®è¦æ±‚ç›‘æ§æ­¤èŠ‚ç‚¹ä½†æ‰¾ä¸åˆ°è¿›ç¨‹
                            if dnode in self.monitor_nodes:
                                self.write_zero_metrics(dnode, timestamp, is_warm_up, instance_id)
                            
                    except Exception as e:
                        if not is_warm_up:
                            print(f"ç›‘æ§ {dnode} å‡ºé”™: {str(e)} (å®ä¾‹ID: {instance_id})")
                        self.write_zero_metrics(dnode, timestamp, is_warm_up, instance_id)
                
                # æ·»åŠ åˆ†éš”çº¿
                separator = f"---------------- (å®ä¾‹ID: {instance_id}) ----------------\n"
                for f in self.perf_files.values():
                    try:
                        if not f.closed:
                            f.write(separator)
                            f.flush()
                    except:
                        pass
                
                if not is_warm_up:
                    print(separator.strip())
                    
                # é¢„çƒ­æœŸç»“æŸæç¤º
                if is_warm_up and cycle_count == warm_up_cycles:
                    print(f"\n=== é¢„çƒ­æœŸç»“æŸï¼Œå¼€å§‹æ­£å¼æ€§èƒ½ç›‘æ§ (å®ä¾‹ID: {instance_id}) ===")
                    
                    # å†™å…¥é¢„çƒ­ç»“æŸæ ‡è®°
                    for f in self.perf_files.values():
                        try:
                            if not f.closed:
                                f.write(f"\n=== é¢„çƒ­æœŸç»“æŸï¼Œæ­£å¼ç›‘æ§å¼€å§‹ (å®ä¾‹ID: {instance_id}) ===\n")
                                f.write(f"{'='*60}\n")
                                f.flush()
                                print(f"è°ƒè¯•: å·²å†™å…¥é¢„çƒ­æœŸç»“æŸæ ‡è®°åˆ°æ€§èƒ½æ–‡ä»¶")
                        except Exception as e:
                            print(f"è°ƒè¯•: å†™å…¥é¢„çƒ­æœŸç»“æŸæ ‡è®°å¤±è´¥: {str(e)}")
                            pass
                
                # ç²¾ç¡®æ§åˆ¶é—´éš”æ—¶é—´
                elapsed = time.time() - start_time
                if elapsed < self.interval:
                    remaining_time = self.interval - elapsed
                    
                    # ä½¿ç”¨Event.wait()æ›¿ä»£sleepï¼Œèƒ½ç«‹å³å“åº”åœæ­¢ä¿¡å·
                    if self._stop_event.wait(timeout=remaining_time):
                        print(f"è°ƒè¯•: ç›‘æ§å®ä¾‹ {instance_id} åœ¨waitæœŸé—´æ”¶åˆ°åœæ­¢ä¿¡å·")
                        break
                
                self.count -= 1
                
                # æœ€ç»ˆæ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
                if not self._should_continue_monitoring():
                    print(f"è°ƒè¯•: ç›‘æ§å®ä¾‹ {instance_id} æ­£å¸¸ç»“æŸå¾ªç¯")
                    break
                
        except Exception as e:
            print(f"ç›‘æ§å‡ºé”™ (å®ä¾‹ID: {instance_id}): {str(e)}")  
        finally:
            self._is_running = False
            print(f"è°ƒè¯•: ç›‘æ§çº¿ç¨‹æ­£åœ¨æ¸…ç†èµ„æº (å®ä¾‹ID: {instance_id})...")
            
            # âœ… ç¡®ä¿æ‰€æœ‰æ–‡ä»¶å¥æŸ„éƒ½è¢«å…³é—­
            if hasattr(self, 'perf_files'):
                for f in self.perf_files.values():
                    try:
                        if not f.closed:
                            f.close()
                    except:
                        pass
                        
            print(f"è°ƒè¯•: ç›‘æ§çº¿ç¨‹å·²ç»“æŸ (å®ä¾‹ID: {instance_id})")
                   

def do_monitor(runtime, perf_file, deployment_mode='cluster', warm_up_time=0):
    """ç›‘æ§çº¿ç¨‹å‡½æ•°"""
    try:
        # ä¸åœ¨å­çº¿ç¨‹ä¸­ä½¿ç”¨ä¿¡å·å¤„ç†
        loader = MonitorSystemLoad(
            'taosd -c', 
            runtime, 
            perf_file, 
            use_signal=False,
            deployment_mode=deployment_mode,
            warm_up_time=warm_up_time 
        )
        loader.get_proc_status()
    except Exception as e:
        print(f"ç›‘æ§çº¿ç¨‹å‡ºé”™: {str(e)}")
    finally:
        print("ç›‘æ§çº¿ç¨‹ç»“æŸ")

def get_table_list(cursor):
    cursor.execute('use stream_from')

    sql = "select table_name from information_schema.ins_tables where db_name = 'stream_from' and stable_name='stb' order by table_name"
    cursor.execute(sql)

    res = cursor.fetchall()
    return res

def do_multi_insert(index, total, host, user, passwd, conf, tz):
    conn = taos.connect(
        host=host, user=user, password=passwd, config=conf, timezone=tz
    )

    cursor = conn.cursor()
    cursor.execute('use stream_from')

    start_ts = 1609430400000
    step = 5

    cursor.execute("create stable if not exists stb_result(wstart timestamp, minx float, maxx float, countx bigint) tags(gid bigint unsigned)")

    list = get_table_list(cursor)

    list = list[index*total: (index+1)*total]

    print("there are %d tables" % len(list))

    for index, n in enumerate(list):
        cursor.execute(f"create table if not exists {n[0]}_1 using stb_result tags(1)")
        count = 1
        while True:
            sql = (f"select cast({start_ts + step * 1000 * (count - 1)} as timestamp), min(c1), max(c2), count(c3) from stream_from.{n[0]} "
                   f"where ts >= {start_ts + step * 1000 * (count - 1)} and ts < {start_ts + step * 1000 * count}")
            cursor.execute(sql)

            res = cursor.fetchall()
            if res[0][3] == 0:
                break

            insert = f"insert into {n[0]}_1 values ({start_ts + step * 1000 * (count - 1)}, {res[0][1]}, {res[0][2]}, {res[0][3]})"
            cursor.execute(insert)
            count += 1
    conn.close()


'''
class StreamSQLTemplates_bak_for_phase1:
    """æµè®¡ç®— SQL æ¨¡æ¿é›†åˆ"""
    
    s2_2 = """
    create stream s2_2 trigger at_once 
        ignore expired 0 ignore update 0 into stream_to.stb2_2 
        as select _wstart as wstart,
        avg(c0), avg(c1),avg(c2), avg(c3),
        max(c0), max(c1), max(c2), max(c3),
        min(c0), min(c1), min(c2), min(c3)
        from stream_from.stb partition by tbname interval(15s);
    """
    
    s2_3 = """
    create stream s2_3 trigger window_close 
        ignore expired 0 ignore update 0 into stream_to.stb2_3 
        as select _wstart as wstart,
        avg(c0), avg(c1),avg(c2), avg(c3),
        max(c0), max(c1), max(c2), max(c3),
        min(c0), min(c1), min(c2), min(c3)
        from stream_from.stb partition by tbname interval(15s);
    """
    
    s2_4 = """
    create stream s2_4 trigger max_delay 5s 
        ignore expired 0 ignore update 0 into stream_to.stb2_4 
        as select _wstart as wstart,
        avg(c0), avg(c1),avg(c2), avg(c3),
        max(c0), max(c1), max(c2), max(c3),
        min(c0), min(c1), min(c2), min(c3)
        from stream_from.stb partition by tbname interval(15s);
    """
    
    s2_5 = """
    create stream s2_5 trigger FORCE_WINDOW_CLOSE into stream_to.stb2_5 
        as select _wstart as wstart,
        avg(c0), avg(c1),avg(c2), avg(c3),
        max(c0), max(c1), max(c2), max(c3),
        min(c0), min(c1), min(c2), min(c3)
        from stream_from.stb partition by tbname interval(15s);
    """
    
    s2_6 = """
    create stream s2_6 trigger CONTINUOUS_WINDOW_CLOSE 
        ignore expired 0 ignore update 0 into stream_to.stb2_6 
        as select _wstart as wstart,
        avg(c0), avg(c1),avg(c2), avg(c3),
        max(c0), max(c1), max(c2), max(c3),
        min(c0), min(c1), min(c2), min(c3)
        from stream_from.stb partition by tbname interval(15s);
    """
    
    s2_7 = """
    create stream stream_from.s2_7 INTERVAL(15s) SLIDING(15s)
            from stream_from.stb 
            partition by tbname 
            into stream_to.stb2_7
            as select _twstart ts, avg(c0), avg(c1), avg(c2), avg(c3),
            max(c0), max(c1), max(c2), max(c3),
            min(c0), min(c1), min(c2), min(c3)
            from %%tbname where ts >= _twstart and ts < _twend;
    """
    
    s2_8 = """
    create stream stream_from.s2_8 INTERVAL(15s) SLIDING(15s)
            from stream_from.stb 
            partition by tbname 
            into stream_to.stb2_8
            as select _twstart ts, avg(c0), avg(c1), avg(c2), avg(c3),
            max(c0), max(c1), max(c2), max(c3),
            min(c0), min(c1), min(c2), min(c3)
            from %%trows ;
    """
    
    s2_9 = """
    create stream stream_from.s2_9 INTERVAL(15s) SLIDING(15s)
            from stream_from.stb partition by tbname 
            STREAM_OPTIONS(MAX_DELAY(5s)) 
            into stream_to.stb2_9
            as select _twstart ts, avg(c0), avg(c1), avg(c2), avg(c3),
            max(c0), max(c1), max(c2), max(c3),
            min(c0), min(c1), min(c2), min(c3)
            from %%tbname where ts >= _twstart and ts < _twend;
    """
    
    s2_10 = """
    create stream stream_from.s2_10 INTERVAL(15s) SLIDING(15s) 
            from stream_from.stb 
            STREAM_OPTIONS(MAX_DELAY(5s))
            into stream_to.stb2_10
            as select _twstart ts, avg(c0), avg(c1), avg(c2), avg(c3),
            max(c0), max(c1), max(c2), max(c3),
            min(c0), min(c1), min(c2), min(c3)
            from %%trows ;
    """
    
    s2_11 = """
    create stream stream_from.s2_11 period(15s) 
            from stream_from.stb partition by tbname  
            into stream_to.stb2_11
            as select cast(_tlocaltime/1000000 as timestamp) ts, avg(c0), avg(c1), avg(c2), avg(c3),
            max(c0), max(c1), max(c2), max(c3),
            min(c0), min(c1), min(c2), min(c3)
            from %%tbname ;
    """    
    
    s2_12 = """
    create stream stream_from.s2_12 session(ts,10a)
            from stream_from.stb partition by tbname 
            into stream_to.stb2_12
            as select _twstart ts, avg(c0), avg(c1), avg(c2), avg(c3),
            max(c0), max(c1), max(c2), max(c3),
            min(c0), min(c1), min(c2), min(c3)
            from %%tbname where ts >= _twstart and ts <= _twend;
    """
    
    s2_13 = """
    create stream stream_from.s2_13 COUNT_WINDOW(1000)
            from stream_from.stb partition by tbname 
            into stream_to.stb2_13
            as select _twstart ts, avg(c0), avg(c1), avg(c2), avg(c3),
            max(c0), max(c1), max(c2), max(c3),
            min(c0), min(c1), min(c2), min(c3)
            from %%tbname where ts >= _twstart and ts <= _twend;
    """
    
    s2_14 = """
    create stream stream_from.s2_14 EVENT_WINDOW(START WITH c0 > -10000000 END WITH c0 < 10000000) 
            from stream_from.stb partition by tbname 
            into stream_to.stb2_14
            as select _twstart ts, avg(c0), avg(c1), avg(c2), avg(c3),
            max(c0), max(c1), max(c2), max(c3),
            min(c0), min(c1), min(c2), min(c3)
            from %%tbname where ts >= _twstart and ts <= _twend;
    """
    
    s2_15 = """
    create stream stream_from.s2_15 STATE_WINDOW(c0) 
            from stream_from.stb partition by tbname 
            into stream_to.stb2_15
            as select _twstart ts, avg(c0), avg(c1), avg(c2), avg(c3),
            max(c0), max(c1), max(c2), max(c3),
            min(c0), min(c1), min(c2), min(c3)
            from %%tbname where ts >= _twstart and ts <= _twend;
    """
    
    @classmethod
    def get_sql(cls, sql_type):
        """
        è·å–æŒ‡å®šç±»å‹çš„ SQL æ¨¡æ¿
        Args:
            sql_type: SQL ç±»å‹æ ‡è¯†ç¬¦ (case_id)
        Returns:
            å¯¹åº”çš„ SQL æ¨¡æ¿
        """
        sql_map = {
            's2_2': cls.s2_2,
            's2_3': cls.s2_3,
            's2_4': cls.s2_4,
            's2_5': cls.s2_5,
            's2_6': cls.s2_6,
            's2_7': cls.s2_7,
            's2_8': cls.s2_8,
            's2_9': cls.s2_9,
            's2_10': cls.s2_10,
            's2_11': cls.s2_11,
            's2_12': cls.s2_12,
            's2_13': cls.s2_13,
            's2_14': cls.s2_14,
            's2_15': cls.s2_15,
        }
        
        # å®šä¹‰æ‰¹é‡ SQL ç»„åˆ
        batch_map = {
            'all': {
                's2_7': cls.s2_7,
                's2_8': cls.s2_8,
                's2_9': cls.s2_9,
                's2_10': cls.s2_10,
                's2_11': cls.s2_11,
                's2_12': cls.s2_12,
                's2_13': cls.s2_13,
                's2_14': cls.s2_14,
                's2_15': cls.s2_15,
            }
        }
        # å¦‚æœè¯·æ±‚æ‰¹é‡ SQL
        if sql_type in batch_map:
            return batch_map[sql_type]
        
        # è¿”å›å•ä¸ª SQLï¼Œé»˜è®¤è¿”å› s2_7
        return sql_map.get(sql_type, cls.s2_7)  
'''


class StreamSQLTemplates:
    """æµè®¡ç®— SQL æ¨¡æ¿é›†åˆ - é‡æ„ç‰ˆæœ¬"""
    
    def __init__(self):
        # å®šä¹‰é»˜è®¤çš„èšåˆå‡½æ•°ç»„åˆ
        self.default_agg_columns = [
                "avg(c0), avg(c1), avg(c2), avg(c3)",
                "max(c0), max(c1), max(c2), max(c3)", 
                "min(c0), min(c1), min(c2), min(c3)"
        ]
        
        # å®šä¹‰é»˜è®¤çš„æŠ•å½±åˆ—
        self.default_select_columns = ["c0", "c1", "c2", "c3"]
        
    def get_select_stream(self, **kwargs):
        """æŸ¥è¯¢æ‰€æœ‰æµä¿¡æ¯"""
        return "select * from information_schema.ins_streams;"
    
    def _build_columns(self, agg_or_select='agg', custom_columns=None):
        """æ„å»ºæŸ¥è¯¢åˆ—
        
        Args:
            agg_or_select: 'agg' è¡¨ç¤ºèšåˆæŸ¥è¯¢, 'select' è¡¨ç¤ºæŠ•å½±æŸ¥è¯¢
            custom_columns: è‡ªå®šä¹‰åˆ—ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨è‡ªå®šä¹‰åˆ—
            
        Returns:
            str: æ„å»ºå¥½çš„åˆ—å­—ç¬¦ä¸²
        """
        if custom_columns:
            if isinstance(custom_columns, list):
                return ", ".join(custom_columns)
            return custom_columns
            
        if agg_or_select == 'agg':
            return ",\n        ".join(self.default_agg_columns)
        else:  
            return ", ".join(self.default_select_columns)
    
    def _build_from_clause(self, tbname_or_trows_or_sourcetable='sourcetable', is_period=False, is_sliding=False):
        """æ„å»º FROM å­å¥
        
        Args:
            tbname_or_trows_or_sourcetable: FROM å­å¥ç±»å‹
                - 'tbname': %%tbname where ts >= _twstart and ts <= _twend  
                - 'trows': %%trows
                - 'sourcetable': æ˜ç¡®æŒ‡å®šæºè¡¨å where ts >= _twstart and ts <= _twend
            is_period: æ˜¯å¦ä¸º period ç±»å‹çš„æµè®¡ç®—
            is_sliding: æ˜¯å¦ä¸º sliding ç±»å‹çš„æµè®¡ç®—
            
        Returns:
            str: FROM å­å¥å­—ç¬¦ä¸²
        """
        if tbname_or_trows_or_sourcetable == 'trows':
            return "%%trows "
        elif tbname_or_trows_or_sourcetable == 'sourcetable':
            # è¿”å›å ä½ç¬¦ï¼Œåœ¨å…·ä½“æ¨¡æ¿ä¸­ä¼šè¢«æ›¿æ¢ä¸ºå®é™…çš„è¡¨å
            if is_sliding:
                return "SOURCE_TABLE_PLACEHOLDER where ts >= _tprev_ts and ts <= _tnext_ts"
            elif is_period:
                return "SOURCE_TABLE_PLACEHOLDER where ts >= cast(_tprev_localtime/1000000 as timestamp) and ts <= cast(_tnext_localtime/1000000 as timestamp)"
            else:
                return "SOURCE_TABLE_PLACEHOLDER where ts >= _twstart and ts <= _twend"
        else:  # 'tbname' and çº³ç§’
            if is_sliding:
                return "%%tbname where ts >= _tprev_ts and ts <= _tnext_ts"
            elif is_period:
                return "%%tbname where ts >= cast(_tprev_localtime/1000000 as timestamp) and ts <= cast(_tnext_localtime/1000000 as timestamp)"
            else:
                return "%%tbname where ts >= _twstart and ts <= _twend"

    
    def _build_from_source_and_clause(self, source_type='stb', stream_index=None, tbname_or_trows_or_sourcetable='sourcetable', is_period=False, is_sliding=False):
        """æ„å»ºæ•°æ®æºå’ŒFROMå­å¥çš„ç»„åˆ
        
        Args:
            source_type: æ•°æ®æºç±»å‹ ('stb', 'tb')
            stream_index: æµçš„ç´¢å¼•ç¼–å·
            tbname_or_trows_or_sourcetable: FROMå­å¥ç±»å‹
            is_period: æ˜¯å¦ä¸º period ç±»å‹çš„æµè®¡ç®—
            is_sliding: æ˜¯å¦ä¸º sliding ç±»å‹çš„æµè®¡ç®—
                
        Returns:
            tuple: (from_source, from_clause)
        """
        from_source = self._build_from_source(source_type, stream_index)
        
        if tbname_or_trows_or_sourcetable == 'sourcetable':
            # å¯¹äºsourcetableæ¨¡å¼ï¼ŒFROMå­å¥ç›´æ¥ä½¿ç”¨å…·ä½“çš„è¡¨å
            if source_type == 'tb':
                # å­è¡¨æƒ…å†µï¼šä½¿ç”¨å…·ä½“çš„å­è¡¨å
                if stream_index is not None:
                    table_name = f"stream_from.ctb0_{stream_index}"
                else:
                    table_name = "stream_from.ctb0_0"
            else:  # 'stb'
                # è¶…çº§è¡¨æƒ…å†µï¼šä½¿ç”¨è¶…çº§è¡¨å
                table_name = "stream_from.stb"
                
            # period ç±»å‹ä½¿ç”¨ä¸åŒçš„æ—¶é—´å˜é‡
            if is_sliding:
                from_clause = f"{table_name} where ts >= _tprev_ts and ts <= _tnext_ts"
            elif is_period:
                from_clause = f"{table_name} where ts >= cast(_tprev_localtime/1000000 as timestamp) and ts <= cast(_tnext_localtime/1000000 as timestamp)"
            else:
                from_clause = f"{table_name} where ts >= _twstart and ts <= _twend"
        else:
            # å…¶ä»–æ¨¡å¼ä½¿ç”¨åŸæœ‰é€»è¾‘
            from_clause = self._build_from_clause(tbname_or_trows_or_sourcetable, is_period, is_sliding)
            
        return from_source, from_clause        
    
    def _build_partition_clause(self, partition_type='none'):
        """æ„å»ºåˆ†åŒºå­å¥
        
        Args:
            partition_type: åˆ†åŒºç±»å‹
                - 'none': ä¸åˆ†ç»„
                - 'tbname': æŒ‰å­è¡¨ååˆ†ç»„  
                - 'tag': æŒ‰tagåˆ†ç»„
                
        Returns:
            str: åˆ†åŒºå­å¥å­—ç¬¦ä¸²
        """
        if partition_type == 'tbname':
            return "partition by tbname"
        elif partition_type == 'tag':
            return "partition by t0"  # å‡è®¾ç¬¬ä¸€ä¸ªtagå­—æ®µåä¸ºt0
        else:  # 'none'
            return ""
    
    def _build_from_source(self, source_type='stb', stream_index=None):
        """æ„å»ºæ•°æ®æº
        
        Args:
            source_type: æ•°æ®æºç±»å‹
                - 'stb': è¶…çº§è¡¨ stream_from.stb
                - 'tb': å­è¡¨ stream_from.ctb0_X
            stream_index: æµçš„ç´¢å¼•ç¼–å·ï¼ˆç”¨äºå¤šæµæ—¶é€‰æ‹©ä¸åŒå­è¡¨ï¼‰
                
        Returns:
            str: æ•°æ®æºå­—ç¬¦ä¸²
        """
        if source_type == 'tb':
            # å¦‚æœæœ‰æµç´¢å¼•ï¼Œä½¿ç”¨å¯¹åº”çš„å­è¡¨ï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤çš„ctb0_0
            if stream_index is not None:
                # å­è¡¨ç¼–å·ä»1å¼€å§‹
                table_index = stream_index 
                #print(f"è°ƒè¯•: ç”Ÿæˆå­è¡¨å stream_from.ctb0_{table_index}")
                return f"stream_from.ctb0_{table_index}"
            else:
                #print(f"è°ƒè¯•: ä½¿ç”¨é»˜è®¤å­è¡¨ stream_from.ctb0_0")
                return "stream_from.ctb0_0"  # é»˜è®¤å­è¡¨
        else:  # 'stb'
            return "stream_from.stb"
    
    def _generate_stream_name(self, base_type, source_type, partition_type, agg_or_select='agg', tbname_or_trows_or_sourcetable='sourcetable', stream_index=None):
        """ç”Ÿæˆæµåç§°
        
        Args:
            base_type: åŸºç¡€ç±»å‹ (å¦‚ 's2_7', 'intervalsliding')
            source_type: æ•°æ®æºç±»å‹ ('stb', 'tb') 
            partition_type: åˆ†åŒºç±»å‹ ('none', 'tbname', 'tag')
            agg_or_select: æŸ¥è¯¢ç±»å‹ ('agg', 'select')
            tbname_or_trows_or_sourcetable: FROMå­å¥ç±»å‹ ('tbname', 'trows', 'sourcetable')
            stream_index: æµç´¢å¼•ç¼–å·ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            str: ç”Ÿæˆçš„æµåç§°
        """
        # æ„å»ºåç§°ç»„ä»¶
        source_part = "stb" if source_type == "stb" else "tb"
        
        if partition_type == 'none':
            partition_part = ""
        elif partition_type == 'tbname':
            partition_part = "_partition_by_tbname" 
        elif partition_type == 'tag':
            partition_part = "_partition_by_tag"
        else:
            partition_part = ""
            
        # æŸ¥è¯¢ç±»å‹éƒ¨åˆ†
        query_part = f"_{agg_or_select}"
        
        # FROMå­å¥ç±»å‹éƒ¨åˆ†
        if tbname_or_trows_or_sourcetable == 'sourcetable':
            if source_type == 'stb':
                # è¶…çº§è¡¨æƒ…å†µï¼šåŠ ä¸Š stb åç¼€
                from_part = "_sourcetable_stb"
            else:  # 'tb'
                # å­è¡¨æƒ…å†µï¼šåŠ ä¸Šå…·ä½“çš„å­è¡¨å
                if stream_index is not None:
                    from_part = f"_sourcetable_ctb0_{stream_index}"
                else:
                    from_part = "_sourcetable_ctb0_0"
        else:
            from_part = f"_{tbname_or_trows_or_sourcetable}"
        
        # åŸºç¡€åç§°ç»„åˆ
        stream_name = f"stream_from.{base_type}_{source_part}{partition_part}{query_part}{from_part}"
        
        # å¦‚æœæœ‰æµç´¢å¼•ï¼Œæ·»åŠ åˆ°æœ«å°¾
        if stream_index is not None and not (tbname_or_trows_or_sourcetable == 'sourcetable' and source_type == 'tb'):
            stream_name += f"_{stream_index}"
            
        return stream_name
    
    def _generate_target_table(self, base_type, source_type, partition_type, agg_or_select='agg', tbname_or_trows_or_sourcetable='sourcetable', stream_index=None):
        """ç”Ÿæˆç›®æ ‡è¡¨åç§°"""
        source_part = "stb" if source_type == "stb" else "tb"
        
        if partition_type == 'none':
            partition_part = ""
        elif partition_type == 'tbname':
            partition_part = "_partition_by_tbname"
        elif partition_type == 'tag':
            partition_part = "_partition_by_tag"
        else:
            partition_part = ""
        
        # æŸ¥è¯¢ç±»å‹éƒ¨åˆ†
        query_part = f"_{agg_or_select}"
        
        # FROMå­å¥ç±»å‹éƒ¨åˆ†
        if tbname_or_trows_or_sourcetable == 'sourcetable':
            if source_type == 'stb':
                # è¶…çº§è¡¨æƒ…å†µï¼šåŠ ä¸Š stb åç¼€
                from_part = "_sourcetable_stb"
            else:  # 'tb'
                # å­è¡¨æƒ…å†µï¼šåŠ ä¸Šå…·ä½“çš„å­è¡¨å
                if stream_index is not None:
                    from_part = f"_sourcetable_ctb0_{stream_index}"
                else:
                    from_part = "_sourcetable_ctb0_0"
        else:
            from_part = f"_{tbname_or_trows_or_sourcetable}"
        
        # åŸºç¡€åç§°ç»„åˆ
        target_table = f"stream_to.{base_type}_{source_part}{partition_part}{query_part}{from_part}"
        
        # å¦‚æœæœ‰æµç´¢å¼•ï¼Œæ·»åŠ åˆ°æœ«å°¾
        if stream_index is not None and not (tbname_or_trows_or_sourcetable == 'sourcetable' and source_type == 'tb'):
            target_table += f"_{stream_index}"
            
        return target_table
    
        
    # ========== é€šç”¨æ¨¡æ¿ç”Ÿæˆæ–¹æ³• ==========
    def get_intervalsliding_template(self, source_type='stb', partition_type='tbname', 
                           agg_or_select='agg', tbname_or_trows_or_sourcetable='sourcetable', custom_columns=None, stream_index=None):
        """ç”Ÿæˆæ»‘åŠ¨çª—å£æ¨¡æ¿
        
        Args:
            source_type: 'stb'(è¶…çº§è¡¨) æˆ– 'tb'(å­è¡¨)
            partition_type: 'none'(ä¸åˆ†ç»„), 'tbname'(æŒ‰å­è¡¨å), 'tag'(æŒ‰tag)
            agg_or_select: 'agg' æˆ– 'select'
            tbname_or_trows_or_sourcetable: 'tbname' æˆ– 'trows' æˆ– 'sourcetable'
            custom_columns: è‡ªå®šä¹‰åˆ—
        """
        columns = self._build_columns(agg_or_select, custom_columns)
        from_source, from_clause = self._build_from_source_and_clause(source_type, stream_index, tbname_or_trows_or_sourcetable, is_period=False)
        partition_clause = self._build_partition_clause(partition_type)
        
        stream_name = self._generate_stream_name('intervalsliding', source_type, partition_type, agg_or_select, tbname_or_trows_or_sourcetable, stream_index)
        target_table = self._generate_target_table('intervalsliding', source_type, partition_type, agg_or_select, tbname_or_trows_or_sourcetable, stream_index)
        
        # å¦‚æœæœ‰æµç´¢å¼•ï¼Œåœ¨æµåç§°å’Œç›®æ ‡è¡¨ä¸­æ·»åŠ ç´¢å¼•
        if stream_index is not None:
            stream_name += f"_{stream_index}"
            target_table += f"_{stream_index}"
        
        # æ„å»ºå®Œæ•´çš„SQL
        partition_line = f"\n            {partition_clause}" if partition_clause else ""
        
        return f"""
    create stream {stream_name} INTERVAL(15s) SLIDING(15s)
            from {from_source}{partition_line}
            into {target_table}
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_session_template(self, source_type='stb', partition_type='tbname',
                           agg_or_select='agg', tbname_or_trows_or_sourcetable='sourcetable', custom_columns=None, stream_index=None):
        """ç”Ÿæˆä¼šè¯çª—å£æ¨¡æ¿"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_source, from_clause = self._build_from_source_and_clause(source_type, stream_index, tbname_or_trows_or_sourcetable, is_period=False)
        partition_clause = self._build_partition_clause(partition_type)
         
        stream_name = self._generate_stream_name('session', source_type, partition_type, agg_or_select, tbname_or_trows_or_sourcetable, stream_index)
        target_table = self._generate_target_table('session', source_type, partition_type, agg_or_select, tbname_or_trows_or_sourcetable, stream_index)
        
        # å¦‚æœæœ‰æµç´¢å¼•ï¼Œåœ¨æµåç§°å’Œç›®æ ‡è¡¨ä¸­æ·»åŠ ç´¢å¼•
        if stream_index is not None:
            stream_name += f"_{stream_index}"
            target_table += f"_{stream_index}"
        
        partition_line = f"\n            {partition_clause}" if partition_clause else ""
        
        return f"""
    create stream {stream_name} session(ts,10a)
            from {from_source}{partition_line}
            into {target_table}
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_count_template(self, source_type='stb', partition_type='tbname',
                         agg_or_select='agg', tbname_or_trows_or_sourcetable='sourcetable', custom_columns=None, stream_index=None):
        """ç”Ÿæˆè®¡æ•°çª—å£æ¨¡æ¿"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_source, from_clause = self._build_from_source_and_clause(source_type, stream_index, tbname_or_trows_or_sourcetable, is_period=False)
        partition_clause = self._build_partition_clause(partition_type)
        
        stream_name = self._generate_stream_name('count', source_type, partition_type, agg_or_select, tbname_or_trows_or_sourcetable, stream_index)
        target_table = self._generate_target_table('count', source_type, partition_type, agg_or_select, tbname_or_trows_or_sourcetable, stream_index)
        
        # å¦‚æœæœ‰æµç´¢å¼•ï¼Œåœ¨æµåç§°å’Œç›®æ ‡è¡¨ä¸­æ·»åŠ ç´¢å¼•
        if stream_index is not None:
            stream_name += f"_{stream_index}"
            target_table += f"_{stream_index}"
        
        partition_line = f"\n            {partition_clause}" if partition_clause else ""
        
        return f"""
    create stream {stream_name} COUNT_WINDOW(1000)
            from {from_source}{partition_line}
            into {target_table}
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_event_template(self, source_type='stb', partition_type='tbname',
                         agg_or_select='agg', tbname_or_trows_or_sourcetable='sourcetable', custom_columns=None, stream_index=None):
        """ç”Ÿæˆäº‹ä»¶çª—å£æ¨¡æ¿"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_source, from_clause = self._build_from_source_and_clause(source_type, stream_index, tbname_or_trows_or_sourcetable, is_period=False)
        partition_clause = self._build_partition_clause(partition_type)
        
        stream_name = self._generate_stream_name('event', source_type, partition_type, agg_or_select, tbname_or_trows_or_sourcetable, stream_index)
        target_table = self._generate_target_table('event', source_type, partition_type, agg_or_select, tbname_or_trows_or_sourcetable, stream_index)
        
        # å¦‚æœæœ‰æµç´¢å¼•ï¼Œåœ¨æµåç§°å’Œç›®æ ‡è¡¨ä¸­æ·»åŠ ç´¢å¼•
        if stream_index is not None:
            stream_name += f"_{stream_index}"
            target_table += f"_{stream_index}"
        
        partition_line = f"\n            {partition_clause}" if partition_clause else ""
        
        return f"""
    create stream {stream_name} EVENT_WINDOW(START WITH c0 > -10000000 END WITH c0 < 10000000)
            from {from_source}{partition_line}
            into {target_table}
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_state_template(self, source_type='stb', partition_type='tbname',
                         agg_or_select='agg', tbname_or_trows_or_sourcetable='sourcetable', custom_columns=None, stream_index=None):
        """ç”ŸæˆçŠ¶æ€çª—å£æ¨¡æ¿"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_source, from_clause = self._build_from_source_and_clause(source_type, stream_index, tbname_or_trows_or_sourcetable, is_period=False)
        partition_clause = self._build_partition_clause(partition_type)
         
        stream_name = self._generate_stream_name('state', source_type, partition_type, agg_or_select, tbname_or_trows_or_sourcetable, stream_index)
        target_table = self._generate_target_table('state', source_type, partition_type, agg_or_select, tbname_or_trows_or_sourcetable, stream_index)
        
        # å¦‚æœæœ‰æµç´¢å¼•ï¼Œåœ¨æµåç§°å’Œç›®æ ‡è¡¨ä¸­æ·»åŠ ç´¢å¼•
        if stream_index is not None:
            stream_name += f"_{stream_index}"
            target_table += f"_{stream_index}"
        
        partition_line = f"\n            {partition_clause}" if partition_clause else ""
        
        return f"""
    create stream {stream_name} STATE_WINDOW(c0)
            from {from_source}{partition_line}
            into {target_table}
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_period_template(self, source_type='stb', partition_type='tbname',
                          agg_or_select='agg', tbname_or_trows_or_sourcetable='sourcetable', custom_columns=None, stream_index=None):
        """ç”Ÿæˆå®šæ—¶è§¦å‘æ¨¡æ¿ """
        columns = self._build_columns(agg_or_select, custom_columns)
        # from_source, from_clause = self._build_from_source_and_clause(source_type, stream_index, tbname_or_trows_or_sourcetable)
        partition_clause = self._build_partition_clause(partition_type)
        
        # period ç±»å‹ç‰¹æ®Šå¤„ç†ï¼šä½¿ç”¨ä¸“é—¨çš„æ—¶é—´å˜é‡
        from_source = self._build_from_source(source_type, stream_index)
        
        # period ç±»å‹çš„ FROM å­å¥å¤„ç†
        if tbname_or_trows_or_sourcetable == 'trows':
            # period + trows çš„ç»„åˆï¼šç›´æ¥ä½¿ç”¨ %%trowsï¼ˆä¸æ”¯æŒæ—¶é—´æ¡ä»¶ï¼‰
            from_clause = "%%trows"
        elif tbname_or_trows_or_sourcetable == 'sourcetable':
            # period + sourcetableï¼šä½¿ç”¨å…·ä½“è¡¨å + period ä¸“ç”¨æ—¶é—´å˜é‡
            if source_type == 'tb':
                if stream_index is not None:
                    table_name = f"stream_from.ctb0_{stream_index}"
                else:
                    table_name = "stream_from.ctb0_0"
            else:  # 'stb'
                table_name = "stream_from.stb"
            from_clause = f"{table_name} where ts >= cast(_tprev_localtime/1000000 as timestamp) and ts <= cast(_tnext_localtime/1000000 as timestamp)"
        else:  # 'tbname'
            # period + tbnameï¼šä½¿ç”¨ %%tbname + period ä¸“ç”¨æ—¶é—´å˜é‡
            from_clause = "%%tbname where ts >= cast(_tprev_localtime/1000000 as timestamp) and ts <= cast(_tnext_localtime/1000000 as timestamp)"
         
        stream_name = self._generate_stream_name('period', source_type, partition_type, agg_or_select, tbname_or_trows_or_sourcetable, stream_index)
        target_table = self._generate_target_table('period', source_type, partition_type, agg_or_select, tbname_or_trows_or_sourcetable, stream_index)
        
        # å¦‚æœæœ‰æµç´¢å¼•ï¼Œåœ¨æµåç§°å’Œç›®æ ‡è¡¨ä¸­æ·»åŠ ç´¢å¼•
        if stream_index is not None:
            stream_name += f"_{stream_index}"
            target_table += f"_{stream_index}"
        
        partition_line = f"\n            {partition_clause}" if partition_clause else ""
        
        return f"""
    create stream {stream_name} period(15s)
            from {from_source}{partition_line}
            into {target_table}
            as select cast(_tlocaltime/1000000 as timestamp) ts, {columns}
            from {from_clause};
    """


    def get_sliding_template(self, source_type='stb', partition_type='tbname',
                           agg_or_select='agg', tbname_or_trows_or_sourcetable='sourcetable', custom_columns=None, stream_index=None):
        """ç”Ÿæˆ sliding çª—å£æ¨¡æ¿ - æ–°å¢çª—å£ç±»å‹"""
        columns = self._build_columns(agg_or_select, custom_columns)
        partition_clause = self._build_partition_clause(partition_type)
        
        # sliding ç±»å‹ç‰¹æ®Šå¤„ç†ï¼šä½¿ç”¨ä¸“é—¨çš„æ—¶é—´å˜é‡
        from_source = self._build_from_source(source_type, stream_index)
        
        # sliding ç±»å‹çš„ FROM å­å¥å¤„ç†
        if tbname_or_trows_or_sourcetable == 'trows':
            # sliding + trows çš„ç»„åˆï¼šç›´æ¥ä½¿ç”¨ %%trowsï¼ˆä¸æ”¯æŒæ—¶é—´æ¡ä»¶ï¼‰
            from_clause = "%%trows"
        elif tbname_or_trows_or_sourcetable == 'sourcetable':
            # sliding + sourcetableï¼šä½¿ç”¨å…·ä½“è¡¨å + sliding ä¸“ç”¨æ—¶é—´å˜é‡
            if source_type == 'tb':
                if stream_index is not None:
                    table_name = f"stream_from.ctb0_{stream_index}"
                else:
                    table_name = "stream_from.ctb0_0"
            else:  # 'stb'
                table_name = "stream_from.stb"
            from_clause = f"{table_name} where ts >= _tprev_ts and ts <= _tnext_ts"
        else:  # 'tbname'
            # sliding + tbnameï¼šä½¿ç”¨ %%tbname + sliding ä¸“ç”¨æ—¶é—´å˜é‡
            from_clause = "%%tbname where ts >= _tprev_ts and ts <= _tnext_ts"
         
        stream_name = self._generate_stream_name('sliding', source_type, partition_type, agg_or_select, tbname_or_trows_or_sourcetable, stream_index)
        target_table = self._generate_target_table('sliding', source_type, partition_type, agg_or_select, tbname_or_trows_or_sourcetable, stream_index)
        
        partition_line = f"\n            {partition_clause}" if partition_clause else ""
        
        return f"""
    create stream {stream_name} sliding(15s)
            from {from_source}{partition_line}
            into {target_table}
            as select _tcurrent_ts ts, {columns}
            from {from_clause};
    """
       
    # ========== ç»„åˆç”Ÿæˆæ–¹æ³• ==========
    def get_intervalsliding_group_detailed(self, **kwargs):
        """è·å–è¯¦ç»†çš„æ»‘åŠ¨çª—å£ç»„åˆ (4ç§ç»„åˆ)"""
        return {
            'intervalsliding_stb': self.get_intervalsliding_template(source_type='stb', partition_type='none', **kwargs),
            'intervalsliding_stb_partition_by_tbname': self.get_intervalsliding_template(source_type='stb', partition_type='tbname', **kwargs),
            'intervalsliding_stb_partition_by_tag': self.get_intervalsliding_template(source_type='stb', partition_type='tag', **kwargs),
            'intervalsliding_tb': self.get_intervalsliding_template(source_type='tb', partition_type='none', **kwargs)
        }

    def get_sliding_group_detailed(self, **kwargs):
        """è·å–è¯¦ç»†çš„ sliding çª—å£ç»„åˆ"""
        return {
            'sliding_stb': self.get_sliding_template(source_type='stb', partition_type='none', **kwargs),
            'sliding_stb_partition_by_tbname': self.get_sliding_template(source_type='stb', partition_type='tbname', **kwargs),
            'sliding_stb_partition_by_tag': self.get_sliding_template(source_type='stb', partition_type='tag', **kwargs),
            'sliding_tb': self.get_sliding_template(source_type='tb', partition_type='none', **kwargs)
        }
            
    def get_session_group_detailed(self, **kwargs):
        """è·å–è¯¦ç»†çš„ä¼šè¯çª—å£ç»„åˆ"""
        return {
            'session_stb': self.get_session_template(source_type='stb', partition_type='none', **kwargs),
            'session_stb_partition_by_tbname': self.get_session_template(source_type='stb', partition_type='tbname', **kwargs),
            'session_stb_partition_by_tag': self.get_session_template(source_type='stb', partition_type='tag', **kwargs),
            'session_tb': self.get_session_template(source_type='tb', partition_type='none', **kwargs)
        }
    
    def get_count_group_detailed(self, **kwargs):
        """è·å–è¯¦ç»†çš„è®¡æ•°çª—å£ç»„åˆ"""
        return {
            'count_stb': self.get_count_template(source_type='stb', partition_type='none', **kwargs),
            'count_stb_partition_by_tbname': self.get_count_template(source_type='stb', partition_type='tbname', **kwargs),
            'count_stb_partition_by_tag': self.get_count_template(source_type='stb', partition_type='tag', **kwargs),
            'count_tb': self.get_count_template(source_type='tb', partition_type='none', **kwargs)
        }
    
    def get_event_group_detailed(self, **kwargs):
        """è·å–è¯¦ç»†çš„äº‹ä»¶çª—å£ç»„åˆ"""
        return {
            'event_stb': self.get_event_template(source_type='stb', partition_type='none', **kwargs),
            'event_stb_partition_by_tbname': self.get_event_template(source_type='stb', partition_type='tbname', **kwargs),
            'event_stb_partition_by_tag': self.get_event_template(source_type='stb', partition_type='tag', **kwargs),
            'event_tb': self.get_event_template(source_type='tb', partition_type='none', **kwargs)
        }
    
    def get_state_group_detailed(self, **kwargs):
        """è·å–è¯¦ç»†çš„çŠ¶æ€çª—å£ç»„åˆ"""
        return {
            'state_stb': self.get_state_template(source_type='stb', partition_type='none', **kwargs),
            'state_stb_partition_by_tbname': self.get_state_template(source_type='stb', partition_type='tbname', **kwargs),
            'state_stb_partition_by_tag': self.get_state_template(source_type='stb', partition_type='tag', **kwargs),
            'state_tb': self.get_state_template(source_type='tb', partition_type='none', **kwargs)
        }
    
    def get_period_group_detailed(self, **kwargs):
        """è·å–è¯¦ç»†çš„å®šæ—¶è§¦å‘ç»„åˆ"""
        # period ä¸æ”¯æŒ tbname_or_trows_or_sourcetable å‚æ•°
        filtered_kwargs = {k: v for k, v in kwargs.items() if k != 'tbname_or_trows_or_sourcetable'}
        return {
            'period_stb': self.get_period_template(source_type='stb', partition_type='none', **filtered_kwargs),
            'period_stb_partition_by_tbname': self.get_period_template(source_type='stb', partition_type='tbname', **filtered_kwargs),
            'period_stb_partition_by_tag': self.get_period_template(source_type='stb', partition_type='tag', **filtered_kwargs),
            'period_tb': self.get_period_template(source_type='tb', partition_type='none', **filtered_kwargs)
        }
        
    def get_tbname_agg_group(self, **kwargs):
        """è·å– tbname + agg ç»„åˆçš„æ‰€æœ‰çª—å£ç±»å‹
        
        å›ºå®šå‚æ•°: tbname_or_trows_or_sourcetable='tbname', agg_or_select='agg'
        å¿½ç•¥å‘½ä»¤è¡Œä¼ å…¥çš„è¿™ä¸¤ä¸ªå‚æ•°
        """
        # å›ºå®šå‚æ•°ï¼Œå¿½ç•¥ä¼ å…¥çš„å‚æ•°
        fixed_kwargs = {k: v for k, v in kwargs.items() if k not in ['tbname_or_trows_or_sourcetable', 'agg_or_select']}
        fixed_kwargs.update({
            'tbname_or_trows_or_sourcetable': 'tbname',
            'agg_or_select': 'agg'
        })
        
        return {
            # intervalsliding çª—å£ - æ‰€æœ‰ç»„åˆ
            'intervalsliding_stb_tbname_agg': self.get_intervalsliding_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'intervalsliding_stb_partition_by_tbname_agg': self.get_intervalsliding_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'intervalsliding_stb_partition_by_tag_agg': self.get_intervalsliding_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'intervalsliding_tb_agg': self.get_intervalsliding_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # sliding çª—å£ - æ‰€æœ‰ç»„åˆ
            'sliding_stb_tbname_agg': self.get_sliding_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'sliding_stb_partition_by_tbname_agg': self.get_sliding_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'sliding_stb_partition_by_tag_agg': self.get_sliding_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'sliding_tb_agg': self.get_sliding_template(source_type='tb', partition_type='none', **fixed_kwargs),
                        
            # ä¼šè¯çª—å£ - æ‰€æœ‰ç»„åˆ
            'session_stb_tbname_agg': self.get_session_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'session_stb_partition_by_tbname_agg': self.get_session_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'session_stb_partition_by_tag_agg': self.get_session_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'session_tb_agg': self.get_session_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # è®¡æ•°çª—å£ - æ‰€æœ‰ç»„åˆ
            'count_stb_tbname_agg': self.get_count_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'count_stb_partition_by_tbname_agg': self.get_count_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'count_stb_partition_by_tag_agg': self.get_count_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'count_tb_agg': self.get_count_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # äº‹ä»¶çª—å£ - æ‰€æœ‰ç»„åˆ
            'event_stb_tbname_agg': self.get_event_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'event_stb_partition_by_tbname_agg': self.get_event_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'event_stb_partition_by_tag_agg': self.get_event_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'event_tb_agg': self.get_event_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # çŠ¶æ€çª—å£ - æ‰€æœ‰ç»„åˆ
            'state_stb_tbname_agg': self.get_state_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'state_stb_partition_by_tbname_agg': self.get_state_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'state_stb_partition_by_tag_agg': self.get_state_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'state_tb_agg': self.get_state_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # å®šæ—¶è§¦å‘ - æ‰€æœ‰ç»„åˆ
            'period_stb_tbname_agg': self.get_period_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'period_stb_partition_by_tbname_agg': self.get_period_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'period_stb_partition_by_tag_agg': self.get_period_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'period_tb_agg': self.get_period_template(source_type='tb', partition_type='none', **fixed_kwargs),
        }

    def get_tbname_select_group(self, **kwargs):
        """è·å– tbname + select ç»„åˆçš„æ‰€æœ‰çª—å£ç±»å‹
        
        å›ºå®šå‚æ•°: tbname_or_trows_or_sourcetable='tbname', agg_or_select='select'
        å¿½ç•¥å‘½ä»¤è¡Œä¼ å…¥çš„è¿™ä¸¤ä¸ªå‚æ•°
        """
        # å›ºå®šå‚æ•°ï¼Œå¿½ç•¥ä¼ å…¥çš„å‚æ•°
        fixed_kwargs = {k: v for k, v in kwargs.items() if k not in ['tbname_or_trows_or_sourcetable', 'agg_or_select']}
        fixed_kwargs.update({
            'tbname_or_trows_or_sourcetable': 'tbname',
            'agg_or_select': 'select'
        })
        
        return {
            # intervalslidingçª—å£ - æ‰€æœ‰ç»„åˆ
            'intervalsliding_stb_tbname_select': self.get_intervalsliding_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'intervalsliding_stb_partition_by_tbname_select': self.get_intervalsliding_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'intervalsliding_stb_partition_by_tag_select': self.get_intervalsliding_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'intervalsliding_tb_select': self.get_intervalsliding_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # sliding çª—å£ - æ‰€æœ‰ç»„åˆ
            'sliding_stb_tbname_select': self.get_sliding_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'sliding_stb_partition_by_tbname_select': self.get_sliding_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'sliding_stb_partition_by_tag_select': self.get_sliding_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'sliding_tb_select': self.get_sliding_template(source_type='tb', partition_type='none', **fixed_kwargs),
                        
            # ä¼šè¯çª—å£ - æ‰€æœ‰ç»„åˆ
            'session_stb_tbname_select': self.get_session_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'session_stb_partition_by_tbname_select': self.get_session_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'session_stb_partition_by_tag_select': self.get_session_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'session_tb_select': self.get_session_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # è®¡æ•°çª—å£ - æ‰€æœ‰ç»„åˆ
            'count_stb_tbname_select': self.get_count_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'count_stb_partition_by_tbname_select': self.get_count_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'count_stb_partition_by_tag_select': self.get_count_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'count_tb_select': self.get_count_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # äº‹ä»¶çª—å£ - æ‰€æœ‰ç»„åˆ
            'event_stb_tbname_select': self.get_event_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'event_stb_partition_by_tbname_select': self.get_event_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'event_stb_partition_by_tag_select': self.get_event_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'event_tb_select': self.get_event_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # çŠ¶æ€çª—å£ - æ‰€æœ‰ç»„åˆ
            'state_stb_tbname_select': self.get_state_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'state_stb_partition_by_tbname_select': self.get_state_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'state_stb_partition_by_tag_select': self.get_state_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'state_tb_select': self.get_state_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # å®šæ—¶è§¦å‘ - æ‰€æœ‰ç»„åˆ
            'period_stb_tbname_select': self.get_period_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'period_stb_partition_by_tbname_select': self.get_period_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'period_stb_partition_by_tag_select': self.get_period_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'period_tb_select': self.get_period_template(source_type='tb', partition_type='none', **fixed_kwargs),
        }

    def get_trows_agg_group(self, **kwargs):
        """è·å– trows + agg ç»„åˆçš„æ‰€æœ‰çª—å£ç±»å‹
        
        å›ºå®šå‚æ•°: tbname_or_trows_or_sourcetable='trows', agg_or_select='agg'
        å¿½ç•¥å‘½ä»¤è¡Œä¼ å…¥çš„è¿™ä¸¤ä¸ªå‚æ•°
        """
        # å›ºå®šå‚æ•°ï¼Œå¿½ç•¥ä¼ å…¥çš„å‚æ•°
        fixed_kwargs = {k: v for k, v in kwargs.items() if k not in ['tbname_or_trows_or_sourcetable', 'agg_or_select']}
        fixed_kwargs.update({
            'tbname_or_trows_or_sourcetable': 'trows',
            'agg_or_select': 'agg'
        })
        
        return {
            # intervalsliding çª—å£ - æ‰€æœ‰ç»„åˆ
            'intervalsliding_stb_trows_agg': self.get_intervalsliding_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'intervalsliding_stb_partition_by_tbname_trows_agg': self.get_intervalsliding_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'intervalsliding_stb_partition_by_tag_trows_agg': self.get_intervalsliding_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'intervalsliding_tb_trows_agg': self.get_intervalsliding_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # sliding çª—å£ - æ‰€æœ‰ç»„åˆ
            'sliding_stb_trows_agg': self.get_sliding_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'sliding_stb_partition_by_tbname_trows_agg': self.get_sliding_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'sliding_stb_partition_by_tag_trows_agg': self.get_sliding_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'sliding_tb_trows_agg': self.get_sliding_template(source_type='tb', partition_type='none', **fixed_kwargs),
                     
            # ä¼šè¯çª—å£ - æ‰€æœ‰ç»„åˆ
            'session_stb_trows_agg': self.get_session_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'session_stb_partition_by_tbname_trows_agg': self.get_session_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'session_stb_partition_by_tag_trows_agg': self.get_session_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'session_tb_trows_agg': self.get_session_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # è®¡æ•°çª—å£ - æ‰€æœ‰ç»„åˆ
            'count_stb_trows_agg': self.get_count_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'count_stb_partition_by_tbname_trows_agg': self.get_count_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'count_stb_partition_by_tag_trows_agg': self.get_count_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'count_tb_trows_agg': self.get_count_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # äº‹ä»¶çª—å£ - æ‰€æœ‰ç»„åˆ
            'event_stb_trows_agg': self.get_event_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'event_stb_partition_by_tbname_trows_agg': self.get_event_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'event_stb_partition_by_tag_trows_agg': self.get_event_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'event_tb_trows_agg': self.get_event_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # çŠ¶æ€çª—å£ - æ‰€æœ‰ç»„åˆ
            'state_stb_trows_agg': self.get_state_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'state_stb_partition_by_tbname_trows_agg': self.get_state_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'state_stb_partition_by_tag_trows_agg': self.get_state_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'state_tb_trows_agg': self.get_state_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # å®šæ—¶è§¦å‘ - æ‰€æœ‰ç»„åˆ (æ³¨æ„: periodä¸æ”¯æŒtrowsï¼Œæ‰€ä»¥è¿™é‡Œå®é™…ä¸Šä»ç„¶ä½¿ç”¨tbname)
            'period_stb_trows_agg': self.get_period_template(source_type='stb', partition_type='none', **{k: v for k, v in fixed_kwargs.items() if k != 'tbname_or_trows_or_sourcetable'}),
            'period_stb_partition_by_tbname_trows_agg': self.get_period_template(source_type='stb', partition_type='tbname', **{k: v for k, v in fixed_kwargs.items() if k != 'tbname_or_trows_or_sourcetable'}),
            'period_stb_partition_by_tag_trows_agg': self.get_period_template(source_type='stb', partition_type='tag', **{k: v for k, v in fixed_kwargs.items() if k != 'tbname_or_trows_or_sourcetable'}),
            'period_tb_trows_agg': self.get_period_template(source_type='tb', partition_type='none', **{k: v for k, v in fixed_kwargs.items() if k != 'tbname_or_trows_or_sourcetable'}),
        }

    def get_trows_select_group(self, **kwargs):
        """è·å– trows + select ç»„åˆçš„æ‰€æœ‰çª—å£ç±»å‹
        
        å›ºå®šå‚æ•°: tbname_or_trows_or_sourcetable='trows', agg_or_select='select'
        å¿½ç•¥å‘½ä»¤è¡Œä¼ å…¥çš„è¿™ä¸¤ä¸ªå‚æ•°
        """
        # å›ºå®šå‚æ•°ï¼Œå¿½ç•¥ä¼ å…¥çš„å‚æ•°
        fixed_kwargs = {k: v for k, v in kwargs.items() if k not in ['tbname_or_trows_or_sourcetable', 'agg_or_select']}
        fixed_kwargs.update({
            'tbname_or_trows_or_sourcetable': 'trows',
            'agg_or_select': 'select'
        })
        
        return {
            # intervalsliding çª—å£ - æ‰€æœ‰ç»„åˆ
            'intervalsliding_stb_trows_select': self.get_intervalsliding_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'intervalsliding_stb_partition_by_tbname_trows_select': self.get_intervalsliding_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'intervalsliding_stb_partition_by_tag_trows_select': self.get_intervalsliding_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'intervalsliding_tb_trows_select': self.get_intervalsliding_template(source_type='tb', partition_type='none', **fixed_kwargs),
             
            # sliding çª—å£ - æ‰€æœ‰ç»„åˆ
            'sliding_stb_trows_select': self.get_sliding_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'sliding_stb_partition_by_tbname_trows_select': self.get_sliding_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'sliding_stb_partition_by_tag_trows_select': self.get_sliding_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'sliding_tb_trows_select': self.get_sliding_template(source_type='tb', partition_type='none', **fixed_kwargs),
                       
            # ä¼šè¯çª—å£ - æ‰€æœ‰ç»„åˆ
            'session_stb_trows_select': self.get_session_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'session_stb_partition_by_tbname_trows_select': self.get_session_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'session_stb_partition_by_tag_trows_select': self.get_session_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'session_tb_trows_select': self.get_session_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # è®¡æ•°çª—å£ - æ‰€æœ‰ç»„åˆ
            'count_stb_trows_select': self.get_count_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'count_stb_partition_by_tbname_trows_select': self.get_count_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'count_stb_partition_by_tag_trows_select': self.get_count_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'count_tb_trows_select': self.get_count_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # äº‹ä»¶çª—å£ - æ‰€æœ‰ç»„åˆ
            'event_stb_trows_select': self.get_event_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'event_stb_partition_by_tbname_trows_select': self.get_event_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'event_stb_partition_by_tag_trows_select': self.get_event_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'event_tb_trows_select': self.get_event_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # çŠ¶æ€çª—å£ - æ‰€æœ‰ç»„åˆ
            'state_stb_trows_select': self.get_state_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'state_stb_partition_by_tbname_trows_select': self.get_state_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'state_stb_partition_by_tag_trows_select': self.get_state_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'state_tb_trows_select': self.get_state_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # å®šæ—¶è§¦å‘ - æ‰€æœ‰ç»„åˆ (æ³¨æ„: periodä¸æ”¯æŒtrowsï¼Œæ‰€ä»¥è¿™é‡Œå®é™…ä¸Šä»ç„¶ä½¿ç”¨tbname)
            'period_stb_trows_select': self.get_period_template(source_type='stb', partition_type='none', **{k: v for k, v in fixed_kwargs.items() if k != 'tbname_or_trows_or_sourcetable'}),
            'period_stb_partition_by_tbname_trows_select': self.get_period_template(source_type='stb', partition_type='tbname', **{k: v for k, v in fixed_kwargs.items() if k != 'tbname_or_trows_or_sourcetable'}),
            'period_stb_partition_by_tag_trows_select': self.get_period_template(source_type='stb', partition_type='tag', **{k: v for k, v in fixed_kwargs.items() if k != 'tbname_or_trows_or_sourcetable'}),
            'period_tb_trows_select': self.get_period_template(source_type='tb', partition_type='none', **{k: v for k, v in fixed_kwargs.items() if k != 'tbname_or_trows_or_sourcetable'}),
        }
        
    def get_sourcetable_agg_group(self, **kwargs):
        """è·å– sourcetable + agg ç»„åˆçš„æ‰€æœ‰çª—å£ç±»å‹
        
        å›ºå®šå‚æ•°: tbname_or_trows_or_sourcetable='sourcetable', agg_or_select='agg'
        å¿½ç•¥å‘½ä»¤è¡Œä¼ å…¥çš„è¿™ä¸¤ä¸ªå‚æ•°
        """
        # å›ºå®šå‚æ•°ï¼Œå¿½ç•¥ä¼ å…¥çš„å‚æ•°
        fixed_kwargs = {k: v for k, v in kwargs.items() if k not in ['tbname_or_trows_or_sourcetable', 'agg_or_select']}
        fixed_kwargs.update({
            'tbname_or_trows_or_sourcetable': 'sourcetable',
            'agg_or_select': 'agg'
        })
        
        return {
            # intervalsliding çª—å£ - æ‰€æœ‰ç»„åˆ
            'intervalsliding_stb_sourcetable_agg': self.get_intervalsliding_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'intervalsliding_stb_partition_by_tbname_sourcetable_agg': self.get_intervalsliding_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'intervalsliding_stb_partition_by_tag_sourcetable_agg': self.get_intervalsliding_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'intervalsliding_tb_sourcetable_agg': self.get_intervalsliding_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # sliding çª—å£ - æ‰€æœ‰ç»„åˆ
            'sliding_stb_sourcetable_agg': self.get_sliding_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'sliding_stb_partition_by_tbname_sourcetable_agg': self.get_sliding_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'sliding_stb_partition_by_tag_sourcetable_agg': self.get_sliding_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'sliding_tb_sourcetable_agg': self.get_sliding_template(source_type='tb', partition_type='none', **fixed_kwargs),
                        
            # ä¼šè¯çª—å£ - æ‰€æœ‰ç»„åˆ
            'session_stb_sourcetable_agg': self.get_session_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'session_stb_partition_by_tbname_sourcetable_agg': self.get_session_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'session_stb_partition_by_tag_sourcetable_agg': self.get_session_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'session_tb_sourcetable_agg': self.get_session_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # è®¡æ•°çª—å£ - æ‰€æœ‰ç»„åˆ
            'count_stb_sourcetable_agg': self.get_count_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'count_stb_partition_by_tbname_sourcetable_agg': self.get_count_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'count_stb_partition_by_tag_sourcetable_agg': self.get_count_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'count_tb_sourcetable_agg': self.get_count_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # äº‹ä»¶çª—å£ - æ‰€æœ‰ç»„åˆ
            'event_stb_sourcetable_agg': self.get_event_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'event_stb_partition_by_tbname_sourcetable_agg': self.get_event_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'event_stb_partition_by_tag_sourcetable_agg': self.get_event_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'event_tb_sourcetable_agg': self.get_event_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # çŠ¶æ€çª—å£ - æ‰€æœ‰ç»„åˆ
            'state_stb_sourcetable_agg': self.get_state_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'state_stb_partition_by_tbname_sourcetable_agg': self.get_state_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'state_stb_partition_by_tag_sourcetable_agg': self.get_state_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'state_tb_sourcetable_agg': self.get_state_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # å®šæ—¶è§¦å‘ - æ‰€æœ‰ç»„åˆ (æ³¨æ„: periodæ”¯æŒsourcetable)
            'period_stb_sourcetable_agg': self.get_period_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'period_stb_partition_by_tbname_sourcetable_agg': self.get_period_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'period_stb_partition_by_tag_sourcetable_agg': self.get_period_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'period_tb_sourcetable_agg': self.get_period_template(source_type='tb', partition_type='none', **fixed_kwargs),
        }


    def get_sourcetable_select_group(self, **kwargs):
        """è·å– sourcetable + select ç»„åˆçš„æ‰€æœ‰çª—å£ç±»å‹
        
        å›ºå®šå‚æ•°: tbname_or_trows_or_sourcetable='sourcetable', agg_or_select='select'
        å¿½ç•¥å‘½ä»¤è¡Œä¼ å…¥çš„è¿™ä¸¤ä¸ªå‚æ•°
        """
        # å›ºå®šå‚æ•°ï¼Œå¿½ç•¥ä¼ å…¥çš„å‚æ•°
        fixed_kwargs = {k: v for k, v in kwargs.items() if k not in ['tbname_or_trows_or_sourcetable', 'agg_or_select']}
        fixed_kwargs.update({
            'tbname_or_trows_or_sourcetable': 'sourcetable',
            'agg_or_select': 'select'
        })
        
        return {
            # intervalsliding çª—å£ - æ‰€æœ‰ç»„åˆ
            'intervalsliding_stb_sourcetable_select': self.get_intervalsliding_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'intervalsliding_stb_partition_by_tbname_sourcetable_select': self.get_intervalsliding_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'intervalsliding_stb_partition_by_tag_sourcetable_select': self.get_intervalsliding_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'intervalsliding_tb_sourcetable_select': self.get_intervalsliding_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # sliding çª—å£ - æ‰€æœ‰ç»„åˆ
            'sliding_stb_sourcetable_select': self.get_sliding_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'sliding_stb_partition_by_tbname_sourcetable_select': self.get_sliding_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'sliding_stb_partition_by_tag_sourcetable_select': self.get_sliding_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'sliding_tb_sourcetable_select': self.get_sliding_template(source_type='tb', partition_type='none', **fixed_kwargs),
                        
            # ä¼šè¯çª—å£ - æ‰€æœ‰ç»„åˆ
            'session_stb_sourcetable_select': self.get_session_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'session_stb_partition_by_tbname_sourcetable_select': self.get_session_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'session_stb_partition_by_tag_sourcetable_select': self.get_session_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'session_tb_sourcetable_select': self.get_session_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # è®¡æ•°çª—å£ - æ‰€æœ‰ç»„åˆ
            'count_stb_sourcetable_select': self.get_count_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'count_stb_partition_by_tbname_sourcetable_select': self.get_count_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'count_stb_partition_by_tag_sourcetable_select': self.get_count_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'count_tb_sourcetable_select': self.get_count_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # äº‹ä»¶çª—å£ - æ‰€æœ‰ç»„åˆ
            'event_stb_sourcetable_select': self.get_event_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'event_stb_partition_by_tbname_sourcetable_select': self.get_event_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'event_stb_partition_by_tag_sourcetable_select': self.get_event_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'event_tb_sourcetable_select': self.get_event_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # çŠ¶æ€çª—å£ - æ‰€æœ‰ç»„åˆ
            'state_stb_sourcetable_select': self.get_state_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'state_stb_partition_by_tbname_sourcetable_select': self.get_state_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'state_stb_partition_by_tag_sourcetable_select': self.get_state_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'state_tb_sourcetable_select': self.get_state_template(source_type='tb', partition_type='none', **fixed_kwargs),
            
            # å®šæ—¶è§¦å‘ - æ‰€æœ‰ç»„åˆ (æ³¨æ„: periodæ”¯æŒsourcetable)
            'period_stb_sourcetable_select': self.get_period_template(source_type='stb', partition_type='none', **fixed_kwargs),
            'period_stb_partition_by_tbname_sourcetable_select': self.get_period_template(source_type='stb', partition_type='tbname', **fixed_kwargs),
            'period_stb_partition_by_tag_sourcetable_select': self.get_period_template(source_type='stb', partition_type='tag', **fixed_kwargs),
            'period_tb_sourcetable_select': self.get_period_template(source_type='tb', partition_type='none', **fixed_kwargs),
        }

        
    def generate_multiple_streams(self, base_sql, stream_num=1):
        """æ ¹æ®åŸºç¡€SQLæ¨¡æ¿ç”Ÿæˆå¤šä¸ªç¼–å·çš„æµ
        
        Args:
            base_sql: åŸºç¡€SQLæ¨¡æ¿
            stream_num: è¦ç”Ÿæˆçš„æµæ•°é‡
            
        Returns:
            dict: åŒ…å«å¤šä¸ªæµSQLçš„å­—å…¸ï¼Œkeyä¸ºæµåç§°ï¼Œvalueä¸ºSQL
        """
        if stream_num <= 1:
            return {'stream_1': base_sql}
        
        result = {}
        
        if isinstance(base_sql, str):
            for i in range(1, stream_num + 1):
                modified_sql = self._modify_sql_for_multiple_streams(base_sql, i)
                result[f'stream_{i}'] = modified_sql
        else:
            # å¦‚æœéœ€è¦é‡æ–°ç”Ÿæˆï¼ˆç”¨äºæ”¯æŒä¸åŒå­è¡¨çš„æƒ…å†µï¼‰
            # è¿™ç§æƒ…å†µä¸‹base_sqlåº”è¯¥åŒ…å«ç”Ÿæˆå‡½æ•°å’Œå‚æ•°ä¿¡æ¯
            # æš‚æ—¶ä¿æŒåŸæœ‰é€»è¾‘ï¼Œè¿™ä¸ªåˆ†æ”¯ä¸»è¦ç”¨äºå‘åå…¼å®¹
            for i in range(1, stream_num + 1):
                modified_sql = self._modify_sql_for_multiple_streams(base_sql, i)
                result[f'stream_{i}'] = modified_sql
            
        return result

    def _modify_sql_for_multiple_streams(self, sql, stream_index):
        """ä¿®æ”¹SQLä»¥æ”¯æŒå¤šæµåˆ›å»º
        
        Args:
            sql: åŸå§‹SQL
            stream_index: æµç¼–å·
            
        Returns:
            str: ä¿®æ”¹åçš„SQL
        """
        import re
        
        # 1. ä¿®æ”¹æµåç§°ï¼Œæ·»åŠ ç¼–å·åç¼€
        # åŒ¹é… "create stream xxx" æ¨¡å¼
        sql = re.sub(
            r'(create\s+stream\s+)([^\s]+)',
            rf'\1\2_{stream_index}',
            sql,
            flags=re.IGNORECASE
        )
        
        # 2. ä¿®æ”¹ç›®æ ‡è¡¨åç§°ï¼Œæ·»åŠ ç¼–å·åç¼€
        # åŒ¹é… "into stream_to.xxx" æ¨¡å¼
        sql = re.sub(
            r'(into\s+)(stream_to\.)([^\s]+)',
            rf'\1\2\3_{stream_index}',
            sql,
            flags=re.IGNORECASE
        )
        
        return sql
        
    '''                    
    # ========== oldæµæ¨¡æ¿ï¼Œä»ç„¶æ”¯æŒï¼Œä½œä¸ºç¬¬ä¸€è½®çš„æ‘¸åº•æµ‹è¯•ï¼Œä½†ä¸æ¨èä½¿ç”¨äº†ï¼Œä½¿ç”¨ä¸Šé¢çš„å»ºæµæ¨¡ç‰ˆè¿›è¡Œç¬¬äºŒè½®æµ‹è¯• (s2_2 åˆ° s2_6) ==========
    def get_s2_2(self, agg_or_select='agg', custom_columns=None):
        """trigger at_once"""
        columns = self._build_columns(agg_or_select, custom_columns)
        return f"""
    create stream s2_2 trigger at_once 
        ignore expired 0 ignore update 0 into stream_to.stb2_2 
        as select _wstart as wstart,
        {columns}
        from stream_from.stb partition by tbname interval(15s);
    """
    
    def get_s2_3(self, agg_or_select='agg', custom_columns=None):
        """trigger window_close"""
        columns = self._build_columns(agg_or_select, custom_columns)
        return f"""
    create stream s2_3 trigger window_close 
        ignore expired 0 ignore update 0 into stream_to.stb2_3 
        as select _wstart as wstart,
        {columns}
        from stream_from.stb partition by tbname interval(15s);
    """
    
    def get_s2_4(self, agg_or_select='agg', custom_columns=None):
        """trigger max_delay 5s"""
        columns = self._build_columns(agg_or_select, custom_columns)
        return f"""
    create stream s2_4 trigger max_delay 5s 
        ignore expired 0 ignore update 0 into stream_to.stb2_4 
        as select _wstart as wstart,
        {columns}
        from stream_from.stb partition by tbname interval(15s);
    """
    
    def get_s2_5(self, agg_or_select='agg', custom_columns=None):
        """trigger FORCE_WINDOW_CLOSE"""
        columns = self._build_columns(agg_or_select, custom_columns)
        return f"""
    create stream s2_5 trigger FORCE_WINDOW_CLOSE into stream_to.stb2_5 
        as select _wstart as wstart,
        {columns}
        from stream_from.stb partition by tbname interval(15s);
    """
    
    def get_s2_6(self, agg_or_select='agg', custom_columns=None):
        """trigger CONTINUOUS_WINDOW_CLOSE"""
        columns = self._build_columns(agg_or_select, custom_columns)
        return f"""
    create stream s2_6 trigger CONTINUOUS_WINDOW_CLOSE 
        ignore expired 0 ignore update 0 into stream_to.stb2_6 
        as select _wstart as wstart,
        {columns}
        from stream_from.stb partition by tbname interval(15s);
    """
    
    # ========== newæµæ¨¡æ¿ï¼Œä»ç„¶æ”¯æŒï¼Œä½œä¸ºç¬¬ä¸€è½®çš„æ‘¸åº•æµ‹è¯•ï¼Œä½†ä¸æ¨èä½¿ç”¨äº†ï¼Œä½¿ç”¨ä¸Šé¢çš„å»ºæµæ¨¡ç‰ˆè¿›è¡Œç¬¬äºŒè½®æµ‹è¯•  (s2_7 åˆ° s2_15) ==========
    def get_s2_7(self, agg_or_select='agg', tbname_or_trows_or_sourcetable='tbname', custom_columns=None):
        """INTERVAL(15s) çª—å£"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows_or_sourcetable)
        return f"""
    create stream stream_from.s2_7 INTERVAL(15s) SLIDING(15s)
            from stream_from.stb 
            partition by tbname 
            into stream_to.stb2_7
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_s2_8(self, agg_or_select='agg', tbname_or_trows_or_sourcetable='trows', custom_columns=None):
        """INTERVAL with %%trows"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows_or_sourcetable)
        return f"""
    create stream stream_from.s2_8 INTERVAL(15s) SLIDING(15s)
            from stream_from.stb 
            partition by tbname 
            into stream_to.stb2_8
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_s2_9(self, agg_or_select='agg', tbname_or_trows_or_sourcetable='tbname', custom_columns=None):
        """INTERVAL with MAX_DELAY"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows_or_sourcetable)
        return f"""
    create stream stream_from.s2_9 INTERVAL(15s) SLIDING(15s)
            from stream_from.stb partition by tbname 
            STREAM_OPTIONS(MAX_DELAY(5s)) 
            into stream_to.stb2_9
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_s2_10(self, agg_or_select='agg', tbname_or_trows_or_sourcetable='trows', custom_columns=None):
        """INTERVAL with MAX_DELAY and %%trows"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows_or_sourcetable)
        return f"""
    create stream stream_from.s2_10 INTERVAL(15s) SLIDING(15s) 
            from stream_from.stb 
            STREAM_OPTIONS(MAX_DELAY(5s))
            into stream_to.stb2_10
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_s2_11(self, agg_or_select='agg', custom_columns=None):
        """PERIOD(15s) å®šæ—¶è§¦å‘"""
        columns = self._build_columns(agg_or_select, custom_columns)
        return f"""
    create stream stream_from.s2_11 period(15s) 
            from stream_from.stb partition by tbname  
            into stream_to.stb2_11
            as select cast(_tlocaltime/1000000 as timestamp) ts, {columns}
            from %%tbname;
    """
    
    def get_s2_12(self, agg_or_select='agg', tbname_or_trows_or_sourcetable='tbname', custom_columns=None):
        """SESSION(ts,10a) ä¼šè¯çª—å£"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows_or_sourcetable)
        return f"""
    create stream stream_from.s2_12 session(ts,10a)
            from stream_from.stb partition by tbname 
            into stream_to.stb2_12
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_s2_13(self, agg_or_select='agg', tbname_or_trows_or_sourcetable='tbname', custom_columns=None):
        """COUNT_WINDOW(1000) è®¡æ•°çª—å£"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows_or_sourcetable)
        return f"""
    create stream stream_from.s2_13 COUNT_WINDOW(1000)
            from stream_from.stb partition by tbname 
            into stream_to.stb2_13
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_s2_14(self, agg_or_select='agg', tbname_or_trows_or_sourcetable='tbname', custom_columns=None):
        """EVENT_WINDOW äº‹ä»¶çª—å£"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows_or_sourcetable)
        return f"""
    create stream stream_from.s2_14 EVENT_WINDOW(START WITH c0 > -10000000 END WITH c0 < 10000000) 
            from stream_from.stb partition by tbname 
            into stream_to.stb2_14
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    def get_s2_15(self, agg_or_select='agg', tbname_or_trows_or_sourcetable='tbname', custom_columns=None):
        """STATE_WINDOW çŠ¶æ€çª—å£"""
        columns = self._build_columns(agg_or_select, custom_columns)
        from_clause = self._build_from_clause(tbname_or_trows_or_sourcetable)
        return f"""
    create stream stream_from.s2_15 STATE_WINDOW(c0) 
            from stream_from.stb partition by tbname 
            into stream_to.stb2_15
            as select _twstart ts, {columns}
            from {from_clause};
    """
    
    # ========== åˆ†ç»„å’Œæ‰¹é‡è·å–æ–¹æ³• ==========
    def get_intervalsliding_group(self, **kwargs):
        """è·å–æ»‘åŠ¨çª—å£ç»„ (s2_7, s2_8, s2_9, s2_10)"""
        return {
            's2_7': self.get_s2_7(**kwargs),
            's2_8': self.get_s2_8(**kwargs),
            's2_9': self.get_s2_9(**kwargs),
            's2_10': self.get_s2_10(**kwargs)
        }
    
    def get_count_group(self, **kwargs):
        """è·å–è®¡æ•°çª—å£ç»„ (s2_13)"""
        return {
            's2_13': self.get_s2_13(**kwargs)
        }
    
    def get_session_group(self, **kwargs):
        """è·å–ä¼šè¯çª—å£ç»„ (s2_12)"""
        return {
            's2_12': self.get_s2_12(**kwargs)
        }
    
    def get_period_group(self, **kwargs):
        """è·å–å®šæ—¶è§¦å‘ç»„ (s2_11)"""
        # period ç±»å‹ä¸æ”¯æŒ tbname_or_trows_or_sourcetable å‚æ•°
        filtered_kwargs = {k: v for k, v in kwargs.items() if k != 'tbname_or_trows_or_sourcetable'}
        return {
            's2_11': self.get_s2_11(**filtered_kwargs)
        }
    
    def get_event_group(self, **kwargs):
        """è·å–äº‹ä»¶çª—å£ç»„ (s2_14)"""
        return {
            's2_14': self.get_s2_14(**kwargs)
        }
    
    def get_state_group(self, **kwargs):
        """è·å–çŠ¶æ€çª—å£ç»„ (s2_15)"""
        return {
            's2_15': self.get_s2_15(**kwargs)
        }
    
    def get_basic_group(self, **kwargs):
        """è·å–åŸºç¡€è§¦å‘ç»„ (s2_2 åˆ° s2_6)"""
        # åŸºç¡€ç»„ä¸æ”¯æŒ tbname_or_trows_or_sourcetable å‚æ•°
        filtered_kwargs = {k: v for k, v in kwargs.items() if k != 'tbname_or_trows_or_sourcetable'}
        return {
            's2_2': self.get_s2_2(**filtered_kwargs),
            's2_3': self.get_s2_3(**filtered_kwargs),
            's2_4': self.get_s2_4(**filtered_kwargs),
            's2_5': self.get_s2_5(**filtered_kwargs),
            's2_6': self.get_s2_6(**filtered_kwargs)
        }
    
    def get_all_advanced(self, **kwargs):
        """è·å–æ‰€æœ‰é«˜çº§æµ (s2_7 åˆ° s2_15)"""
        result = {}
        result.update(self.get_intervalsliding_group(**kwargs))
        result.update(self.get_period_group(**kwargs))
        result.update(self.get_session_group(**kwargs))
        result.update(self.get_count_group(**kwargs))
        result.update(self.get_event_group(**kwargs))
        result.update(self.get_state_group(**kwargs))
        return result
    
    def get_all(self, **kwargs):
        """è·å–æ‰€æœ‰æµæ¨¡æ¿"""
        result = {}
        result.update(self.get_basic_group(**kwargs))
        result.update(self.get_all_advanced(**kwargs))
        return result
    
    # @classmethod
    # def get_sql_bak(cls, sql_type, **kwargs):
    #     """
    #     è·å–æŒ‡å®šç±»å‹çš„ SQL æ¨¡æ¿ - ä¸»è¦å…¥å£æ–¹æ³•
        
    #     Args:
    #         sql_type: SQL ç±»å‹æ ‡è¯†ç¬¦æˆ–åˆ†ç»„å
    #         **kwargs: å¯é€‰å‚æ•°
    #             - agg_or_select: 'agg'(é»˜è®¤) æˆ– 'select'ï¼Œæ§åˆ¶èšåˆè¿˜æ˜¯æŠ•å½±æŸ¥è¯¢
    #             - tbname_or_trows_or_sourcetable: 'tbname'(é»˜è®¤) æˆ– 'trows'ï¼Œæ§åˆ¶FROMå­å¥
    #             - custom_columns: è‡ªå®šä¹‰åˆ—ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨è‡ªå®šä¹‰åˆ—
                
    #     Returns:
    #         str or dict: å•ä¸ªSQLå­—ç¬¦ä¸²æˆ–SQLå­—å…¸
            
    #     Usage Examples:
    #         # è·å–å•ä¸ªæ¨¡æ¿
    #         sql = StreamSQLTemplates.get_sql('s2_7')
            
    #         # ä½¿ç”¨æŠ•å½±æŸ¥è¯¢
    #         sql = StreamSQLTemplates.get_sql('s2_7', agg_or_select='select')
            
    #         # ä½¿ç”¨trows
    #         sql = StreamSQLTemplates.get_sql('s2_8', tbname_or_trows_or_sourcetable='trows')
            
    #         # è‡ªå®šä¹‰åˆ—
    #         sql = StreamSQLTemplates.get_sql('s2_7', custom_columns=['sum(c0)', 'count(*)'])
            
    #         # è·å–åˆ†ç»„
    #         sqls = StreamSQLTemplates.get_sql('intervalsliding')  # æ»‘åŠ¨çª—å£ç»„
    #         sqls = StreamSQLTemplates.get_sql('all')      # æ‰€æœ‰æ¨¡æ¿
    #     """
    #     instance = cls()
        
    #     # å•ä¸ªæ¨¡æ¿æ˜ å°„
    #     single_templates = {
    #         's2_2': instance.get_s2_2,
    #         's2_3': instance.get_s2_3,
    #         's2_4': instance.get_s2_4,
    #         's2_5': instance.get_s2_5,
    #         's2_6': instance.get_s2_6,
    #         's2_7': instance.get_s2_7,
    #         's2_8': instance.get_s2_8,
    #         's2_9': instance.get_s2_9,
    #         's2_10': instance.get_s2_10,
    #         's2_11': instance.get_s2_11,
    #         's2_12': instance.get_s2_12,
    #         's2_13': instance.get_s2_13,
    #         's2_14': instance.get_s2_14,
    #         's2_15': instance.get_s2_15,
    #     }
        
    #     # åˆ†ç»„æ¨¡æ¿æ˜ å°„
    #     group_templates = {
    #         'basic': instance.get_basic_group,
    #         'intervalsliding': instance.get_intervalsliding_group,
    #         'count': instance.get_count_group,
    #         'session': instance.get_session_group,
    #         'period': instance.get_period_group,
    #         'event': instance.get_event_group,
    #         'state': instance.get_state_group,
    #         'advanced': instance.get_all_advanced,
    #         'all': instance.get_all,
    #     }
        
    #     # å¤„ç†å•ä¸ªæ¨¡æ¿
    #     if sql_type in single_templates:
    #         method = single_templates[sql_type]
    #         # ä¸ºä¸åŒç±»å‹çš„æ¨¡æ¿è¿‡æ»¤å‚æ•°
    #         if sql_type in ['s2_2', 's2_3', 's2_4', 's2_5', 's2_6', 's2_11']:
    #             # åŸºç¡€æ¨¡æ¿å’Œperiodæ¨¡æ¿ä¸æ”¯æŒ tbname_or_trows_or_sourcetable
    #             filtered_kwargs = {k: v for k, v in kwargs.items() if k != 'tbname_or_trows_or_sourcetable'}
    #             return method(**filtered_kwargs)
    #         else:
    #             return method(**kwargs)
        
    #     # å¤„ç†åˆ†ç»„æ¨¡æ¿
    #     if sql_type in group_templates:
    #         return group_templates[sql_type](**kwargs)
        
    #     # é»˜è®¤è¿”å› s2_7
    #     print(f"è­¦å‘Š: æœªæ‰¾åˆ°æ¨¡æ¿ '{sql_type}'ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿ 's2_7'")
    #     return instance.get_s2_7(**kwargs)
    
    '''
    
    @classmethod
    def get_sql(cls, sql_type, stream_num=1, auto_combine=False, **kwargs):
        """
        è·å–æŒ‡å®šç±»å‹çš„ SQL æ¨¡æ¿ - ä¸»è¦å…¥å£æ–¹æ³•
        
        Args:
            sql_type: SQL ç±»å‹æ ‡è¯†ç¬¦æˆ–åˆ†ç»„å
                å•ä¸ªæ¨¡æ¿: 'intervalsliding_stb', 'intervalsliding_stb_partition_by_tbname' ç­‰
                ç»„åˆæ¨¡æ¿: 'intervalsliding_detailed', 'session_detailed' ç­‰
            stream_num: æµæ•°é‡ï¼ˆä»…å¯¹å•ä¸ªæµç±»å‹æœ‰æ•ˆï¼‰
            auto_combine: æ˜¯å¦è‡ªåŠ¨ç”Ÿæˆ6ç§å‚æ•°ç»„åˆ
            **kwargs: å¯é€‰å‚æ•°
                
        Returns:
            str or dict: å•ä¸ªSQLå­—ç¬¦ä¸²æˆ–SQLå­—å…¸
            
        Usage Examples:
            # è·å–å•ä¸ªè¯¦ç»†æ¨¡æ¿
            sql = StreamSQLTemplates.get_sql('intervalsliding_stb_partition_by_tbname')
            
            # è·å–è¯¦ç»†ç»„åˆ
            sqls = StreamSQLTemplates.get_sql('intervalsliding_detailed')
            
            # è·å–æ‰€æœ‰çª—å£ç±»å‹çš„è¯¦ç»†ç»„åˆ
            sqls = StreamSQLTemplates.get_sql('all_detailed')
        """
        instance = cls()
        
        # å¤„ç†ç‰¹æ®Šçš„æŸ¥è¯¢è¯­å¥
        if sql_type == 'select_stream':
            return instance.get_select_stream(**kwargs)
        
        if sql_type == 'tbname_agg':
            if stream_num > 1:
                print("è­¦å‘Š: å›ºå®šå‚æ•°ç»„åˆæ¨¡æ¿ä¸æ”¯æŒ --stream-num å‚æ•°ï¼Œå°†å¿½ç•¥è¯¥å‚æ•°")
            return instance.get_tbname_agg_group(**kwargs)
        elif sql_type == 'tbname_select':
            if stream_num > 1:
                print("è­¦å‘Š: å›ºå®šå‚æ•°ç»„åˆæ¨¡æ¿ä¸æ”¯æŒ --stream-num å‚æ•°ï¼Œå°†å¿½ç•¥è¯¥å‚æ•°")
            return instance.get_tbname_select_group(**kwargs)
        elif sql_type == 'trows_agg':
            if stream_num > 1:
                print("è­¦å‘Š: å›ºå®šå‚æ•°ç»„åˆæ¨¡æ¿ä¸æ”¯æŒ --stream-num å‚æ•°ï¼Œå°†å¿½ç•¥è¯¥å‚æ•°")
            return instance.get_trows_agg_group(**kwargs)
        elif sql_type == 'trows_select':
            if stream_num > 1:
                print("è­¦å‘Š: å›ºå®šå‚æ•°ç»„åˆæ¨¡æ¿ä¸æ”¯æŒ --stream-num å‚æ•°ï¼Œå°†å¿½ç•¥è¯¥å‚æ•°")
            return instance.get_trows_select_group(**kwargs)
        elif sql_type == 'sourcetable_agg':
            if stream_num > 1:
                print("è­¦å‘Š: å›ºå®šå‚æ•°ç»„åˆæ¨¡æ¿ä¸æ”¯æŒ --stream-num å‚æ•°ï¼Œå°†å¿½ç•¥è¯¥å‚æ•°")
            return instance.get_sourcetable_agg_group(**kwargs)
        elif sql_type == 'sourcetable_select':
            if stream_num > 1:
                print("è­¦å‘Š: å›ºå®šå‚æ•°ç»„åˆæ¨¡æ¿ä¸æ”¯æŒ --stream-num å‚æ•°ï¼Œå°†å¿½ç•¥è¯¥å‚æ•°")
            return instance.get_sourcetable_select_group(**kwargs)
        
        # å•ä¸ªè¯¦ç»†æ¨¡æ¿æ˜ å°„
        detailed_templates = {
            'intervalsliding_stb': lambda stream_index=None, **kw: instance.get_intervalsliding_template(source_type='stb', partition_type='none', stream_index=stream_index, **kw),
            'intervalsliding_stb_partition_by_tbname': lambda stream_index=None, **kw: instance.get_intervalsliding_template(source_type='stb', partition_type='tbname', stream_index=stream_index, **kw),
            'intervalsliding_stb_partition_by_tag': lambda stream_index=None, **kw: instance.get_intervalsliding_template(source_type='stb', partition_type='tag', stream_index=stream_index, **kw),
            'intervalsliding_tb': lambda stream_index=None, **kw: instance.get_intervalsliding_template(source_type='tb', partition_type='none', stream_index=stream_index, **kw),
            
            'sliding_stb': lambda stream_index=None, **kw: instance.get_sliding_template(source_type='stb', partition_type='none', stream_index=stream_index, **kw),
            'sliding_stb_partition_by_tbname': lambda stream_index=None, **kw: instance.get_sliding_template(source_type='stb', partition_type='tbname', stream_index=stream_index, **kw),
            'sliding_stb_partition_by_tag': lambda stream_index=None, **kw: instance.get_sliding_template(source_type='stb', partition_type='tag', stream_index=stream_index, **kw),
            'sliding_tb': lambda stream_index=None, **kw: instance.get_sliding_template(source_type='tb', partition_type='none', stream_index=stream_index, **kw),
                       
            'session_stb': lambda stream_index=None, **kw: instance.get_session_template(source_type='stb', partition_type='none', stream_index=stream_index, **kw),
            'session_stb_partition_by_tbname': lambda stream_index=None, **kw: instance.get_session_template(source_type='stb', partition_type='tbname', stream_index=stream_index, **kw),
            'session_stb_partition_by_tag': lambda stream_index=None, **kw: instance.get_session_template(source_type='stb', partition_type='tag', stream_index=stream_index, **kw),
            'session_tb': lambda stream_index=None, **kw: instance.get_session_template(source_type='tb', partition_type='none', stream_index=stream_index, **kw),
            
            'count_stb': lambda stream_index=None, **kw: instance.get_count_template(source_type='stb', partition_type='none', stream_index=stream_index, **kw),
            'count_stb_partition_by_tbname': lambda stream_index=None, **kw: instance.get_count_template(source_type='stb', partition_type='tbname', stream_index=stream_index, **kw),
            'count_stb_partition_by_tag': lambda stream_index=None, **kw: instance.get_count_template(source_type='stb', partition_type='tag', stream_index=stream_index, **kw),
            'count_tb': lambda stream_index=None, **kw: instance.get_count_template(source_type='tb', partition_type='none', stream_index=stream_index, **kw),
            
            'event_stb': lambda stream_index=None, **kw: instance.get_event_template(source_type='stb', partition_type='none', stream_index=stream_index, **kw),
            'event_stb_partition_by_tbname': lambda stream_index=None, **kw: instance.get_event_template(source_type='stb', partition_type='tbname', stream_index=stream_index, **kw),
            'event_stb_partition_by_tag': lambda stream_index=None, **kw: instance.get_event_template(source_type='stb', partition_type='tag', stream_index=stream_index, **kw),
            'event_tb': lambda stream_index=None, **kw: instance.get_event_template(source_type='tb', partition_type='none', stream_index=stream_index, **kw),
            
            'state_stb': lambda stream_index=None, **kw: instance.get_state_template(source_type='stb', partition_type='none', stream_index=stream_index, **kw),
            'state_stb_partition_by_tbname': lambda stream_index=None, **kw: instance.get_state_template(source_type='stb', partition_type='tbname', stream_index=stream_index, **kw),
            'state_stb_partition_by_tag': lambda stream_index=None, **kw: instance.get_state_template(source_type='stb', partition_type='tag', stream_index=stream_index, **kw),
            'state_tb': lambda stream_index=None, **kw: instance.get_state_template(source_type='tb', partition_type='none', stream_index=stream_index, **kw),
            
            'period_stb': lambda stream_index=None, **kw: instance.get_period_template(source_type='stb', partition_type='none', stream_index=stream_index, **kw),
            'period_stb_partition_by_tbname': lambda stream_index=None, **kw: instance.get_period_template(source_type='stb', partition_type='tbname', stream_index=stream_index, **kw),
            'period_stb_partition_by_tag': lambda stream_index=None, **kw: instance.get_period_template(source_type='stb', partition_type='tag', stream_index=stream_index, **kw),
            'period_tb': lambda stream_index=None, **kw: instance.get_period_template(source_type='tb', partition_type='none', stream_index=stream_index, **kw),
        }
        
        # ç»„åˆæ¨¡æ¿æ˜ å°„
        group_templates = {
            'intervalsliding_detailed': instance.get_intervalsliding_group_detailed,
            'sliding_detailed': instance.get_sliding_group_detailed, 
            'session_detailed': instance.get_session_group_detailed,
            'count_detailed': instance.get_count_group_detailed,
            'event_detailed': instance.get_event_group_detailed,
            'state_detailed': instance.get_state_group_detailed,
            'period_detailed': instance.get_period_group_detailed,
        }
        
        # å¤„ç†å•ä¸ªè¯¦ç»†æ¨¡æ¿æ—¶
        if sql_type in detailed_templates:
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªåŠ¨ç»„åˆ
            if auto_combine:
                print(f"è‡ªåŠ¨ç”Ÿæˆ {sql_type} çš„6ç§å‚æ•°ç»„åˆ (å¿½ç•¥ --agg-or-select å’Œ --tbname-or-trows-or-sourcetable å‚æ•°)")
                
                if stream_num > 1:
                    print("è­¦å‘Š: --auto-combine æ¨¡å¼ä¸‹ --stream-num å‚æ•°æ— æ•ˆï¼Œå°†å¿½ç•¥")
                
                # ç”Ÿæˆ6ç§ç»„åˆ
                combinations = [
                    ('tbname', 'agg'),
                    ('tbname', 'select'),
                    ('trows', 'agg'),
                    ('trows', 'select'),
                    ('sourcetable', 'agg'),
                    ('sourcetable', 'select')
                ]
                
                result = {}
                for tbname_param, agg_param in combinations:
                    # åˆ›å»ºå‚æ•°å‰¯æœ¬ï¼Œè¦†ç›–æŒ‡å®šå‚æ•°
                    combo_kwargs = kwargs.copy()
                    combo_kwargs['tbname_or_trows_or_sourcetable'] = tbname_param
                    combo_kwargs['agg_or_select'] = agg_param
                    
                    # ç”Ÿæˆç»„åˆåç§°
                    combo_name = f"{sql_type}_{agg_param}_{tbname_param}"
                    
                    # ç”ŸæˆSQL
                    combo_sql = detailed_templates[sql_type](**combo_kwargs)
                    result[combo_name] = combo_sql
                    
                    print(f"  ç”Ÿæˆç»„åˆ: {combo_name}")
                
                return result
                
            else:
                # åŸæœ‰é€»è¾‘ï¼šæ ¹æ® stream_num å¤„ç†
                if stream_num > 1:
                    print(f"ç”Ÿæˆ {stream_num} ä¸ª {sql_type} ç±»å‹çš„æµ")
                    result = {}
                    
                    for i in range(1, stream_num + 1):
                        stream_sql = detailed_templates[sql_type](stream_index=i, **kwargs)
                        result[f'stream_{i}'] = stream_sql
                        
                    return result
                else:
                    return detailed_templates[sql_type](**kwargs)
        
        # å¤„ç†ç»„åˆæ¨¡æ¿ï¼ˆæ”¯æŒ stream_numï¼‰ï¼ˆä¸å— auto_combine å½±å“ï¼‰
        if sql_type in group_templates:
            if auto_combine:
                print("è­¦å‘Š: ç»„åˆæ¨¡æ¿ä¸æ”¯æŒ --auto-combine å‚æ•°ï¼Œå°†å¿½ç•¥è¯¥å‚æ•°")
                
            base_sqls = group_templates[sql_type](**kwargs)
            
            # å¦‚æœstream_num > 1ï¼Œä¸ºæ¯ä¸ªç»„åˆä¸­çš„æ¯ä¸ªæµç”Ÿæˆå¤šä¸ªå‰¯æœ¬
            if stream_num > 1:
                print(f"ç”Ÿæˆ {sql_type} ç»„åˆï¼Œæ¯ç§æµç±»å‹åˆ›å»º {stream_num} ä¸ªæµ")
                result = {}
                
                for base_name, base_sql in base_sqls.items():
                    # ä¸ºæ¯ä¸ªåŸºç¡€æµç”Ÿæˆå¤šä¸ªç¼–å·å‰¯æœ¬
                    multiple_streams = instance.generate_multiple_streams(base_sql, stream_num)
                    
                    # é‡å‘½åé”®å€¼ä»¥é¿å…å†²çª
                    for stream_key, stream_sql in multiple_streams.items():
                        # ä¾‹å¦‚: intervalsliding_stb_1, intervalsliding_stb_2, intervalsliding_stb_partition_by_tbname_1, etc.
                        new_key = f"{base_name}_{stream_key.split('_')[-1]}"  # æå–ç¼–å·
                        result[new_key] = stream_sql
                
                return result
            else:
                return base_sqls
        
        # å¤„ç†ç‰¹æ®Šç»„åˆ
        if sql_type == 'all_detailed':
            if auto_combine:
                print("è­¦å‘Š: all_detailed æ¨¡æ¿ä¸æ”¯æŒ --auto-combine å‚æ•°ï¼Œå°†å¿½ç•¥è¯¥å‚æ•°")
                
            result = {}
            for group_type in ['intervalsliding_detailed', 'sliding_detailed', 'session_detailed', 'count_detailed', 
                            'event_detailed', 'state_detailed', 'period_detailed']:
                group_sqls = group_templates[group_type](**kwargs)
                
                # å¦‚æœstream_num > 1ï¼Œä¸ºæ¯ä¸ªæµç”Ÿæˆå¤šä¸ªå‰¯æœ¬
                if stream_num > 1:
                    for base_name, base_sql in group_sqls.items():
                        multiple_streams = instance.generate_multiple_streams(base_sql, stream_num)
                        
                        for stream_key, stream_sql in multiple_streams.items():
                            new_key = f"{base_name}_{stream_key.split('_')[-1]}"
                            result[new_key] = stream_sql
                else:
                    result.update(group_sqls)
                    
            if stream_num > 1:
                print(f"ç”Ÿæˆ all_detailed ç»„åˆï¼Œæ¯ç§æµç±»å‹åˆ›å»º {stream_num} ä¸ªæµ")
                
            return result
        
        # é»˜è®¤è¿”å› select_stream
        print(f"è­¦å‘Š: æœªæ‰¾åˆ°æ¨¡æ¿ '{sql_type}'ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿ 'select_stream'")
        return instance.get_select_stream(**kwargs)

            
def format_delay_time(delay_ms):
    """å°†å»¶è¿Ÿæ¯«ç§’æ•°æ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„æ—¶é—´æ ¼å¼
    
    Args:
        delay_ms: å»¶è¿Ÿæ¯«ç§’æ•°
        
    Returns:
        str: æ ¼å¼åŒ–åçš„æ—¶é—´å­—ç¬¦ä¸²
    """
    if delay_ms is None:
        return "N/A"
    
    if delay_ms < 1000:  # å°äº1ç§’ï¼Œæ˜¾ç¤ºæ¯«ç§’
        return f"{delay_ms}ms"
    elif delay_ms < 60000:  # å°äº1åˆ†é’Ÿï¼Œæ˜¾ç¤ºç§’
        seconds = delay_ms / 1000.0
        return f"{seconds:.1f}s"
    elif delay_ms < 3600000:  # å°äº1å°æ—¶ï¼Œæ˜¾ç¤ºåˆ†é’Ÿå’Œç§’
        minutes = delay_ms // 60000
        seconds = (delay_ms % 60000) / 1000.0
        if seconds >= 1:
            return f"{minutes}m{seconds:.1f}s"
        else:
            return f"{minutes}m"
    else:  # 1å°æ—¶ä»¥ä¸Šï¼Œæ˜¾ç¤ºå°æ—¶ã€åˆ†é’Ÿã€ç§’
        hours = delay_ms // 3600000
        minutes = (delay_ms % 3600000) // 60000
        seconds = (delay_ms % 60000) / 1000.0
        
        parts = [f"{hours}h"]
        if minutes > 0:
            parts.append(f"{minutes}m")
        if seconds >= 1:
            parts.append(f"{seconds:.1f}s")
        
        return "".join(parts)
    
def extract_stream_creation_error(error_message):
    """ä»æµåˆ›å»ºé”™è¯¯ä¿¡æ¯ä¸­æå–TDengineçš„å…·ä½“é”™è¯¯
    
    Args:
        error_message: å®Œæ•´çš„å¼‚å¸¸é”™è¯¯ä¿¡æ¯
        
    Returns:
        str: æå–çš„TDengineé”™è¯¯ä¿¡æ¯
    """
    #print(f"è°ƒè¯• extract_stream_creation_error: è¾“å…¥å‚æ•°ç±»å‹={type(error_message)}, é•¿åº¦={len(str(error_message))}")
    
    try:
        # ç®€å•å¤„ç†ï¼šç›´æ¥è¿”å›é”™è¯¯ä¿¡æ¯ï¼ŒåªåšåŸºæœ¬æ¸…ç†
        if not error_message:
            print(f"è°ƒè¯•: é”™è¯¯ä¿¡æ¯ä¸ºç©ºï¼Œè¿”å›é»˜è®¤æ¶ˆæ¯")
            return ""
        
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        error_str = str(error_message).strip()
        #print(f"è°ƒè¯•: è½¬æ¢åå­—ç¬¦ä¸²é•¿åº¦: {len(error_str)}")
        
    
        if not error_str:
            print(f"è°ƒè¯•: è½¬æ¢åå­—ç¬¦ä¸²ä¸ºç©ºï¼Œè¿”å›é»˜è®¤æ¶ˆæ¯")
            return "é”™è¯¯ä¿¡æ¯ä¸ºç©º"
        
        # å¦‚æœå¤ªé•¿å°±æˆªæ–­
        if len(error_str) > 500:
            error_str = error_str[:500] + "..."
        
        # æ¸…ç†æ¢è¡Œç¬¦ï¼Œä¿æŒå•è¡Œæ˜¾ç¤º
        error_str = error_str.replace('\n', ' ').replace('\r', ' ')
        
        # æ¸…ç†HTMLç‰¹æ®Šå­—ç¬¦
        error_str = error_str.replace('<', '&lt;').replace('>', '&gt;')
        
        return error_str
        
    except Exception as e:
        return f"é”™è¯¯ä¿¡æ¯å¤„ç†å¤±è´¥: {str(e)}"

  
def extract_tdengine_error(error_message):
    """ä»é”™è¯¯ä¿¡æ¯ä¸­æå–TDengineçš„å…·ä½“é”™è¯¯
    
    Args:
        error_message: å®Œæ•´çš„é”™è¯¯ä¿¡æ¯
        
    Returns:
        str: æå–çš„TDengineé”™è¯¯ä¿¡æ¯
    """
    try:
        # ç®€åŒ–å¤„ç†ï¼šç›´æ¥è¿”å›é”™è¯¯ä¿¡æ¯ï¼ŒåªåšåŸºæœ¬çš„é•¿åº¦æ§åˆ¶
        if not error_message:
            return ""
        
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆé˜²æ­¢ä¼ å…¥éå­—ç¬¦ä¸²ç±»å‹ï¼‰
        error_str = str(error_message).strip()
        #print(f"è°ƒè¯•: è½¬æ¢åå­—ç¬¦ä¸²é•¿åº¦: {len(error_str)}")
        print(f"è°ƒè¯•: åŸå§‹é”™è¯¯ä¿¡æ¯å‰300ä¸ªå­—ç¬¦: {error_str[:300]}")
        
        # å¦‚æœé”™è¯¯ä¿¡æ¯å¤ªé•¿ï¼Œæˆªæ–­åˆ°åˆç†é•¿åº¦
        max_length = 500
        if len(error_str) > max_length:
            error_str = error_str[:max_length] + "..."
        
        # æ¸…ç†ä¸€äº›å¯èƒ½å½±å“HTMLæ˜¾ç¤ºçš„å­—ç¬¦
        error_str = error_str.replace('<', '&lt;').replace('>', '&gt;')
        error_str = error_str.replace('\n', ' ').replace('\r', ' ')
        print(f"è°ƒè¯•: æ¸…ç†åé”™è¯¯ä¿¡æ¯é•¿åº¦: {len(error_str)}")
        print(f"è°ƒè¯•: æœ€ç»ˆè¿”å›çš„é”™è¯¯ä¿¡æ¯: {error_str[:200]}...")
        
        return error_str
        
    except Exception as e:
        # å¦‚æœå¤„ç†å‡ºé”™ï¼Œè¿”å›ç®€å•çš„é”™è¯¯ä¿¡æ¯
        return f"é”™è¯¯ä¿¡æ¯å¤„ç†å¤±è´¥: {str(e)}"
    

def format_sql_for_display(sql_text):
    """æ ¼å¼åŒ–SQLç”¨äºæ˜¾ç¤ºï¼Œå»æ‰å¤šä½™çš„ç©ºç™½å’Œæ¢è¡Œ
    
    Args:
        sql_text: åŸå§‹SQLæ–‡æœ¬
        
    Returns:
        str: æ ¼å¼åŒ–åçš„SQL
    """
    try:
        import re
        
        # 1. å»æ‰å‰åçš„ç©ºç™½
        sql = sql_text.strip()
        
        # 2. å°†å¤šä¸ªè¿ç»­çš„ç©ºç™½å­—ç¬¦ï¼ˆåŒ…æ‹¬æ¢è¡Œã€åˆ¶è¡¨ç¬¦ã€ç©ºæ ¼ï¼‰æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
        sql = re.sub(r'\s+', ' ', sql)
        
        # 3. åœ¨ä¸»è¦çš„SQLå…³é”®å­—å‰åæ·»åŠ é€‚å½“çš„æ¢è¡Œï¼Œä½¿ç»“æ„æ›´æ¸…æ™°
        # ä½†ä¿æŒæ•´ä½“ç´§å‡‘
        keywords = [
            'create stream', 'from', 'partition by', 'into', 'as select', 
            'where', 'interval\\(', 'sliding\\(', 'session\\(', 'period\\(', 
            'count_window\\(', 'event_window\\(', 'state_window\\('
        ]
        
        for keyword in keywords:
            # åœ¨å…³é”®å­—å‰æ·»åŠ æ¢è¡Œï¼ˆé™¤äº†ç¬¬ä¸€ä¸ªcreate streamï¼‰
            if keyword != 'create stream':
                sql = re.sub(f'\\s+({keyword})', r' \1', sql, flags=re.IGNORECASE)
        
        # 4. ç‰¹æ®Šå¤„ç†ï¼šåœ¨ä¸»è¦å­å¥ä¹‹é—´æ·»åŠ åˆ†éš”
        # ä½†ä¸æ·»åŠ å®é™…æ¢è¡Œï¼Œè€Œæ˜¯ç”¨ | ç¬¦å·åˆ†éš”å¢å¼ºå¯è¯»æ€§
        sql = re.sub(r'\s+(from\s+)', r' | \1', sql, flags=re.IGNORECASE)
        sql = re.sub(r'\s+(partition\s+by)', r' | \1', sql, flags=re.IGNORECASE)
        sql = re.sub(r'\s+(into\s+)', r' | \1', sql, flags=re.IGNORECASE)
        sql = re.sub(r'\s+(as\s+select)', r' | \1', sql, flags=re.IGNORECASE)
        
        return sql
        
    except Exception as e:
        # å¦‚æœæ ¼å¼åŒ–å‡ºé”™ï¼Œè¿”å›ç®€å•æ¸…ç†åçš„ç‰ˆæœ¬
        return ' '.join(sql_text.split())



class StreamBatchTester:
    """æµè®¡ç®—æ‰¹é‡æµ‹è¯•å™¨ - è‡ªåŠ¨æ‰§è¡Œå¤šç§å‚æ•°ç»„åˆæµ‹è¯•"""
    
    def __init__(self, base_args=None, specified_sql_types=None, filter_mode='all', single_template_mode='default'):
        """åˆå§‹åŒ–æ‰¹é‡æµ‹è¯•å™¨
        
        Args:
            base_args: åŸºç¡€å‚æ•°å­—å…¸ï¼ŒåŒ…å«ä¸å˜çš„å‚æ•°
            specified_sql_types: æŒ‡å®šçš„SQLç±»å‹åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤çš„å…¨éƒ¨ç»„åˆ
            filter_mode: è¿‡æ»¤æ¨¡å¼ ('all', 'skip-known-failures', 'only-known-failures')
        """
        self.base_args = base_args or {}
        self.test_results = []
        self.failed_tests = []
        self.current_test_index = 0
        self.total_tests = 0
        self.start_time = None
        self.specified_sql_types = specified_sql_types or []
        self.filter_mode = filter_mode
        self.single_template_mode = single_template_mode
        
        # ç¼“å­˜å·²çŸ¥å¤±è´¥æµ‹è¯•åˆ—è¡¨ï¼Œé¿å…é‡å¤è°ƒç”¨å’Œæ‰“å°
        self._known_failure_tests = None
        
        # é»˜è®¤çš„å›ºå®šå‚æ•°
        self.fixed_params = {
            'mode': 1,  # å®æ—¶æµè®¡ç®—æµ‹è¯•
            'check_stream_delay': True,
            'stream_num': 1,
            'real_time_batch_sleep': 0,
            'monitor_interval': 30,
            'delay_check_interval': 30,
            'max_delay_threshold': 30000,
            'deployment_mode': 'single',
            'table_count': 2000,
            'histroy_rows': 1,
            'real_time_batch_rows': 200,
            'disorder_ratio': 0,
            'vgroups': 10,
            'debug_flag': 131,
            'num_of_log_lines': 500000
        }
        
        # æ›´æ–°å›ºå®šå‚æ•°
        self.fixed_params.update(self.base_args)       
        
        # æ‰¹é‡æµ‹è¯•ä¸­å¼ºåˆ¶å¯ç”¨æµå»¶è¿Ÿæ£€æŸ¥
        self.fixed_params['check_stream_delay'] = True
        
        print(f"æ‰¹é‡æµ‹è¯•å™¨åˆå§‹åŒ–å®Œæˆ:")
        if self.specified_sql_types:
            print(f"  æŒ‡å®šSQLç±»å‹: {', '.join(self.specified_sql_types)}")
        else:
            print(f"  SQLç±»å‹: å…¨éƒ¨ç»„åˆ")
            
        print(f"  è¿‡æ»¤æ¨¡å¼: {self.filter_mode}")
        if self.filter_mode != 'all':
            failure_count = len(self.get_known_failure_tests())
            print(f"  å·²çŸ¥å¤±è´¥æµ‹è¯•: {failure_count} ä¸ª")
            if self.filter_mode == 'skip-known-failures':
                print(f"  å°†è·³è¿‡å·²çŸ¥å¤±è´¥çš„æµ‹è¯•åœºæ™¯ï¼Œä¸“æ³¨äºæˆåŠŸåœºæ™¯")
            elif self.filter_mode == 'only-known-failures':
                print(f"  ä»…è¿è¡Œå·²çŸ¥å¤±è´¥çš„æµ‹è¯•åœºæ™¯ï¼Œç”¨äºè°ƒè¯•å¤±è´¥åŸå› ")
                
        print(f"  æµå»¶è¿Ÿæ£€æŸ¥: {'å¯ç”¨' if self.fixed_params['check_stream_delay'] else 'ç¦ç”¨'} (æ‰¹é‡æµ‹è¯•å¼ºåˆ¶å¯ç”¨)")
        print(f"  å»¶è¿Ÿæ£€æŸ¥é—´éš”: {self.fixed_params['delay_check_interval']}ç§’")
        print(f"  æœ€å¤§å»¶è¿Ÿé˜ˆå€¼: {self.fixed_params['max_delay_threshold']}ms")
        print(f"  æµæ•°é‡: {self.fixed_params['stream_num']}") 


    def get_known_failure_tests(self):
        """è·å–å·²çŸ¥å¤±è´¥çš„æµ‹è¯•åœºæ™¯åˆ—è¡¨
        
        è¿™é‡Œç»´æŠ¤æ‰€æœ‰å·²çŸ¥ä¼šå¤±è´¥çš„æµ‹è¯•åœºæ™¯åç§°
        æ‚¨å¯ä»¥ç›´æ¥åœ¨è¿™é‡Œæ·»åŠ æˆ–åˆ é™¤å¤±è´¥çš„æµ‹è¯•åç§°
        
        Returns:
            set: å·²çŸ¥å¤±è´¥çš„æµ‹è¯•åç§°é›†åˆ
        """
        # ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤åˆ›å»ºå’Œæ‰“å°
        if self._known_failure_tests is not None:
            return self._known_failure_tests
        
        # åœ¨è¿™é‡Œç»´æŠ¤å·²çŸ¥å¤±è´¥çš„æµ‹è¯•åç§°åˆ—è¡¨
        known_failures = {
            # period 
            'period_stb_agg_tbname',
            'period_stb_select_tbname', 
            'period_stb_partition_by_tag_agg_tbname',
            'period_stb_partition_by_tag_select_tbname',
            'period_tb_agg_tbname',
            'period_tb_select_tbname',
            
            # count 
            'count_stb_agg_tbname',
            'count_stb_select_tbname',
            'count_stb_agg_trows', 
            'count_stb_select_trows',
            'count_stb_agg_sourcetable_stb',
            'count_stb_select_sourcetable_stb',
            'count_stb_partition_by_tag_agg_tbname',
            'count_stb_partition_by_tag_select_tbname',
            'count_stb_partition_by_tag_agg_trows', 
            'count_stb_partition_by_tag_select_trows',
            'count_stb_partition_by_tag_agg_sourcetable_stb',
            'count_stb_partition_by_tag_select_sourcetable_stb',
            'count_tb_agg_tbname',
            'count_tb_select_tbname',
            
            # event 
            'event_stb_agg_tbname',
            'event_stb_select_tbname',
            'event_stb_agg_trows',
            'event_stb_select_trows',
            'event_stb_agg_sourcetable_stb', 
            'event_stb_select_sourcetable_stb',
            'event_stb_partition_by_tag_agg_tbname',
            'event_stb_partition_by_tag_select_tbname',
            'event_stb_partition_by_tag_agg_trows',
            'event_stb_partition_by_tag_select_trows',
            'event_stb_partition_by_tag_agg_sourcetable_stb', 
            'event_stb_partition_by_tag_select_sourcetable_stb',
            'event_tb_agg_tbname',
            'event_tb_select_tbname',
            
            # state 
            'state_stb_agg_tbname',
            'state_stb_select_tbname',
            'state_stb_agg_trows',
            'state_stb_select_trows',
            'state_stb_agg_sourcetable_stb', 
            'state_stb_select_sourcetable_stb',
            'state_stb_partition_by_tag_agg_tbname',
            'state_stb_partition_by_tag_select_tbname',
            'state_stb_partition_by_tag_agg_trows',
            'state_stb_partition_by_tag_select_trows',
            'state_stb_partition_by_tag_agg_sourcetable_stb', 
            'state_stb_partition_by_tag_select_sourcetable_stb',
            'state_tb_agg_tbname',
            'state_tb_select_tbname',
            
            # intervalsliding
            'intervalsliding_stb_agg_tbname',
            'intervalsliding_stb_select_tbname',
            'intervalsliding_stb_partition_by_tag_agg_tbname',
            'intervalsliding_stb_partition_by_tag_select_tbname',
            'intervalsliding_tb_agg_tbname',
            'intervalsliding_tb_select_tbname',
            
            # session            
            'session_stb_agg_tbname',
            'session_stb_select_tbname',
            'session_stb_partition_by_tag_agg_tbname',
            'session_stb_partition_by_tag_select_tbname',
            'session_tb_agg_tbname',
            'session_tb_select_tbname',
            
            # sliding
            'sliding_stb_agg_tbname',
            'sliding_stb_select_tbname',
            'sliding_stb_partition_by_tag_agg_tbname',
            'sliding_stb_partition_by_tag_select_tbname',
            'sliding_tb_agg_tbname',
            'sliding_tb_select_tbname',
            
        }
        
        # ç¼“å­˜ç»“æœå¹¶åªæ‰“å°ä¸€æ¬¡
        self._known_failure_tests = known_failures
        print(f"ç»´æŠ¤çš„å·²çŸ¥å¤±è´¥æµ‹è¯•åˆ—è¡¨åŒ…å« {len(known_failures)} ä¸ªåœºæ™¯")
        return self._known_failure_tests
    
    def extract_stream_names_from_sql(self, sql_templates):
        """ä»SQLæ¨¡æ¿ä¸­æå–æµåç§°
        
        Args:
            sql_templates: SQLæ¨¡æ¿å­—ç¬¦ä¸²æˆ–å­—å…¸
            
        Returns:
            set: æµåç§°é›†åˆ
        """
        import re
        stream_names = set()
        
        try:
            if isinstance(sql_templates, dict):
                for sql_template in sql_templates.values():
                    match = re.search(r'create\s+stream\s+([^\s]+)', sql_template, re.IGNORECASE)
                    if match:
                        full_stream_name = match.group(1)
                        # å»æ‰æ•°æ®åº“å‰ç¼€ï¼Œåªä¿ç•™æµåç§°éƒ¨åˆ†
                        if '.' in full_stream_name:
                            stream_name = full_stream_name.split('.')[-1]
                        else:
                            stream_name = full_stream_name
                        stream_names.add(stream_name)
                        print(f"è°ƒè¯•: ä»SQLä¸­æå–æµåç§°: {full_stream_name} -> {stream_name}")
            else:
                match = re.search(r'create\s+stream\s+([^\s]+)', sql_templates, re.IGNORECASE)
                if match:
                    full_stream_name = match.group(1)
                    # å»æ‰æ•°æ®åº“å‰ç¼€ï¼Œåªä¿ç•™æµåç§°éƒ¨åˆ†
                    if '.' in full_stream_name:
                        stream_name = full_stream_name.split('.')[-1]
                    else:
                        stream_name = full_stream_name
                    stream_names.add(stream_name)
                    print(f"è°ƒè¯•: ä»SQLä¸­æå–æµåç§°: {full_stream_name} -> {stream_name}")
        except Exception as e:
            print(f"æå–æµåç§°æ—¶å‡ºé”™: {str(e)}")
        
        return stream_names

    def should_skip_test_by_stream_names(self, stream_names):
        """æ ¹æ®æµåç§°åˆ¤æ–­æ˜¯å¦è·³è¿‡æµ‹è¯•
        
        Args:
            stream_names: å½“å‰æµ‹è¯•è¦åˆ›å»ºçš„æµåç§°é›†åˆ
            
        Returns:
            tuple: (æ˜¯å¦è·³è¿‡, åŸå› )
        """
        if self.filter_mode == 'all':
            return False, ""
        
        print(f"è°ƒè¯•è¿‡æ»¤: å½“å‰æµåç§°é›†åˆ: {stream_names}")
        print(f"è°ƒè¯•è¿‡æ»¤: è¿‡æ»¤æ¨¡å¼: {self.filter_mode}")
        
        known_failures = self.get_known_failure_tests()
        print(f"è°ƒè¯•è¿‡æ»¤: å·²çŸ¥å¤±è´¥æµæ•°é‡: {len(known_failures)}")
        
        failed_streams = stream_names.intersection(known_failures)
        success_streams = stream_names - known_failures
        
        print(f"è°ƒè¯•è¿‡æ»¤: åŒ¹é…åˆ°çš„å¤±è´¥æµ: {failed_streams}")
        print(f"è°ƒè¯•è¿‡æ»¤: æˆåŠŸæµ: {success_streams}")
        
        if self.filter_mode == 'skip-known-failures':
            # å¦‚æœåŒ…å«å·²çŸ¥å¤±è´¥çš„æµï¼Œè·³è¿‡
            if failed_streams:
                return True, f"åŒ…å«å·²çŸ¥å¤±è´¥æµ: {', '.join(failed_streams)}"
            return False, ""
            
        elif self.filter_mode == 'only-known-failures':
            # å¦‚æœä¸åŒ…å«å·²çŸ¥å¤±è´¥çš„æµï¼Œè·³è¿‡
            if not failed_streams:
                return True, f"ä¸åŒ…å«å·²çŸ¥å¤±è´¥æµï¼Œå½“å‰æµ: {', '.join(stream_names)}"
            return False, ""
            
        return False, ""
        
    def get_test_combinations(self):
        """è·å–æ‰€æœ‰æµ‹è¯•ç»„åˆ
        
        Returns:
            list: åŒ…å«æ‰€æœ‰å‚æ•°ç»„åˆçš„åˆ—è¡¨
        """
        if self.specified_sql_types:
            # å¦‚æœæŒ‡å®šäº†SQLç±»å‹ï¼Œä½¿ç”¨æŒ‡å®šçš„ç±»å‹è¿›è¡Œç»„åˆ
            return self.get_combinations_for_specified_types()
        else:
            # ä½¿ç”¨é»˜è®¤çš„å…¨éƒ¨ç»„åˆ
            return self.get_default_combinations()
    
    def get_default_combinations(self):
        """è·å–é»˜è®¤çš„å…¨éƒ¨æµ‹è¯•ç»„åˆ """
        # å®šä¹‰å˜åŒ–çš„å‚æ•°
        variable_params = {
            'tbname_or_trows_or_sourcetable': ['tbname', 'trows', 'sourcetable'],
            'agg_or_select': ['agg', 'select'],
            'sql_type': [
                # å•ä¸ªè¯¦ç»†æ¨¡æ¿ - é—´éš”æ»‘åŠ¨çª—å£
                'intervalsliding_stb', 'intervalsliding_stb_partition_by_tbname', 
                'intervalsliding_stb_partition_by_tag', 'intervalsliding_tb',
                # å•ä¸ªè¯¦ç»†æ¨¡æ¿ - æ»‘åŠ¨çª—å£  
                'sliding_stb', 'sliding_stb_partition_by_tbname',
                'sliding_stb_partition_by_tag', 'sliding_tb',
                # å•ä¸ªè¯¦ç»†æ¨¡æ¿ - ä¼šè¯çª—å£
                'session_stb', 'session_stb_partition_by_tbname',
                'session_stb_partition_by_tag', 'session_tb',
                # å•ä¸ªè¯¦ç»†æ¨¡æ¿ - è®¡æ•°çª—å£
                'count_stb', 'count_stb_partition_by_tbname',
                'count_stb_partition_by_tag', 'count_tb',
                # å•ä¸ªè¯¦ç»†æ¨¡æ¿ - äº‹ä»¶çª—å£
                'event_stb', 'event_stb_partition_by_tbname',
                'event_stb_partition_by_tag', 'event_tb',
                # å•ä¸ªè¯¦ç»†æ¨¡æ¿ - çŠ¶æ€çª—å£
                'state_stb', 'state_stb_partition_by_tbname',
                'state_stb_partition_by_tag', 'state_tb',
                # å•ä¸ªè¯¦ç»†æ¨¡æ¿ - å®šæ—¶è§¦å‘
                'period_stb', 'period_stb_partition_by_tbname',
                'period_stb_partition_by_tag', 'period_tb'
            ]
        }
        
        # ç”Ÿæˆæ‰€æœ‰ç»„åˆ
        combinations = []
        keys = list(variable_params.keys())
        values = list(variable_params.values())
        
        for combination in itertools.product(*values):
            param_dict = dict(zip(keys, combination))
            combinations.append(param_dict)
            
        return combinations
    
    def get_combinations_for_specified_types(self):
        """ä¸ºæŒ‡å®šçš„SQLç±»å‹ç”Ÿæˆæµ‹è¯•ç»„åˆ"""
        combinations = []
        
        # è§£ææŒ‡å®šçš„SQLç±»å‹å¹¶ç”Ÿæˆå¯¹åº”çš„ç»„åˆ
        for sql_type in self.specified_sql_types:
            type_combinations = self.get_combinations_for_single_type(sql_type)
            combinations.extend(type_combinations)
        
        return combinations
    
    def get_combinations_for_single_type(self, sql_type):
        """ä¸ºå•ä¸ªSQLç±»å‹ç”Ÿæˆæµ‹è¯•ç»„åˆ"""
        combinations = []
        
        # å›ºå®šå‚æ•°ç»„åˆæ¨¡æ¿ - è¿™äº›å¿½ç•¥å‘½ä»¤è¡Œçš„ tbname_or_trows_or_sourcetable å’Œ agg_or_select
        fixed_param_templates = {
            'tbname_agg': [
                ('tbname', 'agg', 'intervalsliding_stb'), ('tbname', 'agg', 'intervalsliding_stb_partition_by_tbname'),
                ('tbname', 'agg', 'intervalsliding_stb_partition_by_tag'), ('tbname', 'agg', 'intervalsliding_tb'),
                ('tbname', 'agg', 'sliding_stb'), ('tbname', 'agg', 'sliding_stb_partition_by_tbname'),
                ('tbname', 'agg', 'sliding_stb_partition_by_tag'), ('tbname', 'agg', 'sliding_tb'),
                ('tbname', 'agg', 'session_stb'), ('tbname', 'agg', 'session_stb_partition_by_tbname'),
                ('tbname', 'agg', 'session_stb_partition_by_tag'), ('tbname', 'agg', 'session_tb'),
                ('tbname', 'agg', 'count_stb'), ('tbname', 'agg', 'count_stb_partition_by_tbname'),
                ('tbname', 'agg', 'count_stb_partition_by_tag'), ('tbname', 'agg', 'count_tb'),
                ('tbname', 'agg', 'event_stb'), ('tbname', 'agg', 'event_stb_partition_by_tbname'),
                ('tbname', 'agg', 'event_stb_partition_by_tag'), ('tbname', 'agg', 'event_tb'),
                ('tbname', 'agg', 'state_stb'), ('tbname', 'agg', 'state_stb_partition_by_tbname'),
                ('tbname', 'agg', 'state_stb_partition_by_tag'), ('tbname', 'agg', 'state_tb'),
                ('tbname', 'agg', 'period_stb'), ('tbname', 'agg', 'period_stb_partition_by_tbname'),
                ('tbname', 'agg', 'period_stb_partition_by_tag'), ('tbname', 'agg', 'period_tb')
            ],
            'tbname_select': [
                ('tbname', 'select', 'intervalsliding_stb'), ('tbname', 'select', 'intervalsliding_stb_partition_by_tbname'),
                ('tbname', 'select', 'intervalsliding_stb_partition_by_tag'), ('tbname', 'select', 'intervalsliding_tb'),
                ('tbname', 'select', 'sliding_stb'), ('tbname', 'select', 'sliding_stb_partition_by_tbname'),
                ('tbname', 'select', 'sliding_stb_partition_by_tag'), ('tbname', 'select', 'sliding_tb'),
                ('tbname', 'select', 'session_stb'), ('tbname', 'select', 'session_stb_partition_by_tbname'),
                ('tbname', 'select', 'session_stb_partition_by_tag'), ('tbname', 'select', 'session_tb'),
                ('tbname', 'select', 'count_stb'), ('tbname', 'select', 'count_stb_partition_by_tbname'),
                ('tbname', 'select', 'count_stb_partition_by_tag'), ('tbname', 'select', 'count_tb'),
                ('tbname', 'select', 'event_stb'), ('tbname', 'select', 'event_stb_partition_by_tbname'),
                ('tbname', 'select', 'event_stb_partition_by_tag'), ('tbname', 'select', 'event_tb'),
                ('tbname', 'select', 'state_stb'), ('tbname', 'select', 'state_stb_partition_by_tbname'),
                ('tbname', 'select', 'state_stb_partition_by_tag'), ('tbname', 'select', 'state_tb'),
                ('tbname', 'select', 'period_stb'), ('tbname', 'select', 'period_stb_partition_by_tbname'),
                ('tbname', 'select', 'period_stb_partition_by_tag'), ('tbname', 'select', 'period_tb')
            ],
            'trows_agg': [
                ('trows', 'agg', 'intervalsliding_stb'), ('trows', 'agg', 'intervalsliding_stb_partition_by_tbname'),
                ('trows', 'agg', 'intervalsliding_stb_partition_by_tag'), ('trows', 'agg', 'intervalsliding_tb'),
                ('trows', 'agg', 'sliding_stb'), ('trows', 'agg', 'sliding_stb_partition_by_tbname'),
                ('trows', 'agg', 'sliding_stb_partition_by_tag'), ('trows', 'agg', 'sliding_tb'),
                ('trows', 'agg', 'session_stb'), ('trows', 'agg', 'session_stb_partition_by_tbname'),
                ('trows', 'agg', 'session_stb_partition_by_tag'), ('trows', 'agg', 'session_tb'),
                ('trows', 'agg', 'count_stb'), ('trows', 'agg', 'count_stb_partition_by_tbname'),
                ('trows', 'agg', 'count_stb_partition_by_tag'), ('trows', 'agg', 'count_tb'),
                ('trows', 'agg', 'event_stb'), ('trows', 'agg', 'event_stb_partition_by_tbname'),
                ('trows', 'agg', 'event_stb_partition_by_tag'), ('trows', 'agg', 'event_tb'),
                ('trows', 'agg', 'state_stb'), ('trows', 'agg', 'state_stb_partition_by_tbname'),
                ('trows', 'agg', 'state_stb_partition_by_tag'), ('trows', 'agg', 'state_tb'),
                ('trows', 'agg', 'period_stb'), ('trows', 'agg', 'period_stb_partition_by_tbname'),
                ('trows', 'agg', 'period_stb_partition_by_tag'), ('trows', 'agg', 'period_tb')
            ],
            'trows_select': [
                ('trows', 'select', 'intervalsliding_stb'), ('trows', 'select', 'intervalsliding_stb_partition_by_tbname'),
                ('trows', 'select', 'intervalsliding_stb_partition_by_tag'), ('trows', 'select', 'intervalsliding_tb'),
                ('trows', 'select', 'sliding_stb'), ('trows', 'select', 'sliding_stb_partition_by_tbname'),
                ('trows', 'select', 'sliding_stb_partition_by_tag'), ('trows', 'select', 'sliding_tb'),
                ('trows', 'select', 'session_stb'), ('trows', 'select', 'session_stb_partition_by_tbname'),
                ('trows', 'select', 'session_stb_partition_by_tag'), ('trows', 'select', 'session_tb'),
                ('trows', 'select', 'count_stb'), ('trows', 'select', 'count_stb_partition_by_tbname'),
                ('trows', 'select', 'count_stb_partition_by_tag'), ('trows', 'select', 'count_tb'),
                ('trows', 'select', 'event_stb'), ('trows', 'select', 'event_stb_partition_by_tbname'),
                ('trows', 'select', 'event_stb_partition_by_tag'), ('trows', 'select', 'event_tb'),
                ('trows', 'select', 'state_stb'), ('trows', 'select', 'state_stb_partition_by_tbname'),
                ('trows', 'select', 'state_stb_partition_by_tag'), ('trows', 'select', 'state_tb'),
                ('trows', 'select', 'period_stb'), ('trows', 'select', 'period_stb_partition_by_tbname'),
                ('trows', 'select', 'period_stb_partition_by_tag'), ('trows', 'select', 'period_tb')
            ],
            'sourcetable_agg': [
                ('sourcetable', 'agg', 'intervalsliding_stb'), ('sourcetable', 'agg', 'intervalsliding_stb_partition_by_tbname'),
                ('sourcetable', 'agg', 'intervalsliding_stb_partition_by_tag'), ('sourcetable', 'agg', 'intervalsliding_tb'),
                ('sourcetable', 'agg', 'sliding_stb'), ('sourcetable', 'agg', 'sliding_stb_partition_by_tbname'),
                ('sourcetable', 'agg', 'sliding_stb_partition_by_tag'), ('sourcetable', 'agg', 'sliding_tb'),
                ('sourcetable', 'agg', 'session_stb'), ('sourcetable', 'agg', 'session_stb_partition_by_tbname'),
                ('sourcetable', 'agg', 'session_stb_partition_by_tag'), ('sourcetable', 'agg', 'session_tb'),
                ('sourcetable', 'agg', 'count_stb'), ('sourcetable', 'agg', 'count_stb_partition_by_tbname'),
                ('sourcetable', 'agg', 'count_stb_partition_by_tag'), ('sourcetable', 'agg', 'count_tb'),
                ('sourcetable', 'agg', 'event_stb'), ('sourcetable', 'agg', 'event_stb_partition_by_tbname'),
                ('sourcetable', 'agg', 'event_stb_partition_by_tag'), ('sourcetable', 'agg', 'event_tb'),
                ('sourcetable', 'agg', 'state_stb'), ('sourcetable', 'agg', 'state_stb_partition_by_tbname'),
                ('sourcetable', 'agg', 'state_stb_partition_by_tag'), ('sourcetable', 'agg', 'state_tb'),
                ('sourcetable', 'agg', 'period_stb'), ('sourcetable', 'agg', 'period_stb_partition_by_tbname'),
                ('sourcetable', 'agg', 'period_stb_partition_by_tag'), ('sourcetable', 'agg', 'period_tb')
            ],
            'sourcetable_select': [
                ('sourcetable', 'select', 'intervalsliding_stb'), ('sourcetable', 'select', 'intervalsliding_stb_partition_by_tbname'),
                ('sourcetable', 'select', 'intervalsliding_stb_partition_by_tag'), ('sourcetable', 'select', 'intervalsliding_tb'),
                ('sourcetable', 'select', 'sliding_stb'), ('sourcetable', 'select', 'sliding_stb_partition_by_tbname'),
                ('sourcetable', 'select', 'sliding_stb_partition_by_tag'), ('sourcetable', 'select', 'sliding_tb'),
                ('sourcetable', 'select', 'session_stb'), ('sourcetable', 'select', 'session_stb_partition_by_tbname'),
                ('sourcetable', 'select', 'session_stb_partition_by_tag'), ('sourcetable', 'select', 'session_tb'),
                ('sourcetable', 'select', 'count_stb'), ('sourcetable', 'select', 'count_stb_partition_by_tbname'),
                ('sourcetable', 'select', 'count_stb_partition_by_tag'), ('sourcetable', 'select', 'count_tb'),
                ('sourcetable', 'select', 'event_stb'), ('sourcetable', 'select', 'event_stb_partition_by_tbname'),
                ('sourcetable', 'select', 'event_stb_partition_by_tag'), ('sourcetable', 'select', 'event_tb'),
                ('sourcetable', 'select', 'state_stb'), ('sourcetable', 'select', 'state_stb_partition_by_tbname'),
                ('sourcetable', 'select', 'state_stb_partition_by_tag'), ('sourcetable', 'select', 'state_tb'),
                ('sourcetable', 'select', 'period_stb'), ('sourcetable', 'select', 'period_stb_partition_by_tbname'),
                ('sourcetable', 'select', 'period_stb_partition_by_tag'), ('sourcetable', 'select', 'period_tb')
            ]
        }
        
        # ç»„åˆæ¨¡æ¿ - æ¯ç»„åŒ…å«4ç§ç»„åˆï¼Œä½¿ç”¨æ‰€æœ‰çš„ tbname_or_trows_or_sourcetable å’Œ agg_or_select ç»„åˆ
        detailed_templates = {
            'intervalsliding_detailed': [
                'intervalsliding_stb', 'intervalsliding_stb_partition_by_tbname',
                'intervalsliding_stb_partition_by_tag', 'intervalsliding_tb'
            ],
            'sliding_detailed': [
                'sliding_stb', 'sliding_stb_partition_by_tbname',
                'sliding_stb_partition_by_tag', 'sliding_tb'
            ],
            'session_detailed': [
                'session_stb', 'session_stb_partition_by_tbname',
                'session_stb_partition_by_tag', 'session_tb'
            ],
            'count_detailed': [
                'count_stb', 'count_stb_partition_by_tbname',
                'count_stb_partition_by_tag', 'count_tb'
            ],
            'event_detailed': [
                'event_stb', 'event_stb_partition_by_tbname',
                'event_stb_partition_by_tag', 'event_tb'
            ],
            'state_detailed': [
                'state_stb', 'state_stb_partition_by_tbname',
                'state_stb_partition_by_tag', 'state_tb'
            ],
            'period_detailed': [
                'period_stb', 'period_stb_partition_by_tbname',
                'period_stb_partition_by_tag', 'period_tb'
            ]
        }
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å›ºå®šå‚æ•°ç»„åˆæ¨¡æ¿
        if sql_type in fixed_param_templates:
            for tbname_param, agg_param, actual_sql_type in fixed_param_templates[sql_type]:
                combinations.append({
                    'sql_type': actual_sql_type,
                    'tbname_or_trows_or_sourcetable': tbname_param,
                    'agg_or_select': agg_param
                })
            print(f"  {sql_type}: ç”Ÿæˆ {len(fixed_param_templates[sql_type])} ç§ç»„åˆ")
            
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç»„åˆæ¨¡æ¿
        elif sql_type in detailed_templates:
            for actual_sql_type in detailed_templates[sql_type]:
                # å¯¹æ¯ä¸ªsql_typeï¼Œç”Ÿæˆæ‰€æœ‰ tbname_or_trows_or_sourcetable å’Œ agg_or_select çš„ç»„åˆ
                for tbname_param in ['tbname', 'trows', 'sourcetable']:
                    for agg_param in ['agg', 'select']:
                        combinations.append({
                            'sql_type': actual_sql_type,
                            'tbname_or_trows_or_sourcetable': tbname_param,
                            'agg_or_select': agg_param
                        })
            print(f"  {sql_type}: ç”Ÿæˆ {len(detailed_templates[sql_type]) * 3 * 2} ç§ç»„åˆ (4ä¸ªæ¨¡æ¿ Ã— 3ä¸ªfromç±»å‹ Ã— 2ä¸ªæŸ¥è¯¢ç±»å‹)")
            
        # æ£€æŸ¥æ˜¯å¦æ˜¯all_detailedç‰¹æ®Šæ¨¡æ¿
        elif sql_type == 'all_detailed':
            all_sql_types = []
            for template_list in detailed_templates.values():
                all_sql_types.extend(template_list)
            
            for actual_sql_type in all_sql_types:
                for tbname_param in ['tbname', 'trows', 'sourcetable']:
                    for agg_param in ['agg', 'select']:
                        combinations.append({
                            'sql_type': actual_sql_type,
                            'tbname_or_trows_or_sourcetable': tbname_param,
                            'agg_or_select': agg_param
                        })
            print(f"  {sql_type}: ç”Ÿæˆ {len(all_sql_types) * 3 * 2} ç§ç»„åˆ (28ä¸ªæ¨¡æ¿ Ã— 3ä¸ªfromç±»å‹ Ã— 2ä¸ªæŸ¥è¯¢ç±»å‹)")
            
        # æ£€æŸ¥æ˜¯å¦æ˜¯å•ä¸ªæ¨¡æ¿
        else:
            # # å•ä¸ªSQLç±»å‹ï¼Œç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
            # for tbname_param in ['tbname', 'trows', 'sourcetable']:
            #     for agg_param in ['agg', 'select']:
            #         combinations.append({
            #             'sql_type': sql_type,
            #             'tbname_or_trows_or_sourcetable': tbname_param,
            #             'agg_or_select': agg_param
            #         })
            # print(f"  {sql_type}: ç”Ÿæˆ 6 ç§ç»„åˆ (3ä¸ªfromç±»å‹ Ã— 2ä¸ªæŸ¥è¯¢ç±»å‹)")
            
            # å•ä¸ªSQLç±»å‹çš„æ–°é€»è¾‘ï¼šé»˜è®¤åªç”Ÿæˆä¸€ä¸ªæµ‹è¯•ï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼‰
            # ç”¨æˆ·å¯ä»¥é€šè¿‡æ·»åŠ å…¶ä»–å‚æ•°æ¥æ§åˆ¶è¡Œä¸º
            # æ£€æŸ¥æ‰¹é‡æµ‹è¯•æ¨¡å¼è®¾ç½®
            if hasattr(self, 'single_template_mode') and self.single_template_mode == 'all-combinations':
                # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
                for tbname_param in ['tbname', 'trows', 'sourcetable']:
                    for agg_param in ['agg', 'select']:
                        combinations.append({
                            'sql_type': sql_type,
                            'tbname_or_trows_or_sourcetable': tbname_param,
                            'agg_or_select': agg_param
                        })
                print(f"  {sql_type}: ç”Ÿæˆ 6 ç§ç»„åˆ (3ä¸ªfromç±»å‹ Ã— 2ä¸ªæŸ¥è¯¢ç±»å‹)")
            else:
                # åªç”Ÿæˆé»˜è®¤ç»„åˆ
                combinations.append({
                    'sql_type': sql_type,
                    'tbname_or_trows_or_sourcetable': 'sourcetable',
                    'agg_or_select': 'agg'
                })
                print(f"  {sql_type}: ç”Ÿæˆ 1 ç§ç»„åˆ (é»˜è®¤å‚æ•°)")
                print(f"    æç¤º: ä½¿ç”¨ --batch-single-template-mode all-combinations å¯æµ‹è¯•æ‰€æœ‰6ç§ç»„åˆ")
        
        return combinations
    
    def filter_combinations(self, combinations):
        """è¿‡æ»¤æµ‹è¯•ç»„åˆ
        
        ç”±äºéœ€è¦å…ˆç”ŸæˆSQLæ‰èƒ½è·å¾—æµåç§°ï¼Œè¿™é‡ŒåªåšåŸºæœ¬çš„æœ‰æ•ˆæ€§æ£€æŸ¥
        å®é™…çš„å¤±è´¥è¿‡æ»¤å°†åœ¨execute_single_testä¸­è¿›è¡Œ
        
        Args:
            combinations: åŸå§‹ç»„åˆåˆ—è¡¨
            
        Returns:
            list: è¿‡æ»¤åçš„æœ‰æ•ˆç»„åˆåˆ—è¡¨
        """
        valid_combinations = []
        for combo in combinations:
            if self.is_valid_combination(combo):
                valid_combinations.append(combo)
        
        # æ‰“å°è¿‡æ»¤ç»Ÿè®¡
        total_original = len(combinations)
        total_filtered = len(valid_combinations)
        
        print(f"\n=== æµ‹è¯•ç»„åˆåŸºæœ¬è¿‡æ»¤ç»Ÿè®¡ ===")
        print(f"åŸå§‹ç»„åˆæ•°: {total_original}")
        print(f"æœ‰æ•ˆç»„åˆæ•°: {total_filtered}")
        
        if self.filter_mode != 'all':
            print(f"è¿‡æ»¤æ¨¡å¼: {self.filter_mode}")
            print(f"æ³¨æ„: å·²çŸ¥å¤±è´¥æµçš„è¿‡æ»¤å°†åœ¨å®é™…åˆ›å»ºæµæ—¶è¿›è¡Œ")
        
        return valid_combinations
    
    def is_valid_combination(self, combo):
        """æ£€æŸ¥ç»„åˆæ˜¯å¦æœ‰æ•ˆ
        
        Args:
            combo: å‚æ•°ç»„åˆå­—å…¸
            
        Returns:
            bool: æ˜¯å¦ä¸ºæœ‰æ•ˆç»„åˆ
        """
        sql_type = combo['sql_type']
        tbname_param = combo['tbname_or_trows_or_sourcetable']
        
        # Period ç±»å‹çš„ç‰¹æ®Šå¤„ç†
        if sql_type.startswith('period_'):
            pass
            
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šè¿‡æ»¤è§„åˆ™
        # ä¾‹å¦‚ï¼šæŸäº›ç»„åˆå¯èƒ½ä¸æ”¯æŒæˆ–æ²¡æœ‰æ„ä¹‰
        
        return True
    
    def create_batch_result_dir(self):
        """åˆ›å»ºæ‰¹é‡æµ‹è¯•ç»“æœç›®å½•"""
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        result_dir = f"/tmp/stream_batch_test_{timestamp}"
        os.makedirs(result_dir, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        for subdir in ['logs', 'performance', 'reports', 'configs']:
            os.makedirs(os.path.join(result_dir, subdir), exist_ok=True)
            
        return result_dir
    
    def generate_test_config(self, combination, test_index, result_dir):
        """ä¸ºå•ä¸ªæµ‹è¯•ç”Ÿæˆé…ç½®
        
        Args:
            combination: å‚æ•°ç»„åˆ
            test_index: æµ‹è¯•ç¼–å·
            result_dir: ç»“æœç›®å½•
            
        Returns:
            dict: å®Œæ•´çš„æµ‹è¯•é…ç½®
        """
        # åˆå¹¶å›ºå®šå‚æ•°å’Œå˜åŒ–å‚æ•°
        config = self.fixed_params.copy()
        config.update(combination)
        
        # å¼ºåˆ¶ç¡®ä¿æµå»¶è¿Ÿæ£€æŸ¥åœ¨æ‰¹é‡æµ‹è¯•ä¸­å¯ç”¨
        config['check_stream_delay'] = True
    
        # è®¾ç½®è¾“å‡ºæ–‡ä»¶è·¯å¾„
        test_name = f"test_{test_index:03d}_{combination['sql_type']}_{combination['agg_or_select']}_{combination['tbname_or_trows_or_sourcetable']}"
        config['test_name'] = test_name
        config['perf_file'] = os.path.join(result_dir, 'performance', f'{test_name}_perf.log')
        config['delay_log_file'] = os.path.join(result_dir, 'logs', f'{test_name}_delay.log')
        config['test_log_file'] = os.path.join(result_dir, 'logs', f'{test_name}_test.log')
        
        # è°ƒè¯•è¾“å‡ºï¼šéªŒè¯å»¶è¿Ÿæ£€æŸ¥é…ç½®
        print(f"è°ƒè¯•: ä¸ºæµ‹è¯• {test_name} è®¾ç½®æ€§èƒ½æ–‡ä»¶: {config['perf_file']}")
        print(f"è°ƒè¯•: ä¸ºæµ‹è¯• {test_name} è®¾ç½®å»¶è¿Ÿæ—¥å¿—æ–‡ä»¶: {config['delay_log_file']}")
        print(f"  check_stream_delay: {config.get('check_stream_delay', 'NOT_SET')}")
        print(f"  delay_check_interval: {config.get('delay_check_interval', 'NOT_SET')}")
        print(f"  max_delay_threshold: {config.get('max_delay_threshold', 'NOT_SET')}")
        print(f"  stream_num: {config.get('stream_num', 'NOT_SET')}") 
        print(f"  monitor_warm_up_time: {config.get('monitor_warm_up_time', 'NOT_SET')}") 
        
        if not config['check_stream_delay']:
            # å¼ºåˆ¶å¯ç”¨
            config['check_stream_delay'] = True
            print(f"  âœ“ å·²å¼ºåˆ¶å¯ç”¨æµå»¶è¿Ÿæ£€æŸ¥")
        
        return config
    
    def execute_single_test(self, config):
        """æ‰§è¡Œå•ä¸ªæµ‹è¯•
        
        Args:
            config: æµ‹è¯•é…ç½®å­—å…¸
            
        Returns:
            dict: æµ‹è¯•ç»“æœ
        """
        test_name = config['test_name']
        start_time = time.time()
        
        print(f"\n{'='*80}")
        print(f"å¼€å§‹æµ‹è¯•: {test_name}")
        print(f"æµ‹è¯•ç¼–å·: {self.current_test_index}/{self.total_tests}")
        print(f"SQLç±»å‹: {config['sql_type']}")
        print(f"æŸ¥è¯¢ç±»å‹: {config['agg_or_select']}")
        print(f"FROMç±»å‹: {config['tbname_or_trows_or_sourcetable']}")
        print(f"è¿è¡Œæ—¶é—´: {config.get('time', 5)}åˆ†é’Ÿ")
        print(f"æµæ•°é‡: {config.get('stream_num', 1)}") 
        print(f"é¢„çƒ­æ—¶é—´: {config.get('monitor_warm_up_time', 'AUTO')}ç§’")
        print(f"{'='*80}")
        
        result = {
            'test_name': test_name,
            'config': config,
            'start_time': datetime.datetime.now().isoformat(),
            'status': 'RUNNING',
            'duration': 0,
            'error': None
        }
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡ï¼ˆåœ¨å®é™…åˆ›å»ºæµä¹‹å‰ï¼‰
        if self.filter_mode != 'all':
            try:
                # ç”ŸæˆSQLæ¨¡æ¿
                sql_templates = StreamSQLTemplates.get_sql(
                    config['sql_type'],
                    stream_num=config.get('stream_num', 1),
                    agg_or_select=config['agg_or_select'],
                    tbname_or_trows_or_sourcetable=config['tbname_or_trows_or_sourcetable']
                )
                
                # æå–æµåç§°
                stream_names = self.extract_stream_names_from_sql(sql_templates)
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡
                should_skip, skip_reason = self.should_skip_test_by_stream_names(stream_names)
                
                if should_skip:
                    print(f"â­ï¸  è·³è¿‡æµ‹è¯•: {skip_reason}")
                    
                    # è®°å½•è·³è¿‡ç»“æœ
                    end_time = time.time()
                    duration = end_time - start_time
                    
                    result.update({
                        'status': 'SKIPPED',
                        'end_time': datetime.datetime.now().isoformat(),
                        'duration': duration,
                        'skip_reason': skip_reason,
                        'stream_names': list(stream_names)
                    })
                    
                    return result
                else:
                    print(f"âœ… æµ‹è¯•é€šè¿‡è¿‡æ»¤æ£€æŸ¥ï¼Œå°†è¦åˆ›å»ºçš„æµ: {', '.join(stream_names)}")
                    
            except Exception as e:
                print(f"âš ï¸  è¿‡æ»¤æ£€æŸ¥æ—¶å‡ºé”™: {str(e)}ï¼Œç»§ç»­æ‰§è¡Œæµ‹è¯•")
        
        # ç»§ç»­æ‰§è¡ŒåŸæœ‰çš„æµ‹è¯•é€»è¾‘
        log_file = None
        original_stdout = sys.stdout
        
        try:
            # å¼ºåˆ¶æ¸…ç†ç¯å¢ƒ
            print("æ¸…ç†æµ‹è¯•ç¯å¢ƒ...")
            self.cleanup_environment()
            
            # ç­‰å¾…ç¯å¢ƒæ¸…ç†å®Œæˆ
            time.sleep(3)
        
            # é¢å¤–çš„çº¿ç¨‹æ¸…ç†æ£€æŸ¥
            #print("æ£€æŸ¥å¹¶æ¸…ç†å¯èƒ½çš„æ®‹ç•™çº¿ç¨‹...")
            import threading
            active_threads = threading.active_count()
            print(f"å½“å‰æ´»è·ƒçº¿ç¨‹æ•°: {active_threads}")
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()
            time.sleep(2)
        
            # éªŒè¯ç¯å¢ƒæ¸…ç†æ˜¯å¦æˆåŠŸ
            result_check = subprocess.run('ps -ef | grep taosd | grep -v grep', 
                                        shell=True, capture_output=True, text=True)
            if result_check.stdout:
                print("âš ï¸  å‘ç°æ®‹ç•™taosdè¿›ç¨‹ï¼Œå¼ºåˆ¶æ¸…ç†...")
                subprocess.run('pkill -9 taosd', shell=True)
                time.sleep(3)
            
            # åˆ›å»ºStreamStarterå®ä¾‹
            starter = StreamStarter(
                runtime=config.get('time', 5),
                perf_file=config['perf_file'],
                table_count=config['table_count'],
                histroy_rows=config['histroy_rows'],
                real_time_batch_rows=config['real_time_batch_rows'],
                real_time_batch_sleep=config['real_time_batch_sleep'],
                disorder_ratio=config['disorder_ratio'],
                vgroups=config['vgroups'],
                sql_type=config['sql_type'],
                stream_num=config.get('stream_num', 1),
                stream_perf_test_dir=config.get('stream_perf_test_dir', '/home/stream_perf_test_dir'),
                monitor_interval=config['monitor_interval'],
                deployment_mode=config['deployment_mode'],
                debug_flag=config['debug_flag'],
                num_of_log_lines=config['num_of_log_lines'],
                agg_or_select=config['agg_or_select'],
                tbname_or_trows_or_sourcetable=config['tbname_or_trows_or_sourcetable'],
                check_stream_delay=config['check_stream_delay'],
                max_delay_threshold=config['max_delay_threshold'],
                delay_check_interval=config['delay_check_interval'],
                delay_log_file=config['delay_log_file'],
                monitor_warm_up_time=config.get('monitor_warm_up_time') 
            )
            
            # æ‰“å¼€æ—¥å¿—æ–‡ä»¶å¹¶è®¾ç½®è¾“å‡ºé‡å®šå‘
            log_file = open(config['test_log_file'], 'w')
            
            # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
            class TeeOutput:
                def __init__(self, *files):
                    self.files = files
                def write(self, text):
                    for f in self.files:
                        try:
                            f.write(text)
                            f.flush()
                        except (ValueError, OSError) as e:
                            pass
                def flush(self):
                    for f in self.files:
                        try:
                            f.flush()
                        except (ValueError, OSError):
                            pass
            
            sys.stdout = TeeOutput(original_stdout, log_file)
            
            # æ‰§è¡Œæµ‹è¯•
            print(f"å¼€å§‹æ‰§è¡Œæµè®¡ç®—æµ‹è¯•: {test_name}")
            print(f"å»¶è¿Ÿç›‘æ§çŠ¶æ€: {'å¯ç”¨' if config['check_stream_delay'] else 'ç¦ç”¨'}")
            print(f"æ€§èƒ½ç›‘æ§æ–‡ä»¶: {config['perf_file']}")
            print(f"æµæ•°é‡: {config.get('stream_num', 1)}") 
            
            # âœ… éªŒè¯é¢„çƒ­æ—¶é—´æ˜¯å¦æ­£ç¡®ä¼ é€’
            if hasattr(starter, 'monitor_warm_up_time'):
                print(f"é¢„çƒ­æ—¶é—´é…ç½®: {starter.monitor_warm_up_time}ç§’")
            else:
                print(f"è­¦å‘Š: é¢„çƒ­æ—¶é—´å‚æ•°æœªè®¾ç½®")
            
            
            # æ ¸å¿ƒæµ‹è¯•é€»è¾‘ - ä½¿ç”¨å†…éƒ¨å¼‚å¸¸å¤„ç†
            test_error = None
            try:
                starter.do_test_stream_with_realtime_data()
            except Exception as inner_e:
                # æ•è·å…·ä½“çš„æµåˆ›å»ºæˆ–æ‰§è¡Œé”™è¯¯
                error_message = str(inner_e)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æµåˆ›å»ºç›¸å…³çš„é”™è¯¯
                if any(keyword in error_message.lower() for keyword in ['create stream', 'cursor.execute', '%%tbname']):
                    test_error = extract_stream_creation_error(error_message)
                else:
                    test_error = extract_tdengine_error(error_message)
                
            # æ­£å¸¸å®Œæˆæˆ–æœ‰é”™è¯¯ï¼Œéƒ½å…ˆå®‰å…¨åœ°å…³é—­æ–‡ä»¶å’Œæ¢å¤è¾“å‡º
            sys.stdout = original_stdout
            if log_file:
                log_file.close()
                log_file = None
                
            # å¦‚æœæœ‰é”™è¯¯ï¼Œç°åœ¨æŠ›å‡º
            if test_error:
                raise Exception(test_error)
                
            end_time = time.time()
            duration = end_time - start_time
            
            result.update({
                'status': 'SUCCESS',
                'end_time': datetime.datetime.now().isoformat(),
                'duration': duration
            })
            
            print(f"æµ‹è¯• {test_name} å®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’")
            
        except Exception as e:
            end_time = time.time()
            duration = end_time - start_time
            
            # ä½¿ç”¨å¼‚å¸¸æ¶ˆæ¯ä½œä¸ºé”™è¯¯ä¿¡æ¯
            error_message = str(e)
            
            # ç¡®ä¿é”™è¯¯ä¿¡æ¯ä¸ä¸ºç©º
            if not error_message or error_message.strip() == "":
                error_message = "æœªçŸ¥é”™è¯¯: æµ‹è¯•æ‰§è¡Œå¤±è´¥ä½†æ— å…·ä½“é”™è¯¯ä¿¡æ¯"
            
            result.update({
                'status': 'FAILED',
                'end_time': datetime.datetime.now().isoformat(),
                'duration': duration,
                'error': error_message
            })
            
            print(f"æµ‹è¯• {test_name} å¤±è´¥: {error_message}")
            
            self.failed_tests.append(result)
            
        finally:
            # å®‰å…¨åœ°æ¢å¤æ ‡å‡†è¾“å‡ºå’Œå…³é—­æ–‡ä»¶
            if sys.stdout != original_stdout:
                sys.stdout = original_stdout
            
            if log_file and not log_file.closed:
                try:
                    log_file.close()
                except:
                    pass
            
            # å¼ºåˆ¶æ¸…ç†ç¯å¢ƒï¼Œä¸ºä¸‹ä¸€ä¸ªæµ‹è¯•åšå‡†å¤‡
            print("æµ‹è¯•åå…¨é¢æ¸…ç†ç¯å¢ƒ...")
            try:
                # 1. å¼ºåˆ¶åœæ­¢æ‰€æœ‰taosdè¿›ç¨‹
                print("  â†’ åœæ­¢æ‰€æœ‰taosdè¿›ç¨‹")
                subprocess.run('pkill -9 taosd', shell=True)
                time.sleep(2)
                
                # 2. æ¸…ç†å¯èƒ½çš„ç›‘æ§çº¿ç¨‹å’Œæ–‡ä»¶å¥æŸ„
                print("  â†’ æ¸…ç†ç³»ç»Ÿèµ„æº")
                import gc
                gc.collect()
                
                # 3. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                print("  â†’ æ¸…ç†ä¸´æ—¶æ–‡ä»¶")
                subprocess.run('rm -f /tmp/stream_from*.json', shell=True)
                subprocess.run('rm -f /tmp/taosBenchmark_result.log', shell=True)
                
                # 4. æ£€æŸ¥æ´»è·ƒçº¿ç¨‹æ•°
                import threading
                active_threads = threading.active_count()
                print(f"  â†’ å½“å‰æ´»è·ƒçº¿ç¨‹æ•°: {active_threads}")
                
                # 5. éªŒè¯æ¸…ç†ç»“æœ
                check_result = subprocess.run('ps -ef | grep taosd | grep -v grep', 
                                            shell=True, capture_output=True, text=True)
                if check_result.stdout:
                    print(f"  âš ï¸  è­¦å‘Š: ä»æœ‰taosdè¿›ç¨‹è¿è¡Œï¼Œå¯èƒ½å½±å“ä¸‹ä¸ªæµ‹è¯•")
                else:
                    print(f"  âœ… ç¯å¢ƒæ¸…ç†å®Œæˆ")
                    
            except Exception as cleanup_e:
                print(f"  âš ï¸  æ¸…ç†è¿‡ç¨‹å‡ºé”™: {str(cleanup_e)}")
            
            # 5. ä¸ºä¸‹ä¸€ä¸ªæµ‹è¯•ç•™å‡ºç¼“å†²æ—¶é—´
            if self.current_test_index < self.total_tests:
                print("  â†’ ç­‰å¾…3ç§’åå¼€å§‹ä¸‹ä¸€ä¸ªæµ‹è¯•...")
                time.sleep(3)
            
        return result
            
            
    def cleanup_environment(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        try:
            print("æ‰§è¡Œå¢å¼ºç¯å¢ƒæ¸…ç†...")
            
            # 1. å¼ºåˆ¶åœæ­¢æ‰€æœ‰taosdè¿›ç¨‹
            print("  â†’ å¼ºåˆ¶åœæ­¢taosdè¿›ç¨‹")
            subprocess.run('pkill -9 taosd', shell=True)
            time.sleep(2)
            # å¼ºåˆ¶åœæ­¢æ‰€æœ‰æ´»è·ƒçš„ç›‘æ§çº¿ç¨‹
            stopped_threads = []
            for thread in threading.enumerate():
                if thread.name in ["MonitorSystemLoad", "TaosdMonitor", "StreamDelayMonitor", "DelayedStreamCreation", "ContinuousDataWriter"]:
                    try:
                        print(f"    å‘ç°æ´»è·ƒç›‘æ§çº¿ç¨‹: {thread.name}")
                        if hasattr(thread, '_target') and hasattr(thread._target, '__self__'):
                            monitor_obj = thread._target.__self__
                            if hasattr(monitor_obj, 'stop'):
                                print(f"    è°ƒç”¨ {thread.name} çš„stop()æ–¹æ³•")
                                monitor_obj.stop()
                            elif hasattr(monitor_obj, '_stop_event'):
                                print(f"    è®¾ç½® {thread.name} çš„_stop_event")
                                monitor_obj._stop_event.set()
                            elif hasattr(monitor_obj, '_should_stop'):
                                print(f"    è®¾ç½® {thread.name} çš„_should_stopæ ‡å¿—")
                                monitor_obj._should_stop = True
                        
                        # é€šç”¨çš„åœæ­¢æ ‡å¿—è®¾ç½®
                        if hasattr(thread, '_should_stop'):
                            thread._should_stop = True
                            print(f"    å·²è®¾ç½®çº¿ç¨‹ {thread.name} çš„åœæ­¢æ ‡å¿—")
                        
                        stopped_threads.append(thread.name)
                    except Exception as e:
                        print(f"    è®¾ç½®çº¿ç¨‹åœæ­¢æ ‡å¿—å¤±è´¥: {str(e)}")
            
            # ç­‰å¾…ç›‘æ§çº¿ç¨‹è‡ªç„¶ç»“æŸ
            if stopped_threads:
                print(f"  â†’ ç­‰å¾… {len(stopped_threads)} ä¸ªç›‘æ§çº¿ç¨‹ç»“æŸ...")
                start_wait = time.time()
                max_wait = 5  # æœ€å¤šç­‰å¾…5ç§’
                
                while time.time() - start_wait < max_wait:
                    active_monitor_threads = []
                    for thread in threading.enumerate():
                        if thread.name in ["MonitorSystemLoad", "TaosdMonitor", "StreamDelayMonitor", "DelayedStreamCreation", "ContinuousDataWriter"]:
                            active_monitor_threads.append(thread.name)
                    
                    if not active_monitor_threads:
                        print(f"  âœ… æ‰€æœ‰ç›‘æ§çº¿ç¨‹å·²åœæ­¢")
                        break
                        
                    print(f"    ç­‰å¾…ä¸­ï¼Œå‰©ä½™æ´»è·ƒçº¿ç¨‹: {', '.join(active_monitor_threads)}")
                    time.sleep(1)
                else:
                    print(f"  âš ï¸  è¶…æ—¶ï¼šä»æœ‰ç›‘æ§çº¿ç¨‹æœªç»“æŸï¼Œå¼ºåˆ¶ç»§ç»­æ¸…ç†")
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼Œæ¸…ç†å¯èƒ½çš„èµ„æº
            import gc
            gc.collect()
                   
            # å¤šæ¬¡å°è¯•åœæ­¢è¿›ç¨‹
            max_attempts = 5
            for attempt in range(max_attempts):
                # é¦–å…ˆå°è¯•æ­£å¸¸åœæ­¢
                subprocess.run('pkill -15 -f "^taosd.*-c.*conf"', shell=True)
                time.sleep(1)
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰è¿›ç¨‹
                result = subprocess.run('pgrep -f "^taosd.*-c.*conf"', shell=True, capture_output=True)
                if result.returncode != 0:  # æ²¡æœ‰æ‰¾åˆ°è¿›ç¨‹
                    print("  âœ… æ‰€æœ‰taosdè¿›ç¨‹å·²åœæ­¢")
                    break
                
                # å¼ºåˆ¶åœæ­¢
                subprocess.run('pkill -9 -f "^taosd.*-c.*conf"', shell=True)
                time.sleep(1)
                
                # å†æ¬¡æ£€æŸ¥
                result = subprocess.run('pgrep -f "^taosd.*-c.*conf"', shell=True, capture_output=True)
                if result.returncode != 0:
                    print("  âœ… æ‰€æœ‰taosdè¿›ç¨‹å·²åœæ­¢")
                    break
                else:
                    print(f"  âš ï¸  ç¬¬{attempt+1}æ¬¡å°è¯•: ä»æœ‰taosdè¿›ç¨‹è¿è¡Œï¼Œç»§ç»­æ¸…ç†...")
        
            # âœ… 2. æ¸…ç†å¯èƒ½çš„ç›‘æ§çº¿ç¨‹å’Œèµ„æº
            print("  â†’ å¼ºåˆ¶æ¸…ç†ç›‘æ§èµ„æº")
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©æ‰€æœ‰çº¿ç¨‹ç»“æŸ
            time.sleep(2)
            
            # âœ… 3. æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œsocketæ–‡ä»¶
            print("  â†’ æ¸…ç†ä¸´æ—¶æ–‡ä»¶")
            temp_patterns = [
                '/tmp/stream_from*.json',
                '/tmp/taosBenchmark_result.log',
                '/tmp/testlog/*',
            ]
            
            # å•ç‹¬å¤„ç†å¯èƒ½æœ‰é—®é¢˜çš„æ–‡ä»¶æ¨¡å¼
            socket_patterns = [
                '/tmp/*.sock*',
                '/tmp/taos*.sock'
            ]
            
            for pattern in temp_patterns:
                try:
                    subprocess.run(f'rm -f {pattern}', shell=True)
                except:
                    pass
                    
            # å°å¿ƒå¤„ç†socketæ–‡ä»¶ï¼Œé¿å…åˆ é™¤é‡è¦æ–‡ä»¶
            for pattern in socket_patterns:
                try:
                    # ä½¿ç”¨findå‘½ä»¤æ›´å®‰å…¨åœ°åˆ é™¤
                    subprocess.run(f'find /tmp -name "{pattern.split("/")[-1]}" -type f -delete 2>/dev/null', shell=True)
                except:
                    pass
            
            # 4. æœ€ç»ˆéªŒè¯
            print("  â†’ éªŒè¯æ¸…ç†ç»“æœ")
            result = subprocess.run('ps -ef | grep "taosd.*-c.*conf" | grep -v grep', 
                                    shell=True, capture_output=True, text=True)
            if result.stdout:
                print(f"  âš ï¸  è­¦å‘Š: å‘ç°æ®‹ç•™è¿›ç¨‹:")
                for line in result.stdout.strip().split('\n'):
                    print(f"       {line}")
                # æœ€åä¸€æ¬¡å¼ºåˆ¶æ¸…ç†
                subprocess.run('pkill -9 -f "^taosd.*-c.*conf"', shell=True)
                time.sleep(1)
            else:
                print(f"  âœ… ç¯å¢ƒæ¸…ç†éªŒè¯é€šè¿‡")
            
            # 5. æ£€æŸ¥å’ŒæŠ¥å‘Šæ´»è·ƒçº¿ç¨‹çŠ¶æ€
            print("  â†’ æ£€æŸ¥æ´»è·ƒçº¿ç¨‹çŠ¶æ€")
            active_monitor_threads = []
            for thread in threading.enumerate():
                if thread.name in ["MonitorSystemLoad", "TaosdMonitor", "StreamDelayMonitor", "DelayedStreamCreation", "ContinuousDataWriter"]:
                    active_monitor_threads.append(thread.name)
            
            if active_monitor_threads:
                print(f"  âš ï¸  è­¦å‘Š: ä»æœ‰æ´»è·ƒçš„ç›‘æ§çº¿ç¨‹: {', '.join(active_monitor_threads)}")
                print(f"    è¿™äº›çº¿ç¨‹åœ¨ä¸‹æ¬¡æµ‹è¯•æ—¶ä¼šè‡ªåŠ¨åœæ­¢")
            else:
                print(f"  âœ… æ‰€æœ‰ç›‘æ§çº¿ç¨‹å·²æ¸…ç†")
                
            total_active_threads = threading.active_count()
            print(f"  â†’ å½“å‰æ€»æ´»è·ƒçº¿ç¨‹æ•°: {total_active_threads}")
            
            print("å¢å¼ºç¯å¢ƒæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            print(f"å¢å¼ºç¯å¢ƒæ¸…ç†æ—¶å‡ºé”™: {str(e)}")
    
    def save_test_config(self, config, result_dir):
        """ä¿å­˜æµ‹è¯•é…ç½®åˆ°æ–‡ä»¶"""
        config_file = os.path.join(result_dir, 'configs', f"{config['test_name']}_config.json")
        
        # åˆ›å»ºå¯åºåˆ—åŒ–çš„é…ç½®å‰¯æœ¬
        serializable_config = {}
        for key, value in config.items():
            if isinstance(value, (str, int, float, bool, list, dict, type(None))):
                serializable_config[key] = value
            else:
                serializable_config[key] = str(value)
        
        with open(config_file, 'w') as f:
            json.dump(serializable_config, f, indent=2)
    
    def generate_progress_report(self, result_dir):
        """ç”Ÿæˆè¿›åº¦æŠ¥å‘Š"""
        if not self.test_results:
            return
            
        report_file = os.path.join(result_dir, 'reports', 'progress_report.txt')
        
        with open(report_file, 'w') as f:
            f.write(f"æ‰¹é‡æµè®¡ç®—æµ‹è¯•è¿›åº¦æŠ¥å‘Š\n")
            f.write(f"{'='*60}\n")
            f.write(f"å¼€å§‹æ—¶é—´: {self.start_time}\n")
            f.write(f"å½“å‰æ—¶é—´: {datetime.datetime.now().isoformat()}\n")
            f.write(f"æ€»æµ‹è¯•æ•°: {self.total_tests}\n")
            f.write(f"å·²å®Œæˆ: {len(self.test_results)}\n")
            f.write(f"æˆåŠŸ: {len([r for r in self.test_results if r['status'] == 'SUCCESS'])}\n")
            f.write(f"å¤±è´¥: {len([r for r in self.test_results if r['status'] == 'FAILED'])}\n")
            f.write(f"è¿›åº¦: {(len(self.test_results)/self.total_tests)*100:.1f}%\n")
            f.write(f"\næœ€è¿‘å®Œæˆçš„æµ‹è¯•:\n")
            
            for result in self.test_results[-5:]:  # æ˜¾ç¤ºæœ€è¿‘5ä¸ª
                status_icon = "âœ“" if result['status'] == 'SUCCESS' else "âœ—"
                f.write(f"  {status_icon} {result['test_name']} - {result.get('duration', 0):.1f}s\n")
                
    
    def generate_final_report_bak(self, result_dir):
        """ç”Ÿæˆæœ€ç»ˆæµ‹è¯•æŠ¥å‘Š"""
        report_file = os.path.join(result_dir, 'reports', 'final_report.html')
        
        success_count = len([r for r in self.test_results if r['status'] == 'SUCCESS'])
        failed_count = len([r for r in self.test_results if r['status'] == 'FAILED'])
        total_duration = sum(r.get('duration', 0) for r in self.test_results)
        
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>æµè®¡ç®—æ‰¹é‡æµ‹è¯•æŠ¥å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
        .summary {{ display: flex; justify-content: space-around; margin: 20px 0; }}
        .metric {{ text-align: center; padding: 10px; background-color: #e9e9e9; border-radius: 5px; }}
        .success {{ color: green; }}
        .failed {{ color: red; }}
        table {{ width: 100%; border-collapse: collapse; margin-top: 20px; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
        .status-success {{ background-color: #d4edda; }}
        .status-failed {{ background-color: #f8d7da; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>TDengine æµè®¡ç®—æ‰¹é‡æµ‹è¯•æŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        <p>æµ‹è¯•å¼€å§‹: {self.start_time}</p>
        <p>æµ‹è¯•ç»“æŸ: {datetime.datetime.now().isoformat()}</p>
    </div>
    
    <div class="summary">
        <div class="metric">
            <h3>æ€»æµ‹è¯•æ•°</h3>
            <h2>{self.total_tests}</h2>
        </div>
        <div class="metric success">
            <h3>æˆåŠŸ</h3>
            <h2>{success_count}</h2>
        </div>
        <div class="metric failed">
            <h3>å¤±è´¥</h3>
            <h2>{failed_count}</h2>
        </div>
        <div class="metric">
            <h3>æˆåŠŸç‡</h3>
            <h2>{(success_count/self.total_tests*100):.1f}%</h2>
        </div>
        <div class="metric">
            <h3>æ€»è€—æ—¶</h3>
            <h2>{total_duration/3600:.1f}å°æ—¶</h2>
        </div>
    </div>
    
    <h2>è¯¦ç»†æµ‹è¯•ç»“æœ</h2>
    <table>
        <tr>
            <th>æµ‹è¯•åç§°</th>
            <th>SQLç±»å‹</th>
            <th>æŸ¥è¯¢ç±»å‹</th>
            <th>FROMç±»å‹</th>
            <th>çŠ¶æ€</th>
            <th>è€—æ—¶(ç§’)</th>
            <th>é”™è¯¯ä¿¡æ¯</th>
        </tr>
"""
        
        for result in self.test_results:
            config = result['config']
            status_class = 'status-success' if result['status'] == 'SUCCESS' else 'status-failed'
            error_msg = result.get('error', '')[:100] if result.get('error') else ''
            
            html_content += f"""
        <tr class="{status_class}">
            <td>{result['test_name']}</td>
            <td>{config['sql_type']}</td>
            <td>{config['agg_or_select']}</td>
            <td>{config['tbname_or_trows_or_sourcetable']}</td>
            <td>{result['status']}</td>
            <td>{result.get('duration', 0):.1f}</td>
            <td>{error_msg}</td>
        </tr>
"""
        
        html_content += """
    </table>
</body>
</html>
"""
        
        with open(report_file, 'w') as f:
            f.write(html_content)
        
        print(f"æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")


    def generate_final_report(self, result_dir):
        """ç”Ÿæˆæœ€ç»ˆæµ‹è¯•æŠ¥å‘Š"""
        report_file = os.path.join(result_dir, 'reports', 'final_report.html')
        
        success_count = len([r for r in self.test_results if r['status'] == 'SUCCESS'])
        failed_count = len([r for r in self.test_results if r['status'] == 'FAILED'])
        skipped_count = len([r for r in self.test_results if r['status'] == 'SKIPPED'])
        total_duration = sum(r.get('duration', 0) for r in self.test_results)
        current_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æµ‹è¯•ç»“æœ
        if not self.test_results:
            print("è­¦å‘Š: æ²¡æœ‰æµ‹è¯•ç»“æœå¯ä»¥ç”ŸæˆæŠ¥å‘Š")
            # ç”Ÿæˆä¸€ä¸ªç®€å•çš„ç©ºæŠ¥å‘Š
            report_file = os.path.join(result_dir, 'reports', 'final_report.html')
            try:
                os.makedirs(os.path.dirname(report_file), exist_ok=True)
                with open(report_file, 'w', encoding='utf-8') as f:
                    f.write(f"""
<!DOCTYPE html>
<html>
<head>
    <title>æµè®¡ç®—æ‰¹é‡æµ‹è¯•æŠ¥å‘Š - æ— æµ‹è¯•ç»“æœ</title>
    <meta charset="UTF-8">
</head>
<body>
    <h1>æµè®¡ç®—æ‰¹é‡æµ‹è¯•æŠ¥å‘Š</h1>
    <p>ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    <p><strong>è­¦å‘Š:</strong> æ²¡æœ‰æµ‹è¯•ç»“æœå¯ä»¥æ˜¾ç¤ºã€‚</p>
    <p>å¯èƒ½çš„åŸå› :</p>
    <ul>
        <li>è¿‡æ»¤è®¾ç½®å¯¼è‡´æ‰€æœ‰æµ‹è¯•éƒ½è¢«è·³è¿‡</li>
        <li>æµ‹è¯•é…ç½®é”™è¯¯</li>
        <li>æµ‹è¯•æå‰ç»ˆæ­¢</li>
    </ul>
    <p>è¯·æ£€æŸ¥è¿‡æ»¤æ¨¡å¼å’ŒSQLç±»å‹è®¾ç½®ã€‚</p>
</body>
</html>
""")
                print(f"ç©ºæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
            except Exception as e:
                print(f"ç”Ÿæˆç©ºæŠ¥å‘Šå¤±è´¥: {str(e)}")
            return
        
        report_file = os.path.join(result_dir, 'reports', 'final_report.html')
        
        # é˜²æ­¢é™¤é›¶é”™è¯¯
        total_tests = len(self.test_results)
        if total_tests == 0:
            success_rate = 0
            avg_duration = 0
            efficiency = 0
        else:
            success_rate = (success_count / total_tests) * 100
            avg_duration = total_duration / total_tests / 60
            efficiency = total_tests / (total_duration / 3600) if total_duration > 0 else 0
        
        # è·å–ç³»ç»Ÿä¿¡æ¯
        def get_system_info():
            """è·å–ç³»ç»ŸCPUæ ¸æ•°å’Œæ€»å†…å­˜"""
            try:
                # è·å–CPUæ ¸æ•°
                cpu_count = None
                try:
                    result = subprocess.run('lscpu | grep "^CPU(s):"', shell=True, 
                                        capture_output=True, text=True)
                    if result.returncode == 0:
                        cpu_line = result.stdout.strip()
                        # è§£æ "CPU(s):                             16" æ ¼å¼
                        cpu_count = cpu_line.split(':')[1].strip()
                        print(f"è°ƒè¯•: è·å–åˆ°CPUæ ¸æ•°: {cpu_count}")
                except Exception as e:
                    print(f"è°ƒè¯•: è·å–CPUæ ¸æ•°å¤±è´¥: {str(e)}")
                
                # è·å–æ€»å†…å­˜
                total_memory_gb = None
                try:
                    result = subprocess.run('free -m | grep "^Mem:"', shell=True, 
                                        capture_output=True, text=True)
                    if result.returncode == 0:
                        mem_line = result.stdout.strip()
                        # è§£æ "Mem:           15944        1234        5678        ..." æ ¼å¼
                        total_memory_mb = int(mem_line.split()[1])
                        total_memory_gb = f"{total_memory_mb/1024:.1f}GB"
                        print(f"è°ƒè¯•: è·å–åˆ°æ€»å†…å­˜: {total_memory_gb}")
                except Exception as e:
                    print(f"è°ƒè¯•: è·å–æ€»å†…å­˜å¤±è´¥: {str(e)}")
                    
                return cpu_count, total_memory_gb
            except Exception as e:
                print(f"è°ƒè¯•: è·å–ç³»ç»Ÿä¿¡æ¯å¤±è´¥: {str(e)}")
                return None, None
        
        system_cpu_count, system_total_memory = get_system_info()
        
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>æµè®¡ç®—æ‰¹é‡æµ‹è¯•è¯¦ç»†æŠ¥å‘Š</title>
    <meta charset="UTF-8">
    <style>
        body {{ font-family: 'Segoe UI', Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px; margin-bottom: 20px; }}
        .summary {{ display: flex; justify-content: space-around; margin: 20px 0; flex-wrap: wrap; }}
        .metric {{ text-align: center; padding: 20px; background-color: white; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); min-width: 150px; margin: 5px; }}
        .metric h3 {{ margin: 0 0 10px 0; color: #666; font-size: 14px; }}
        .metric h2 {{ margin: 0; font-size: 24px; }}
        .success {{ color: #28a745; }}
        .failed {{ color: #dc3545; }}
        .warning {{ color: #ffc107; }}
        .info {{ color: #17a2b8; }}
        
        /* è¡¨æ ¼æ ·å¼ */
        .table-container {{ background: white; border-radius: 10px; overflow: hidden; box-shadow: 0 2px 10px rgba(0,0,0,0.1); margin: 20px 0; }}
        table {{ width: 100%; border-collapse: collapse; }}
        th {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 15px 8px; text-align: left; font-weight: 600; font-size: 12px; }}
        td {{ border: none; padding: 12px 8px; text-align: left; border-bottom: 1px solid #eee; font-size: 11px; }}
        tr:hover {{ background-color: #f8f9fa; }}
        .status-success {{ background-color: #d4edda !important; }}
        .status-failed {{ background-color: #f8d7da !important; }}
        .status-skipped {{ background-color: #f8f9fa !important; }}
        
        /* çŠ¶æ€æ ‡ç­¾ */
        .status-badge {{ padding: 4px 8px; border-radius: 15px; font-size: 10px; font-weight: bold; text-transform: uppercase; }}
        .badge-success {{ background-color: #28a745; color: white; }}
        .badge-failed {{ background-color: #dc3545; color: white; }}
        .badge-skipped {{ background-color: #6c757d; color: white; }} 
        
        /* æ€§èƒ½æŒ‡æ ‡æ ·å¼ */
        .perf-good {{ color: #28a745; font-weight: bold; }}
        .perf-warning {{ color: #ffc107; font-weight: bold; }}
        .perf-danger {{ color: #dc3545; font-weight: bold; }}
        
        /* SQLå±•ç¤ºæ ·å¼ */
        .sql-preview {{ 
            font-family: 'Courier New', monospace; 
            font-size: 10px; 
            background-color: #f8f9fa; 
            padding: 5px; 
            border-radius: 3px; 
            max-width: 300px; 
            overflow: hidden; 
            text-overflow: ellipsis; 
            white-space: nowrap;
            cursor: pointer;
            border: 1px solid #dee2e6;
        }}
        .sql-preview:hover {{ background-color: #e9ecef; }}
        
        /* å»¶è¿ŸæŒ‡æ ‡æ ·å¼ */
        .delay-excellent {{ color: #28a745; font-weight: bold; }}
        .delay-good {{ color: #20c997; font-weight: bold; }}
        .delay-normal {{ color: #ffc107; font-weight: bold; }}
        .delay-warning {{ color: #fd7e14; font-weight: bold; }}
        .delay-danger {{ color: #dc3545; font-weight: bold; }}
        .delay-critical {{ color: #6f42c1; font-weight: bold; }}
        
        /* å·¥å…·æç¤º */
        .tooltip {{ position: relative; display: inline-block; }}
        .tooltip .tooltiptext {{ 
            visibility: hidden; 
            width: 400px; 
            background-color: #333; 
            color: #fff; 
            text-align: left; 
            border-radius: 6px; 
            padding: 10px; 
            position: absolute; 
            z-index: 1; 
            bottom: 125%; 
            left: 50%; 
            margin-left: -200px; 
            opacity: 0; 
            transition: opacity 0.3s; 
            font-family: 'Courier New', monospace; 
            font-size: 10px; 
            white-space: pre-wrap; 
        }}
        .tooltip:hover .tooltiptext {{ visibility: visible; opacity: 1; }}
        
        /* ç­›é€‰å’Œæœç´¢ */
        .controls {{ margin: 20px 0; padding: 15px; background: white; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        .filter-group {{ display: inline-block; margin-right: 20px; }}
        .filter-group label {{ font-weight: bold; margin-right: 5px; }}
        .filter-group select, .filter-group input {{ padding: 5px; border: 1px solid #ddd; border-radius: 4px; }}
        
        /* å“åº”å¼è®¾è®¡ */
        @media (max-width: 1200px) {{
            th, td {{ font-size: 10px; padding: 8px 4px; }}
            .sql-preview {{ max-width: 200px; }}
        }}
    </style>
    <script>
        function filterTable() {{
            const statusFilter = document.getElementById('statusFilter').value;
            const sqlTypeFilter = document.getElementById('sqlTypeFilter').value;
            const searchTerm = document.getElementById('searchInput').value.toLowerCase();
            
            const table = document.getElementById('resultsTable');
            const rows = table.getElementsByTagName('tr');
            
            for (let i = 1; i < rows.length; i++) {{
                const row = rows[i];
                const status = row.cells[4].textContent;
                const sqlType = row.cells[1].textContent;
                const testName = row.cells[0].textContent.toLowerCase();
                
                let showRow = true;
                
                if (statusFilter && !status.includes(statusFilter)) showRow = false;
                if (sqlTypeFilter && sqlType !== sqlTypeFilter) showRow = false;
                if (searchTerm && !testName.includes(searchTerm)) showRow = false;
                
                row.style.display = showRow ? '' : 'none';
            }}
        }}
        
        function sortTable(columnIndex) {{
            const table = document.getElementById('resultsTable');
            const rows = Array.from(table.rows).slice(1);
            const isNumeric = columnIndex === 5 || columnIndex === 7 || columnIndex === 8; // è€—æ—¶ã€CPUã€å†…å­˜åˆ—
            
            rows.sort((a, b) => {{
                const aVal = a.cells[columnIndex].textContent;
                const bVal = b.cells[columnIndex].textContent;
                
                if (isNumeric) {{
                    return parseFloat(aVal) - parseFloat(bVal);
                }} else {{
                    return aVal.localeCompare(bVal);
                }}
            }});
            
            rows.forEach(row => table.appendChild(row));
        }}
    </script>
</head>
<body>
    <div class="header">
        <h1>ğŸš€ TDengine æµè®¡ç®—æ‰¹é‡æµ‹è¯•è¯¦ç»†æŠ¥å‘Š</h1>
        <p>ğŸ“… ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        <p>â° æµ‹è¯•å‘¨æœŸ: {self.start_time} â†’ {datetime.datetime.now().isoformat()}</p>
        <p>ğŸ—ï¸ æµ‹è¯•é…ç½®: {self.fixed_params.get('deployment_mode', 'single')}æ¨¡å¼ | 
           {self.fixed_params.get('table_count', 1000)}å¼ è‡ªè¡¨ | 
           {self.fixed_params.get('real_time_batch_rows', 200)}æ¡/è½® | 
           æ¯è½®å†™å…¥é—´éš”{self.fixed_params.get('real_time_batch_sleep', 0)}ç§’</p>
        <p>ğŸ’» æµ‹è¯•ç¯å¢ƒ: CPUæ ¸æ•°{system_cpu_count or 'æœªçŸ¥'} | æ€»å†…å­˜{system_total_memory or 'æœªçŸ¥'}</p>
    </div>
    
    <div class="summary">
        <div class="metric">
            <h3>ğŸ“Š æ€»æµ‹è¯•æ•°</h3>
            <h2>{self.total_tests}</h2>
        </div>
        <div class="metric success">
            <h3>âœ… æˆåŠŸ</h3>
            <h2>{success_count}</h2>
            <small>{(success_count/self.total_tests*100):.1f}%</small>
        </div>
        <div class="metric failed">
            <h3>âŒ åˆ›å»ºæµè¯­å¥å¤±è´¥ï¼Œéæµè®¡ç®—å¤±è´¥</h3>
            <h2>{failed_count}</h2>
            <small>{(failed_count/self.total_tests*100):.1f}%</small>
        </div>
        <div class="metric skipped">
            <h3>â­ï¸ è·³è¿‡(å·²çŸ¥å¤±è´¥)</h3>
            <h2>{skipped_count}</h2>
            <small>{(skipped_count/total_tests*100):.1f}%</small>
        </div>
        <div class="metric info">
            <h3>â±ï¸ æ€»è€—æ—¶</h3>
            <h2>{total_duration/3600:.1f}h</h2>
            <small>å¹³å‡{total_duration/self.total_tests/60:.1f}åˆ†é’Ÿ/æµ‹è¯•</small>
        </div>
        <div class="metric info">
            <h3>ğŸ’ª æ•ˆç‡</h3>
            <h2>{self.total_tests/(total_duration/3600):.1f}</h2>
            <small>æµ‹è¯•åœºæ™¯ä¸ªæ•°/å°æ—¶</small>
        </div>
    </div>
    
    <div class="controls">
        <div class="filter-group">
            <label for="statusFilter">çŠ¶æ€ç­›é€‰:</label>
            <select id="statusFilter" onchange="filterTable()">
                <option value="">å…¨éƒ¨</option>
                <option value="SUCCESS">æˆåŠŸ</option>
                <option value="FAILED">å¤±è´¥</option>
                <option value="SKIPPED">è·³è¿‡</option>
            </select>
        </div>
        <div class="filter-group">
            <label for="sqlTypeFilter">SQLç±»å‹ç­›é€‰:</label>
            <select id="sqlTypeFilter" onchange="filterTable()">
                <option value="">å…¨éƒ¨</option>
"""
        
        # è·å–æ‰€æœ‰SQLç±»å‹ç”¨äºç­›é€‰
        sql_types = set()
        for result in self.test_results:
            sql_types.add(result['config']['sql_type'])
        
        for sql_type in sorted(sql_types):
            html_content += f'                <option value="{sql_type}">{sql_type}</option>\n'
        
        html_content += f"""
            </select>
        </div>
        <div class="filter-group">
            <label for="searchInput">æœç´¢æµ‹è¯•åç§°:</label>
            <input type="text" id="searchInput" placeholder="è¾“å…¥å…³é”®è¯..." onkeyup="filterTable()">
        </div>
    </div>
    
    <div class="table-container">
        <h2 style="margin: 0; padding: 20px; border-bottom: 1px solid #eee;">ğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ</h2>
        <table id="resultsTable">
            <tr>
                <th onclick="sortTable(0)" style="cursor: pointer;">ğŸ“ æµ‹è¯•åç§°</th>
                <th onclick="sortTable(1)" style="cursor: pointer;">ğŸ”§ SQLç±»å‹</th>
                <th onclick="sortTable(2)" style="cursor: pointer;">ğŸ“Š æŸ¥è¯¢ç±»å‹</th>
                <th onclick="sortTable(3)" style="cursor: pointer;">ğŸ“‚ FROMç±»å‹</th>
                <th onclick="sortTable(4)" style="cursor: pointer;">ğŸ¯ çŠ¶æ€</th>
                <th onclick="sortTable(5)" style="cursor: pointer;">â±ï¸ è€—æ—¶(ç§’)</th>
                <th>ğŸ” SQLé¢„è§ˆ</th>
                <th onclick="sortTable(7)" style="cursor: pointer;">ğŸ’» CPUå³°å€¼èŒƒå›´(%)</th>
                <th onclick="sortTable(8)" style="cursor: pointer;">ğŸ§  å†…å­˜å³°å€¼èŒƒå›´(MB)</th>
                <th onclick="sortTable(9)" style="cursor: pointer;">ğŸ“Š CPUå¹³å‡å€¼(%)</th>
                <th onclick="sortTable(10)" style="cursor: pointer;">ğŸ“ˆ å†…å­˜å¹³å‡å€¼(MB)</th>
                <th>ğŸ“ˆ å»¶è¿Ÿç»Ÿè®¡</th>
                <th>âŒ é”™è¯¯ä¿¡æ¯</th>
            </tr>
"""
        
        for result_index, result in enumerate(self.test_results):
            config = result['config']
            
            if result['status'] == 'SUCCESS':
                status_class = 'status-success'
                status_badge = 'badge-success'
            elif result['status'] == 'FAILED':
                status_class = 'status-failed'
                status_badge = 'badge-failed'
            elif result['status'] == 'SKIPPED':
                status_class = 'status-skipped'
                status_badge = 'badge-skipped'
            else:
                status_class = ''
                status_badge = ''
            
            print(f"è°ƒè¯•: å¤„ç†ç¬¬ {result_index + 1}/{len(self.test_results)} ä¸ªæµ‹è¯•ç»“æœ: {result['test_name']}")
            
            # è·å–SQLé¢„è§ˆ
            sql_preview = "N/A"
            sql_full = "æ— SQLä¿¡æ¯"
            try:
                # ä»StreamSQLTemplatesè·å–SQL
                sql_obj = StreamSQLTemplates.get_sql(
                    config['sql_type'],
                    agg_or_select=config.get('agg_or_select', 'agg'),
                    tbname_or_trows_or_sourcetable=config.get('tbname_or_trows_or_sourcetable', 'sourcetable')
                )
                
                if isinstance(sql_obj, dict):
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œå–ç¬¬ä¸€ä¸ªSQLä½œä¸ºé¢„è§ˆ
                    first_key = list(sql_obj.keys())[0]
                    sql_full = sql_obj[first_key]
                    sql_preview = f"æ‰¹é‡({len(sql_obj)}ä¸ª) - {first_key}"
                else:
                    sql_full = str(sql_obj)
                    # æ ¼å¼åŒ–SQLç”¨äºæ˜¾ç¤º
                    formatted_sql = format_sql_for_display(sql_full)
                    
                    # æå–æµåç§°ä½œä¸ºé¢„è§ˆ
                    import re
                    match = re.search(r'create\s+stream\s+([^\s|]+)', formatted_sql, re.IGNORECASE)
                    if match:
                        stream_name = match.group(1)
                        # åˆ›å»ºç®€æ´çš„é¢„è§ˆï¼Œæ˜¾ç¤ºæµåç§°å’Œä¸»è¦å­å¥
                        preview_parts = []
                        preview_parts.append(f"CREATE STREAM {stream_name}")
                        
                        # æå–çª—å£ç±»å‹
                        if 'interval(' in formatted_sql.lower():
                            interval_match = re.search(r'interval\([^)]+\)', formatted_sql, re.IGNORECASE)
                            if interval_match:
                                preview_parts.append(interval_match.group(0).upper())
                        elif 'sliding(' in formatted_sql.lower():
                            sliding_match = re.search(r'sliding\([^)]+\)', formatted_sql, re.IGNORECASE)
                            if sliding_match:
                                preview_parts.append(sliding_match.group(0).upper())
                        elif 'session(' in formatted_sql.lower():
                            session_match = re.search(r'session\([^)]+\)', formatted_sql, re.IGNORECASE)
                            if session_match:
                                preview_parts.append(session_match.group(0).upper())
                        elif 'period(' in formatted_sql.lower():
                            period_match = re.search(r'period\([^)]+\)', formatted_sql, re.IGNORECASE)
                            if period_match:
                                preview_parts.append(period_match.group(0).upper())
                        elif 'count_window(' in formatted_sql.lower():
                            count_match = re.search(r'count_window\([^)]+\)', formatted_sql, re.IGNORECASE)
                            if count_match:
                                preview_parts.append(count_match.group(0).upper())
                        
                        sql_preview = " ".join(preview_parts)
                    else:
                        # å¦‚æœæ— æ³•æå–æµåç§°ï¼Œæ˜¾ç¤ºå‰50ä¸ªå­—ç¬¦
                        sql_preview = formatted_sql[:50] + "..." if len(formatted_sql) > 50 else formatted_sql
                    
                    # ä½¿ç”¨æ ¼å¼åŒ–åçš„SQLä½œä¸ºå®Œæ•´SQL
                    sql_full = formatted_sql
                        
                        
            except Exception as e:
                sql_preview = f"è·å–å¤±è´¥: {str(e)[:30]}"
                sql_full = f"SQLè·å–é”™è¯¯: {str(e)}"
            
            # æ¸…ç†SQLä¸­çš„ç‰¹æ®Šå­—ç¬¦
            sql_full_escaped = sql_full.replace('"', '&quot;').replace("'", '&#39;').replace('\n', '\\n').replace('\r', '\\r')
            
            # è¯»å–æ€§èƒ½æ•°æ®
            cpu_peak, memory_peak = "N/A", "N/A"
            cpu_range_info = ""
            memory_range_info = ""
            cpu_peak_value = None
            memory_peak_value = None
            cpu_avg_value = None
            memory_avg_value = None
            memory_avg_percentage = None
            cpu_avg_info = ""
            memory_avg_info = ""
            
            
            # åªæœ‰æµ‹è¯•æˆåŠŸæ—¶æ‰å°è¯•è¯»å–æ€§èƒ½æ•°æ®
            if result['status'] == 'SUCCESS':
                try:
                    perf_file = config.get('perf_file', '')
                    print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œå°è¯•è¯»å–æ€§èƒ½æ–‡ä»¶: {perf_file}")
                    
                    # æ£€æŸ¥å„ç§å¯èƒ½çš„æ€§èƒ½æ–‡ä»¶è·¯å¾„
                    possible_files = []
                    if perf_file:
                        # åŸå§‹æ–‡ä»¶è·¯å¾„
                        possible_files.append(perf_file)
                        # å»æ‰æ‰©å±•ååŠ -all.log
                        base_name = os.path.splitext(perf_file)[0]
                        possible_files.append(f"{base_name}-all.log")
                        # ç›´æ¥åŠ -all.log
                        possible_files.append(f"{perf_file}-all.log")
                    
                    # æ·»åŠ ä¸€äº›å¸¸è§çš„æ€§èƒ½æ–‡ä»¶è·¯å¾„
                    possible_files.extend([
                        '/tmp/perf-taosd-all.log',
                        '/tmp/perf-stream-test-all.log', 
                        '/tmp/perf-all.log'
                    ])
                    
                    print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œæ£€æŸ¥æ–‡ä»¶è·¯å¾„: {possible_files}")
                    
                    # å°è¯•è¯»å–ç¬¬ä¸€ä¸ªå­˜åœ¨çš„æ–‡ä»¶
                    perf_content = None
                    used_file = None
                    for file_path in possible_files:
                        if os.path.exists(file_path):
                            print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œæ‰¾åˆ°æ€§èƒ½æ–‡ä»¶: {file_path}")
                            try:
                                with open(file_path, 'r') as f:
                                    perf_content = f.read()
                                    used_file = file_path
                                    break
                            except Exception as e:
                                print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œè¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
                                continue
                    
                    if perf_content:
                        print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼ŒæˆåŠŸè¯»å–æ€§èƒ½æ–‡ä»¶ {used_file}, å†…å®¹é•¿åº¦: {len(perf_content)}")
                        
                        # è§£ææ€§èƒ½æ•°æ®
                        cpu_values = []
                        memory_values = []
                        memory_percentages = []
                        
                        lines = perf_content.split('\n')
                        print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œæ–‡ä»¶æ€»è¡Œæ•°: {len(lines)}")
                        
                        valid_lines_count = 0
                        warm_up_lines_count = 0  
                        formal_monitoring_started = False 
                        warm_up_end_detected = False 
                    
                        for line_num, line in enumerate(lines):
                            line = line.strip()
                            if not line:
                                continue
                            
                            # æ£€æŸ¥æ˜¯å¦æ˜¯é¢„çƒ­æœŸç»“æŸæ ‡è®°
                            if '=== é¢„çƒ­æœŸç»“æŸï¼Œæ­£å¼ç›‘æ§å¼€å§‹ ===' in line or 'æ­£å¼ç›‘æ§å¼€å§‹' in line:
                                formal_monitoring_started = True
                                warm_up_end_detected = True
                                print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œåœ¨ç¬¬{line_num+1}è¡Œå‘ç°æ­£å¼ç›‘æ§å¼€å§‹æ ‡è®°")
                                continue
                                
                            # æŸ¥æ‰¾åŒ…å«CPUå’ŒMemoryçš„è¡Œ
                            # æ ¼å¼: 2025-09-01 17:34:18 [dnode1] CPU: 271.4%, Memory: 787.64MB (1.23%), Read: 0.00MB (7492031), Write: 665.98MB (13546146)
                            if 'CPU:' in line and 'Memory:' in line and '%' in line and 'MB' in line:
                                
                                # æ£€æŸ¥æ˜¯å¦æ˜¯é¢„çƒ­æœŸæ•°æ®
                                is_warm_up_data = '[é¢„çƒ­æœŸæ•°æ®]' in line
                                
                                if is_warm_up_data:
                                    warm_up_lines_count += 1
                                    # è·³è¿‡é¢„çƒ­æœŸæ•°æ®ï¼Œä¸ç»Ÿè®¡
                                    continue
                                
                                # æ–°å¢é€»è¾‘ï¼šå¦‚æœè®¾ç½®äº†é¢„çƒ­æ—¶é—´ä½†æ²¡æœ‰æ£€æµ‹åˆ°é¢„çƒ­æœŸç»“æŸæ ‡è®°
                                # åˆ™æ ¹æ®æ—¶é—´æˆ³åˆ¤æ–­æˆ–è€…æ”¾å®½ç»Ÿè®¡æ¡ä»¶
                                if config.get('monitor_warm_up_time', 0) > 0:
                                    if not warm_up_end_detected and not formal_monitoring_started:
                                        # æ ¹æ®è¡Œæ•°åˆ¤æ–­ï¼šå‰é¢çš„è¡Œå¯èƒ½æ˜¯é¢„çƒ­æœŸæ•°æ®
                                        warm_up_cycles = config.get('monitor_warm_up_time', 0) // config.get('monitor_interval', 5)
                                        # å¯èƒ½æ˜¯æ ‡è®°æ ¼å¼é—®é¢˜ï¼Œå°è¯•æ›´å®½æ¾çš„åŒ¹é…
                                        if warm_up_lines_count > 0:
                                            # å·²ç»å¼€å§‹æœ‰é¢„çƒ­æœŸæ•°æ®ï¼Œç»§ç»­ç­‰å¾…ç»“æŸæ ‡è®°
                                            continue
                                        elif line_num < 10:
                                            # å‰10è¡Œå¯èƒ½è¿˜åœ¨é¢„çƒ­æœŸ
                                            continue
                                        else:
                                            # è¶…è¿‡10è¡Œåï¼Œå¦‚æœæ²¡æœ‰é¢„çƒ­æœŸæ ‡è®°ï¼Œå¯èƒ½æ˜¯æ­£å¼æ•°æ®
                                            formal_monitoring_started = True
                                            print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œæœªæ‰¾åˆ°æ˜ç¡®çš„é¢„çƒ­æœŸç»“æŸæ ‡è®°ï¼Œä»ç¬¬{line_num+1}è¡Œå¼€å§‹ç»Ÿè®¡")
                                
                                # ç»Ÿè®¡æ­£å¼ç›‘æ§æœŸçš„æ•°æ®
                                valid_lines_count += 1
                                
                                try:
                                    import re
                                    
                                    # æå–CPUä½¿ç”¨ç‡
                                    cpu_match = re.search(r'CPU:\s*([\d.]+)%', line)
                                    if cpu_match:
                                        cpu_value = float(cpu_match.group(1))
                                        cpu_values.append(cpu_value)
                                        #print(f"è°ƒè¯•: æå–CPUå€¼: {cpu_value}")
                                    
                                    # æå–å†…å­˜ä½¿ç”¨é‡(MB)
                                    memory_match = re.search(r'Memory:\s*([\d.]+)MB\s*\(([\d.]+)%\)', line)
                                    if memory_match:
                                        memory_value = float(memory_match.group(1))
                                        memory_percentage = float(memory_match.group(2))
                                        memory_values.append(memory_value)
                                        memory_percentages.append(memory_percentage)
                                        #print(f"è°ƒè¯•: æå–å†…å­˜å€¼: {memory_value}")
                                        
                                except Exception as e:
                                    print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œè§£æç¬¬{line_num+1}è¡Œå‡ºé”™: {str(e)}")
                                    continue
                        
                        print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œé¢„çƒ­æœŸæ•°æ®è¡Œæ•°: {warm_up_lines_count}")
                        print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œæ­£å¼ç›‘æ§æœŸæ•°æ®è¡Œæ•°: {valid_lines_count}")
                        print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œé¢„çƒ­æœŸç»“æŸæ£€æµ‹: {warm_up_end_detected}")
                        print(f"è°ƒè¯•: æ”¶é›†åˆ°CPUå³°å€¼: {max(cpu_values) if cpu_values else 'N/A'}")
                        print(f"è°ƒè¯•: æ”¶é›†åˆ°å†…å­˜å³°å€¼: {max(memory_values) if memory_values else 'N/A'}")
                        
                        if cpu_values:
                            cpu_peak_value = max(cpu_values)
                            cpu_min_value = min(cpu_values)
                            cpu_avg_value = sum(cpu_values) / len(cpu_values) 
                            
                            # æ„å»ºCPUèŒƒå›´ä¿¡æ¯ï¼ŒåŒ…å«ç³»ç»Ÿæ ¸æ•°
                            cpu_core_info = f"(CPU(s):{system_cpu_count})" if system_cpu_count else ""
                            cpu_range_info = f"{cpu_min_value:.1f}% -> {cpu_peak_value:.1f}%{cpu_core_info}"
                            cpu_peak = f"{cpu_peak_value:.1f}"
                            cpu_avg_info = f"avg:{cpu_avg_value:.1f}%"
                            
                            print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œæ­£å¼ç›‘æ§æœŸCPUå³°å€¼: {cpu_peak}%, å¹³å‡å€¼: {cpu_avg_value:.1f}%")
                            print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œæ­£å¼ç›‘æ§æœŸCPUèŒƒå›´: {cpu_range_info}")
                            
                        else:
                            print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œæœªæ”¶é›†åˆ°CPUæ•°æ®")
                            cpu_range_info = "æ— æ­£å¼ç›‘æ§æœŸæ•°æ®"
                            cpu_avg_info = ""
                        
                        if memory_values and memory_percentages:
                            memory_peak_value = max(memory_values)
                            memory_min_value = min(memory_values)
                            memory_avg_value = sum(memory_values) / len(memory_values)  # æ–°å¢ï¼šè®¡ç®—å¹³å‡å€¼
                            memory_avg_percentage = sum(memory_percentages) / len(memory_percentages)  # æ–°å¢ï¼šå¹³å‡ç™¾åˆ†æ¯”
                            
                            memory_peak_percentage = memory_percentages[memory_values.index(memory_peak_value)]
                            memory_min_percentage = memory_percentages[memory_values.index(memory_min_value)]
                            
                            memory_peak = f"{memory_peak_value:.0f}"
                            
                            # æ„å»ºå†…å­˜èŒƒå›´ä¿¡æ¯ï¼ŒåŒ…å«æ€»å†…å­˜å’Œç™¾åˆ†æ¯”
                            memory_total_info = f"({system_total_memory})" if system_total_memory else ""
                            memory_range_info = f"{memory_min_value:.0f}MB({memory_min_percentage:.1f}%) -> {memory_peak_value:.0f}MB({memory_peak_percentage:.1f}%){memory_total_info}"
                            # æ–°å¢ï¼šå†…å­˜å¹³å‡å€¼ä¿¡æ¯
                            memory_avg_info = f"avg:{memory_avg_value:.0f}MB({memory_avg_percentage:.1f}%){memory_total_info}"
                        
                            print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œæ­£å¼ç›‘æ§æœŸå†…å­˜å³°å€¼: {memory_peak}MB, å¹³å‡å€¼: {memory_avg_value:.0f}MB({memory_avg_percentage:.1f}%)")
                            print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œæ­£å¼ç›‘æ§æœŸå†…å­˜èŒƒå›´: {memory_range_info}")
                            
                        else:
                            print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œæœªæ”¶é›†åˆ°å†…å­˜æ•°æ®")
                            memory_range_info = "æ— æ­£å¼ç›‘æ§æœŸæ•°æ®"
                            memory_avg_value = None
                            memory_avg_percentage = None
                            memory_avg_info = ""
                            
                        # æ•°æ®è´¨é‡æ£€æŸ¥
                        if valid_lines_count == 0:
                            print(f"è­¦å‘Š: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œæœªæ‰¾åˆ°æ­£å¼ç›‘æ§æœŸæ•°æ®")
                            for line_num, line in enumerate(lines):
                                line = line.strip()
                                if not line:
                                    continue
                                    
                                if 'CPU:' in line and 'Memory:' in line and '%' in line and 'MB' in line:
                                    valid_lines_count += 1
                                    
                                    try:
                                        import re
                                        
                                        cpu_match = re.search(r'CPU:\s*([\d.]+)%', line)
                                        if cpu_match:
                                            cpu_value = float(cpu_match.group(1))
                                            cpu_values.append(cpu_value)
                                        
                                        memory_match = re.search(r'Memory:\s*([\d.]+)MB\s*\(([\d.]+)%\)', line)
                                        if memory_match:
                                            memory_value = float(memory_match.group(1))
                                            memory_percentage = float(memory_match.group(2))
                                            memory_values.append(memory_value)
                                            memory_percentages.append(memory_percentage)
                                            
                                    except Exception as e:
                                        continue
                            
                            print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œå…œåº•å¤„ç†åæ”¶é›†åˆ°æ•°æ®è¡Œæ•°: {valid_lines_count}")
                        elif valid_lines_count < 5:
                            print(f"è­¦å‘Š: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œæ­£å¼ç›‘æ§æœŸæ•°æ®ç‚¹è¿‡å°‘({valid_lines_count}ä¸ª)ï¼Œå¯èƒ½å½±å“ç»Ÿè®¡å‡†ç¡®æ€§")
                            
                    else:
                        print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œæœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„æ€§èƒ½æ–‡ä»¶")
                        cpu_range_info = "æ€§èƒ½æ–‡ä»¶ä¸å­˜åœ¨"
                        memory_range_info = "æ€§èƒ½æ–‡ä»¶ä¸å­˜åœ¨"
                        cpu_avg_value = None
                        memory_avg_value = None
                        memory_avg_percentage = None
                        cpu_avg_info = ""
                        memory_avg_info = ""
                        
                except Exception as e:
                    print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œè¯»å–æ€§èƒ½æ•°æ®æ—¶å‡ºé”™: {str(e)}")
                    cpu_range_info = f"è¯»å–å¤±è´¥: {str(e)}"
                    memory_range_info = f"è¯»å–å¤±è´¥: {str(e)}"
                    cpu_avg_value = None
                    memory_avg_value = None
                    memory_avg_percentage = None
                    cpu_avg_info = ""
                    memory_avg_info = ""
            else:
                print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè·³è¿‡æ€§èƒ½æ•°æ®è¯»å–")
                cpu_range_info = "N/A"
                memory_range_info = "N/A"
                cpu_avg_value = None
                memory_avg_value = None
                memory_avg_percentage = None
                cpu_avg_info = ""
                memory_avg_info = ""
            
            
            # CPUå’Œå†…å­˜çš„é¢œè‰²æ ·å¼
            cpu_class = ""
            if cpu_peak != "N/A" and cpu_peak_value is not None:
                try:
                    # è®¡ç®—å®é™…çš„CPUä½¿ç”¨ç‡ç™¾åˆ†æ¯”
                    if system_cpu_count:
                        try:
                            total_cpu_cores = int(system_cpu_count)
                            # è®¡ç®—å®é™…CPUä½¿ç”¨ç‡ï¼šå½“å‰ä½¿ç”¨ç‡ / æ€»æ ¸æ•° 
                            actual_cpu_usage_percent = cpu_peak_value / total_cpu_cores
                            print(f"è°ƒè¯•: CPUå³°å€¼é¢œè‰²è®¡ç®— - å³°å€¼: {cpu_peak_value}%, æ€»æ ¸æ•°: {total_cpu_cores}, å®é™…ä½¿ç”¨ç‡: {actual_cpu_usage_percent:.1f}%")
                            
                            # åŸºäºå®é™…ä½¿ç”¨ç‡åˆ¤æ–­é¢œè‰²
                            if actual_cpu_usage_percent < 30:      # å®é™…ä½¿ç”¨ç‡ä½äº30%
                                cpu_class = "perf-good"
                                print(f"è°ƒè¯•: CPUå³°å€¼é¢œè‰²åˆ¤æ–­ -> ç»¿è‰² (å®é™…ä½¿ç”¨ç‡ {actual_cpu_usage_percent:.1f}% < 30%)")
                            elif actual_cpu_usage_percent < 70:   # å®é™…ä½¿ç”¨ç‡30%-70%
                                cpu_class = "perf-warning"
                                print(f"è°ƒè¯•: CPUå³°å€¼é¢œè‰²åˆ¤æ–­ -> é»„è‰² (å®é™…ä½¿ç”¨ç‡ {actual_cpu_usage_percent:.1f}% åœ¨30%-70%)")
                            else:                                  # å®é™…ä½¿ç”¨ç‡è¶…è¿‡70%
                                cpu_class = "perf-danger"
                                print(f"è°ƒè¯•: CPUå³°å€¼é¢œè‰²åˆ¤æ–­ -> çº¢è‰² (å®é™…ä½¿ç”¨ç‡ {actual_cpu_usage_percent:.1f}% > 70%)")
                                
                        except (ValueError, TypeError) as e:
                            print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œè§£æCPUæ ¸æ•°å¤±è´¥: {e}")
                            if cpu_peak_value < 500:
                                cpu_class = "perf-good"
                            elif cpu_peak_value < 1000:
                                cpu_class = "perf-warning"
                            else:
                                cpu_class = "perf-danger"
                    else:
                        print(f"è°ƒè¯•: æœªè·å–åˆ°CPUæ ¸æ•°ï¼Œä½¿ç”¨åŸå§‹å€¼åˆ¤æ–­")
                        # æ²¡æœ‰è·å–åˆ°CPUæ ¸æ•°ï¼Œä½¿ç”¨æ›´å®½æ¾çš„åˆ¤æ–­æ ‡å‡†
                        # å‡è®¾æ˜¯å¤šæ ¸ç³»ç»Ÿï¼Œé€‚å½“æ”¾å®½æ ‡å‡†
                        if cpu_peak_value < 500:      # å‡è®¾è‡³å°‘16æ ¸ï¼Œ500%ä»¥ä¸‹ä¸ºè‰¯å¥½
                            cpu_class = "perf-good"
                        elif cpu_peak_value < 1000:   # 500%-1000%ä¸ºè­¦å‘Š
                            cpu_class = "perf-warning"
                        else:                  # è¶…è¿‡1000%ä¸ºå±é™©
                            cpu_class = "perf-danger"
                            
                except Exception as e:
                    print(f"è°ƒè¯•:ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼ŒCPUé¢œè‰²åˆ¤æ–­å‡ºé”™: {e}")
                    cpu_class = ""
            else:
                print(f"è°ƒè¯•: CPUæ•°æ®æ— æ•ˆï¼Œè·³è¿‡é¢œè‰²åˆ¤æ–­ - cpu_peak: {cpu_peak}, cpu_peak_value: {cpu_peak_value}")
            
            
            # ========== CPUå¹³å‡å€¼çš„é¢œè‰²æ ·å¼ï¼ˆæ–°å¢ç‹¬ç«‹åˆ¤æ–­ï¼‰ ==========
            cpu_avg_class = ""
            if cpu_avg_value is not None:  # ä½¿ç”¨å¹³å‡å€¼è¿›è¡Œç‹¬ç«‹çš„é¢œè‰²åˆ¤æ–­
                try:
                    if system_cpu_count:
                        try:
                            total_cpu_cores = int(system_cpu_count)
                            # ä½¿ç”¨å¹³å‡å€¼è®¡ç®—å®é™…CPUä½¿ç”¨ç‡
                            actual_cpu_avg_usage_percent = cpu_avg_value / total_cpu_cores
                            print(f"è°ƒè¯•: CPUå¹³å‡å€¼é¢œè‰²è®¡ç®— - å¹³å‡å€¼: {cpu_avg_value:.1f}%, æ€»æ ¸æ•°: {total_cpu_cores}, å®é™…å¹³å‡ä½¿ç”¨ç‡: {actual_cpu_avg_usage_percent:.1f}%")
                            
                            # åŸºäºå¹³å‡ä½¿ç”¨ç‡åˆ¤æ–­é¢œè‰²
                            if actual_cpu_avg_usage_percent < 30:
                                cpu_avg_class = "perf-good"
                                print(f"è°ƒè¯•: CPUå¹³å‡å€¼é¢œè‰²åˆ¤æ–­ -> ç»¿è‰² (å¹³å‡ä½¿ç”¨ç‡ {actual_cpu_avg_usage_percent:.1f}% < 30%)")
                            elif actual_cpu_avg_usage_percent < 70:
                                cpu_avg_class = "perf-warning"
                                print(f"è°ƒè¯•: CPUå¹³å‡å€¼é¢œè‰²åˆ¤æ–­ -> é»„è‰² (å¹³å‡ä½¿ç”¨ç‡ {actual_cpu_avg_usage_percent:.1f}% åœ¨30%-70%)")
                            else:
                                cpu_avg_class = "perf-danger"
                                print(f"è°ƒè¯•: CPUå¹³å‡å€¼é¢œè‰²åˆ¤æ–­ -> çº¢è‰² (å¹³å‡ä½¿ç”¨ç‡ {actual_cpu_avg_usage_percent:.1f}% > 70%)")
                                
                        except (ValueError, TypeError) as e:
                            print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œè§£æCPUæ ¸æ•°å¤±è´¥: {e}")
                            # ä½¿ç”¨å¹³å‡å€¼çš„å¤‡ç”¨åˆ¤æ–­
                            if cpu_avg_value < 500:
                                cpu_avg_class = "perf-good"
                            elif cpu_avg_value < 1000:
                                cpu_avg_class = "perf-warning"
                            else:
                                cpu_avg_class = "perf-danger"
                    else:
                        print(f"è°ƒè¯•: æœªè·å–åˆ°CPUæ ¸æ•°ï¼Œä½¿ç”¨å¹³å‡å€¼åˆ¤æ–­")
                        if cpu_avg_value < 500:
                            cpu_avg_class = "perf-good"
                        elif cpu_avg_value < 1000:
                            cpu_avg_class = "perf-warning"
                        else:
                            cpu_avg_class = "perf-danger"
                            
                except Exception as e:
                    print(f"è°ƒè¯•:ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼ŒCPUå¹³å‡å€¼é¢œè‰²åˆ¤æ–­å‡ºé”™: {e}")
                    cpu_avg_class = ""
            else:
                print(f"è°ƒè¯•: CPUå¹³å‡å€¼æ•°æ®æ— æ•ˆï¼Œè·³è¿‡é¢œè‰²åˆ¤æ–­ - cpu_avg_value: {cpu_avg_value}")
            
            
            memory_class = ""
            if memory_range_info and "MB" in memory_range_info:
                try:
                    import re
                    # æå–å†…å­˜å³°å€¼å’Œç™¾åˆ†æ¯”ï¼ŒåŒ¹é… "96MB(0.1%) -> 787MB(1.2%)(62.5GB)" ä¸­çš„ 787MB(1.2%) éƒ¨åˆ†
                    match = re.search(r'(\d+)MB\(([\d.]+)%\)', memory_range_info.split('->')[-1])
                    if match:
                        memory_peak_mb = float(match.group(1))
                        memory_peak_percentage = float(match.group(2))
                        
                        print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•å†…å­˜åˆ¤æ–­ - å³°å€¼: {memory_peak_mb}MB, å ç”¨ç‡: {memory_peak_percentage}%")
                        
                        # åŸºäºå†…å­˜ä½¿ç”¨ç‡çš„åˆ¤æ–­
                        if memory_peak_percentage < 25.0:         # ä½¿ç”¨ç‡ < 25%
                            memory_class = "perf-good"
                            judgment_reason = f"å†…å­˜ä½¿ç”¨è‰¯å¥½ (ä½¿ç”¨ç‡{memory_peak_percentage:.1f}%, ç»å¯¹å€¼{memory_peak_mb:.0f}MB)"
                        elif memory_peak_percentage < 50.0:      # ä½¿ç”¨ç‡ 25%-50%
                            memory_class = "perf-warning" 
                            judgment_reason = f"å†…å­˜ä½¿ç”¨ä¸€èˆ¬ (ä½¿ç”¨ç‡{memory_peak_percentage:.1f}%, ç»å¯¹å€¼{memory_peak_mb:.0f}MB)"
                        else:                                     # ä½¿ç”¨ç‡ > 50%
                            memory_class = "perf-danger"
                            judgment_reason = f"å†…å­˜ä½¿ç”¨è¿‡é«˜ (ä½¿ç”¨ç‡{memory_peak_percentage:.1f}%, ç»å¯¹å€¼{memory_peak_mb:.0f}MB)"
                                                    
                        print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•å†…å­˜é¢œè‰²åˆ¤æ–­ -> {memory_class} ({judgment_reason})")
                        
                        # ç‰¹æ®Šæƒ…å†µå¤„ç†
                        # å¦‚æœç³»ç»Ÿå†…å­˜å¾ˆå¤§(>32GB)ä½†ä½¿ç”¨ç‡å¾ˆä½ï¼Œå³ä½¿ç»å¯¹å€¼å¤§ä¹Ÿè®¤ä¸ºæ˜¯å¥½çš„
                        if system_total_memory:
                            try:
                                total_memory_gb = float(system_total_memory.replace('GB', ''))
                                if total_memory_gb > 32 and memory_peak_percentage < 10.0:
                                    memory_class = "perf-good"
                                    print(f"è°ƒè¯•: å¤§å†…å­˜ç³»ç»Ÿç‰¹æ®Šå¤„ç† -> ç»¿è‰² (ç³»ç»Ÿ{total_memory_gb}GB, ä½¿ç”¨ç‡ä»…{memory_peak_percentage:.1f}%)")
                            except Exception as e:
                                print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•è§£æç³»ç»Ÿæ€»å†…å­˜å¤±è´¥: {str(e)}")
                        
                        # æç«¯æƒ…å†µè­¦å‘Š
                        if memory_peak_mb > 49152:  # > 48GB
                            print(f"è°ƒè¯•: âš ï¸  å†…å­˜ä½¿ç”¨é‡è¿‡å¤§è­¦å‘Š: {memory_peak_mb:.0f}MBï¼Œå»ºè®®æ£€æŸ¥æ˜¯å¦æœ‰å†…å­˜æ³„æ¼")
                            
                    else:
                        print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•æ— æ³•ä»å†…å­˜èŒƒå›´ä¿¡æ¯ä¸­è§£æå³°å€¼å’Œç™¾åˆ†æ¯”")
                        print(f"è°ƒè¯•: å†…å­˜èŒƒå›´ä¿¡æ¯å†…å®¹: {memory_range_info}")
                        
                except Exception as e:
                    print(f"è°ƒè¯•:ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•å†…å­˜é¢œè‰²åˆ¤æ–­å‡ºé”™: {str(e)}")
                    import traceback
                    print(f"è°ƒè¯•: å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
                    memory_class = ""
                    
            else:
                print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•å†…å­˜æ•°æ®æ— æ•ˆï¼Œè·³è¿‡é¢œè‰²åˆ¤æ–­ - memory_range_info: {memory_range_info}")
            
            
            # ========== å†…å­˜å¹³å‡å€¼çš„é¢œè‰²æ ·å¼ï¼ˆæ–°å¢ç‹¬ç«‹åˆ¤æ–­ï¼‰ ==========
            memory_avg_class = ""
            if memory_avg_value is not None and memory_avg_percentage is not None:  
                try:
                    print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•å†…å­˜å¹³å‡å€¼åˆ¤æ–­ - å¹³å‡å€¼: {memory_avg_value:.0f}MB, å¹³å‡å ç”¨ç‡: {memory_avg_percentage:.1f}%")
                    
                    # åŸºäºå†…å­˜å¹³å‡ä½¿ç”¨ç‡çš„åˆ¤æ–­
                    if memory_avg_percentage < 25.0:
                        memory_avg_class = "perf-good"
                        judgment_reason = f"å†…å­˜ä½¿ç”¨è‰¯å¥½ (å¹³å‡ä½¿ç”¨ç‡{memory_avg_percentage:.1f}%, å¹³å‡å€¼{memory_avg_value:.0f}MB)"
                    elif memory_avg_percentage < 50.0:
                        memory_avg_class = "perf-warning" 
                        judgment_reason = f"å†…å­˜ä½¿ç”¨ä¸€èˆ¬ (å¹³å‡ä½¿ç”¨ç‡{memory_avg_percentage:.1f}%, å¹³å‡å€¼{memory_avg_value:.0f}MB)"
                    else:
                        memory_avg_class = "perf-danger"
                        judgment_reason = f"å†…å­˜ä½¿ç”¨è¿‡é«˜ (å¹³å‡ä½¿ç”¨ç‡{memory_avg_percentage:.1f}%, å¹³å‡å€¼{memory_avg_value:.0f}MB)"
                                                
                    print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•å†…å­˜å¹³å‡å€¼é¢œè‰²åˆ¤æ–­ -> {memory_avg_class} ({judgment_reason})")
                    
                    # ç‰¹æ®Šæƒ…å†µå¤„ç†
                    if system_total_memory:
                        try:
                            total_memory_gb = float(system_total_memory.replace('GB', ''))
                            if total_memory_gb > 32 and memory_avg_percentage < 10.0:
                                memory_avg_class = "perf-good"
                                print(f"è°ƒè¯•: å¤§å†…å­˜ç³»ç»Ÿç‰¹æ®Šå¤„ç† -> ç»¿è‰² (ç³»ç»Ÿ{total_memory_gb}GB, å¹³å‡ä½¿ç”¨ç‡ä»…{memory_avg_percentage:.1f}%)")
                        except Exception as e:
                            print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•è§£æç³»ç»Ÿæ€»å†…å­˜å¤±è´¥: {str(e)}")
                    
                    # æç«¯æƒ…å†µè­¦å‘Š
                    if memory_avg_value > 49152:  # > 48GB
                        print(f"è°ƒè¯•: âš ï¸  å†…å­˜å¹³å‡ä½¿ç”¨é‡è¿‡å¤§è­¦å‘Š: {memory_avg_value:.0f}MBï¼Œå»ºè®®æ£€æŸ¥æ˜¯å¦æœ‰å†…å­˜æ³„æ¼")
                        
                except Exception as e:
                    print(f"è°ƒè¯•:ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•å†…å­˜å¹³å‡å€¼é¢œè‰²åˆ¤æ–­å‡ºé”™: {str(e)}")
                    memory_avg_class = ""
                    
            else:
                print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•å†…å­˜å¹³å‡å€¼æ•°æ®æ— æ•ˆï¼Œè·³è¿‡é¢œè‰²åˆ¤æ–­ - memory_avg_value: {memory_avg_value}, memory_avg_percentage: {memory_avg_percentage}")
            
            # è¯»å–å»¶è¿Ÿç»Ÿè®¡
            delay_stats = "N/A"
            delay_stats_html = "N/A"
            delay_class = ""
            
            if result['status'] == 'SUCCESS':
                try:
                    delay_file = config.get('delay_log_file', '')
                    print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œå°è¯•è¯»å–å»¶è¿Ÿæ–‡ä»¶: {delay_file}")
                    
                    # éªŒè¯è¿™æ˜¯å¦æ˜¯å½“å‰æµ‹è¯•çš„æ–‡ä»¶ï¼ˆé€šè¿‡æµ‹è¯•åç§°åŒ¹é…ï¼‰
                    test_name = result.get('test_name', '')
                    if delay_file and test_name:
                        # æ£€æŸ¥å»¶è¿Ÿæ–‡ä»¶è·¯å¾„æ˜¯å¦åŒ…å«å½“å‰æµ‹è¯•åç§°
                        if test_name.replace('test_', '').replace('_', '') not in delay_file.replace('_', '').replace('-', ''):
                            print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œå»¶è¿Ÿæ–‡ä»¶è·¯å¾„ä¸æµ‹è¯•åç§°ä¸åŒ¹é…")
                            print(f"  æµ‹è¯•åç§°: {test_name}")
                            print(f"  å»¶è¿Ÿæ–‡ä»¶: {delay_file}")
                            # å°è¯•æ ¹æ®æµ‹è¯•åç§°é‡æ–°æ„å»ºæ­£ç¡®çš„å»¶è¿Ÿæ–‡ä»¶è·¯å¾„
                            result_dir_from_perf = os.path.dirname(os.path.dirname(config.get('perf_file', '')))
                            if result_dir_from_perf:
                                corrected_delay_file = os.path.join(result_dir_from_perf, 'logs', f'{test_name}_delay.log')
                                if os.path.exists(corrected_delay_file):
                                    delay_file = corrected_delay_file
                                    print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œä½¿ç”¨ä¿®æ­£çš„å»¶è¿Ÿæ–‡ä»¶: {delay_file}")
                    
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    if not delay_file:
                        print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œå»¶è¿Ÿæ–‡ä»¶è·¯å¾„ä¸ºç©º")
                        delay_stats = "é…ç½®é”™è¯¯ï¼šå»¶è¿Ÿæ–‡ä»¶è·¯å¾„ä¸ºç©º"
                        delay_stats_html = "é…ç½®é”™è¯¯ï¼šå»¶è¿Ÿæ–‡ä»¶è·¯å¾„ä¸ºç©º"
                    elif not os.path.exists(delay_file):
                        print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œå»¶è¿Ÿæ–‡ä»¶ä¸å­˜åœ¨: {delay_file}")
                        delay_stats = "å»¶è¿Ÿæ–‡ä»¶ä¸å­˜åœ¨"
                        delay_stats_html = "å»¶è¿Ÿæ–‡ä»¶ä¸å­˜åœ¨"
                        
                        # å°è¯•æŸ¥æ‰¾å¯èƒ½çš„å»¶è¿Ÿæ–‡ä»¶
                        possible_delay_files = []
                        delay_dir = os.path.dirname(delay_file)
                        if os.path.exists(delay_dir):
                            try:
                                files = os.listdir(delay_dir)
                                # æŸ¥æ‰¾åŒ…å«å½“å‰æµ‹è¯•åç§°çš„å»¶è¿Ÿæ–‡ä»¶
                                test_specific_files = [f for f in files if 'delay' in f and test_name in f]
                                if test_specific_files:
                                    possible_delay_files.extend([os.path.join(delay_dir, f) for f in test_specific_files])
                                    print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œæ‰¾åˆ°å¯èƒ½çš„å»¶è¿Ÿæ–‡ä»¶: {test_specific_files}")
                            except Exception as e:
                                print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œè¯»å–ç›®å½•å¤±è´¥: {str(e)}")
                        
                        # å°è¯•ä½¿ç”¨æ‰¾åˆ°çš„å»¶è¿Ÿæ–‡ä»¶
                        for possible_file in possible_delay_files:
                            if os.path.exists(possible_file):
                                delay_file = possible_file
                                print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œä½¿ç”¨æ‰¾åˆ°çš„å»¶è¿Ÿæ–‡ä»¶: {possible_file}")
                                break
                    
                    # è¯»å–å»¶è¿Ÿæ–‡ä»¶
                    if delay_file and os.path.exists(delay_file):
                        print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œå¼€å§‹è¯»å–å»¶è¿Ÿæ–‡ä»¶: {delay_file}")
                        
                        with open(delay_file, 'r') as f:
                            content = f.read()
                            
                            if len(content) > 0:
                                print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œå»¶è¿Ÿæ–‡ä»¶å†…å®¹å‰200å­—ç¬¦: {content[:200]}")
                            
                            # ç»Ÿè®¡å»¶è¿Ÿåˆ†å¸ƒ
                            delay_counts = {
                                'ä¼˜ç§€': content.count('å»¶è¿Ÿç­‰çº§: ä¼˜ç§€'),
                                'è‰¯å¥½': content.count('å»¶è¿Ÿç­‰çº§: è‰¯å¥½'), 
                                'æ­£å¸¸': content.count('å»¶è¿Ÿç­‰çº§: æ­£å¸¸'),
                                'è½»å¾®å»¶è¿Ÿ': content.count('å»¶è¿Ÿç­‰çº§: è½»å¾®å»¶è¿Ÿ'),
                                'æ˜æ˜¾å»¶è¿Ÿ': content.count('å»¶è¿Ÿç­‰çº§: æ˜æ˜¾å»¶è¿Ÿ'),
                                'ä¸¥é‡å»¶è¿Ÿ': content.count('å»¶è¿Ÿç­‰çº§: ä¸¥é‡å»¶è¿Ÿ')
                            }
                            
                            print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œå»¶è¿Ÿåˆ†å¸ƒç»Ÿè®¡: {delay_counts}")
                            
                            # æ„å»ºå»¶è¿Ÿç»Ÿè®¡å­—ç¬¦ä¸²
                            total_checks = sum(delay_counts.values())
                            print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œæ€»æ£€æŸ¥æ¬¡æ•°: {total_checks}")
                            
                            if total_checks > 0:
                                # å®šä¹‰å»¶è¿Ÿçº§åˆ«å¯¹åº”çš„CSSç±»
                                delay_level_classes = {
                                    'ä¼˜ç§€': 'delay-excellent',
                                    'è‰¯å¥½': 'delay-good',
                                    'æ­£å¸¸': 'delay-normal',
                                    'è½»å¾®å»¶è¿Ÿ': 'delay-warning',
                                    'æ˜æ˜¾å»¶è¿Ÿ': 'delay-danger',
                                    'ä¸¥é‡å»¶è¿Ÿ': 'delay-critical'
                                }
                            
                                # æ„å»ºçº¯æ–‡æœ¬ç‰ˆæœ¬ï¼ˆç”¨äºå¯¼å‡ºç­‰ï¼‰
                                delay_parts = []
                                # æ„å»ºHTMLç‰ˆæœ¬ï¼ˆæ¯ä¸ªçº§åˆ«æœ‰è‡ªå·±çš„é¢œè‰²ï¼‰
                                delay_parts_html = []
                                
                                for level, count in delay_counts.items():
                                    if count > 0:
                                        percentage = count / total_checks * 100
                                        text_part = f"{level}:{count}({percentage:.0f}%)"
                                        delay_parts.append(text_part)
                                        
                                        # HTMLç‰ˆæœ¬ï¼šæ¯ä¸ªçº§åˆ«ç”¨å¯¹åº”çš„CSSç±»åŒ…è£…
                                        css_class = delay_level_classes.get(level, 'delay-normal')
                                        html_part = f'<span class="{css_class}">{level}:{count}({percentage:.0f}%)</span>'
                                        delay_parts_html.append(html_part)
                                
                                # çº¯æ–‡æœ¬ç‰ˆæœ¬
                                delay_stats = " | ".join(delay_parts) if delay_parts else "æ— æœ‰æ•ˆå»¶è¿Ÿæ•°æ®"
                                
                                # HTMLç‰ˆæœ¬ï¼ˆå¤šé¢œè‰²æ˜¾ç¤ºï¼‰
                                delay_stats_html = " | ".join(delay_parts_html) if delay_parts_html else "æ— æœ‰æ•ˆå»¶è¿Ÿæ•°æ®"
                                
                                print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œç”Ÿæˆçš„å»¶è¿Ÿç»Ÿè®¡: {delay_stats}")
                                print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œç”Ÿæˆçš„HTMLå»¶è¿Ÿç»Ÿè®¡: {delay_stats_html}")
                                
                                # æ ¹æ®ä¸»è¦å»¶è¿Ÿçº§åˆ«è®¾ç½®é¢œè‰²
                                if delay_counts['ä¼˜ç§€'] > total_checks * 0.8:
                                    delay_class = "delay-excellent"
                                elif delay_counts['è‰¯å¥½'] + delay_counts['ä¼˜ç§€'] > total_checks * 0.7:
                                    delay_class = "delay-good"
                                elif delay_counts['æ­£å¸¸'] + delay_counts['è‰¯å¥½'] + delay_counts['ä¼˜ç§€'] > total_checks * 0.6:
                                    delay_class = "delay-normal"
                                elif delay_counts['è½»å¾®å»¶è¿Ÿ'] > 0:
                                    delay_class = "delay-warning"
                                elif delay_counts['æ˜æ˜¾å»¶è¿Ÿ'] > 0:
                                    delay_class = "delay-danger"
                                else:
                                    delay_class = "delay-critical"
                                    
                                print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œå»¶è¿Ÿé¢œè‰²ç±»: {delay_class}")
                            else:
                                print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œæ€»æ£€æŸ¥æ¬¡æ•°ä¸º0ï¼Œå»¶è¿Ÿç›‘æ§å¯èƒ½æœªæ­£å¸¸å·¥ä½œ")
                                delay_stats = "å»¶è¿Ÿç›‘æ§æœªç”Ÿæˆæœ‰æ•ˆæ•°æ®"
                                delay_stats_html = "å»¶è¿Ÿç›‘æ§æœªç”Ÿæˆæœ‰æ•ˆæ•°æ®"
                    else:
                        print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œæœ€ç»ˆæœªæ‰¾åˆ°æœ‰æ•ˆçš„å»¶è¿Ÿæ–‡ä»¶")
                        delay_stats = "N/A"
                        delay_stats_html = "N/A"
                            
                except Exception as e:
                    print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•ï¼Œè¯»å–å»¶è¿Ÿç»Ÿè®¡æ—¶å‡ºé”™: {str(e)}")
                    delay_stats = f"è¯»å–å¤±è´¥: {str(e)}"
                    delay_stats_html = f"è¯»å–å¤±è´¥: {str(e)}"
            else:
                print(f"è°ƒè¯•: ç¬¬ {result_index + 1} ä¸ªæµ‹è¯•çŠ¶æ€ä¸º {result['status']}ï¼Œè·³è¿‡å»¶è¿Ÿæ•°æ®è¯»å–")
                delay_stats = "N/A"
                delay_stats_html = "N/A"
            
             # è°ƒè¯•ï¼šæ‰“å°æ¯ä¸ªç»“æœçš„é”™è¯¯ä¿¡æ¯
            print(f"è°ƒè¯•HTMLæŠ¥å‘Š - æµ‹è¯• {result['test_name']}:")
            print(f"  çŠ¶æ€: {result['status']}")
            print(f"  é”™è¯¯å­—æ®µå­˜åœ¨: {'error' in result}")
            if 'error' in result:
                error_content = result.get('error', '')
                print(f"  é”™è¯¯å†…å®¹: {str(error_content)[:100]}..." if error_content else "  é”™è¯¯å†…å®¹: ç©º")
                
            # é”™è¯¯ä¿¡æ¯å¤„ç†
            error_msg = result.get('error', '')
            print(f"è°ƒè¯•: åŸå§‹error_msg = {repr(error_msg)}")
            
            if error_msg:
                # å¦‚æœé”™è¯¯ä¿¡æ¯å¤ªé•¿ï¼Œæˆªæ–­ä½†ä¿ç•™å…³é”®ä¿¡æ¯
                if len(error_msg) > 300:
                    error_msg = error_msg[:300] + "..."
                error_msg = error_msg.replace('<', '&lt;').replace('>', '&gt;')
                print(f"è°ƒè¯•: HTMLè½¬ä¹‰åerror_msg = {repr(error_msg[:300])}")
            else:
                error_msg = ""
                print(f"è°ƒè¯•: error_msgä¸ºç©º")
                
            # å¤„ç†è·³è¿‡åŸå› æ˜¾ç¤º
            if result['status'] == 'SKIPPED':
                skip_reason = result.get('skip_reason', 'æœªçŸ¥åŸå› ')
                error_msg = f"è·³è¿‡åŸå› : {skip_reason}"
 
            cpu_peak_display = f"{cpu_peak_value:.1f}" if cpu_peak_value is not None else "N/A"
            memory_peak_display = f"{memory_peak_value:.0f}" if memory_peak_value else "N/A"
            # ä¿®æ”¹å¹³å‡å€¼æ˜¾ç¤ºï¼ŒåŒ…å«å®Œæ•´çš„å•ä½å’Œç™¾åˆ†æ¯”ä¿¡æ¯
            if cpu_avg_value is not None:
                cpu_avg_display = f"{cpu_avg_value:.1f}%{cpu_core_info}"
            else:
                cpu_avg_display = "N/A"
                
            if memory_avg_value is not None and memory_avg_percentage is not None:
                memory_avg_display = f"{memory_avg_value:.0f}MB({memory_avg_percentage:.1f}%){memory_total_info}"
            else:
                memory_avg_display = "N/A"
                   
            
            html_content += f"""
            <tr class="{status_class}">
                <td><strong>{result['test_name']}</strong></td>
                <td>{config['sql_type']}</td>
                <td>{config['agg_or_select']}</td>
                <td>{config['tbname_or_trows_or_sourcetable']}</td>
                <td><span class="status-badge {status_badge}">{result['status']}</span></td>
                <td>{result.get('duration', 0):.1f}</td>
                <td>
                    <div class="tooltip">
                        <div class="sql-preview">{sql_preview}</div>
                        <span class="tooltiptext">{sql_full_escaped}</span>
                    </div>
                </td>
                <td class="{cpu_class}">
                    {cpu_range_info if cpu_range_info else 'N/A'}
                </td>
                <td class="{memory_class}">
                    {memory_range_info if memory_range_info else 'N/A'}
                </td>
                <td class="{cpu_class}">
                    {cpu_avg_display}
                </td>
                <td class="{memory_class}">
                    {memory_avg_display}
                </td>
                <td style="font-size: 10px;">{delay_stats_html}</td>
                <td style="color: #dc3545; font-size: 10px;">{error_msg}</td>
            </tr>
"""
        
        print(f"è°ƒè¯•: HTMLè¡Œç”Ÿæˆå®Œæˆï¼Œé”™è¯¯åˆ—å†…å®¹: {error_msg[:300]}...")
        
        
        html_content += f"""
        </table>
    </div>
    
    <div style="margin-top: 30px; padding: 20px; background: white; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1);">
        <h3>ğŸ“Š ä½¿ç”¨è¯´æ˜</h3>
        <ul>
            <li><strong>çŠ¶æ€è¯´æ˜</strong>: 
                <span class="badge-success">æˆåŠŸ</span> - æµåˆ›å»ºæˆåŠŸå¹¶æ­£å¸¸è¿è¡Œ
                <span class="badge-failed">å¤±è´¥</span> - æµåˆ›å»ºå¤±è´¥ï¼Œé€šå¸¸ä¸ºSQLè¯­æ³•æˆ–åŠŸèƒ½é™åˆ¶
                <span class="badge-skipped">è·³è¿‡</span> - å·²çŸ¥å¤±è´¥åœºæ™¯ï¼Œè¢«è¿‡æ»¤å™¨è·³è¿‡æµ‹è¯•
            </li>
            <li><strong>SQLé¢„è§ˆ</strong>: é¼ æ ‡æ‚¬åœæŸ¥çœ‹å®Œæ•´SQLè¯­å¥</li>
            <li><strong>æ€§èƒ½æŒ‡æ ‡</strong>: 
                CPU/å†…å­˜ - <span class="perf-good">ç»¿è‰²(ä¼˜ç§€)</span> 
                <span class="perf-warning">é»„è‰²(ä¸€èˆ¬)</span> 
                <span class="perf-danger">çº¢è‰²(è¾ƒé«˜)</span>
                <br>æ˜¾ç¤ºæ ¼å¼: èµ„æºæ¶ˆè€—èŒƒå›´(æœ€å°->æœ€å¤§) + ç³»ç»Ÿä¿¡æ¯
                <br><strong>CPUé¢œè‰²åˆ¤æ–­</strong>: åŸºäºå®é™…CPUä½¿ç”¨ç‡ (æ˜¾ç¤ºå€¼Ã·æ€»æ ¸æ•°) 
                <br>ã€€ã€€ç»¿è‰²: å®é™…ä½¿ç”¨ç‡ < 30% | é»„è‰²: 30%-70% | çº¢è‰²: > 70%
                <br><strong>å†…å­˜é¢œè‰²åˆ¤æ–­</strong>: ç»¼åˆè€ƒè™‘ä½¿ç”¨ç‡å’Œç»å¯¹å€¼
                <br>ã€€ã€€ç»¿è‰²: ä½¿ç”¨ç‡<25% | é»„è‰²: ä½¿ç”¨ç‡25%-50% | çº¢è‰²: ä½¿ç”¨ç‡>50%
                <br>ã€€ã€€ç‰¹æ®Š: å¤§å†…å­˜ç³»ç»Ÿ(>32GB)ä¸”ä½¿ç”¨ç‡<10%æ—¶ä¼˜å…ˆåˆ¤ä¸ºä¼˜ç§€
            </li>
            <li><strong>æ€§èƒ½èŒƒå›´è¯´æ˜</strong>:
                <br>CPUå³°å€¼èŒƒå›´: æ˜¾ç¤ºæµ‹è¯•æœŸé—´çš„CPUä½¿ç”¨ç‡èŒƒå›´(æœ€å°->æœ€å¤§)ï¼Œæ‹¬å·å†…ä¸ºç³»ç»ŸCPUæ ¸æ•°
                <br>å†…å­˜å³°å€¼èŒƒå›´: æ˜¾ç¤ºå†…å­˜ä½¿ç”¨é‡èŒƒå›´(æœ€å°->æœ€å¤§)å’Œå ç³»ç»Ÿæ€»å†…å­˜ç™¾åˆ†æ¯”ï¼Œæ‹¬å·å†…ä¸ºç³»ç»Ÿæ€»å†…å­˜
                <br>CPUå¹³å‡å€¼: æ˜¾ç¤ºæµ‹è¯•æœŸé—´çš„CPUå¹³å‡å€¼
                <br>å†…å­˜å¹³å‡å€¼: æ˜¾ç¤ºå†…å­˜ä½¿ç”¨å¹³å‡å€¼
            </li>
            <li><strong>å»¶è¿Ÿç­‰çº§</strong> (åŸºäºæœ€å¤§å»¶è¿Ÿé˜ˆå€¼{self.fixed_params.get('max_delay_threshold', 30000)}msè¿›è¡Œåˆ†çº§):                  
                <br>â€¢ å»¶è¿Ÿæ—¶é—´ = æºæ•°æ®è¡¨çš„Lastï¼ˆTsï¼‰- æµç”Ÿæˆæ•°æ®è¡¨çš„Lastï¼ˆTsï¼‰
                <br>â€¢ å»¶è¿Ÿå€æ•° = å»¶è¿Ÿæ—¶é—´ / æœ€å¤§å»¶è¿Ÿé˜ˆå€¼ 
                <br><span class="delay-excellent">ğŸŸ¢ ä¼˜ç§€ (< 0.1å€é—´éš”)</span>: å»¶è¿Ÿ < {self.fixed_params.get('max_delay_threshold', 30000) * 0.1}ç§’ï¼Œæµè®¡ç®—éå¸¸åŠæ—¶
                <br><span class="delay-good">ğŸŸ¢ è‰¯å¥½ (0.1-0.5å€é—´éš”)</span>: å»¶è¿Ÿ {self.fixed_params.get('max_delay_threshold', 30000) * 0.1}-{self.fixed_params.get('max_delay_threshold', 30000) * 0.5}ç§’ï¼Œæµè®¡ç®—åŠæ—¶
                <br><span class="delay-normal">ğŸŸ¡ æ­£å¸¸ (0.5-1å€é—´éš”)</span>: å»¶è¿Ÿ {self.fixed_params.get('max_delay_threshold', 30000) * 0.5}-{self.fixed_params.get('max_delay_threshold', 30000)}ç§’ï¼Œåœ¨æ£€æŸ¥é—´éš”å†…
                <br><span class="delay-warning">ğŸŸ¡ è½»å¾®å»¶è¿Ÿ (1-6å€é—´éš”)</span>: å»¶è¿Ÿ {self.fixed_params.get('max_delay_threshold', 30000)}-{self.fixed_params.get('max_delay_threshold', 30000) * 6}ç§’ï¼Œç•¥æœ‰æ»å
                <br><span class="delay-danger">ğŸŸ  æ˜æ˜¾å»¶è¿Ÿ (6-30å€é—´éš”)</span>: å»¶è¿Ÿ {self.fixed_params.get('max_delay_threshold', 30000) * 6}-{self.fixed_params.get('max_delay_threshold', 30000) * 30}ç§’ï¼Œéœ€è¦å…³æ³¨
                <br><span class="delay-critical">ğŸ”´ ä¸¥é‡å»¶è¿Ÿ (> 30å€é—´éš”)</span>: å»¶è¿Ÿ > {self.fixed_params.get('max_delay_threshold', 30000) * 30}ç§’ï¼Œéœ€è¦ä¼˜åŒ–
            </li>
            <li><strong>äº¤äº’åŠŸèƒ½</strong>: ç‚¹å‡»è¡¨å¤´æ’åºï¼Œä½¿ç”¨ç­›é€‰å™¨å¿«é€ŸæŸ¥æ‰¾</li>
        </ul>
    </div>
    
    <div style="margin-top: 20px; text-align: center; color: #666; font-size: 12px;">
        <p>ğŸ“ è¯¦ç»†æ—¥å¿—æ–‡ä»¶ä½ç½®: {result_dir}</p>
        <p>ğŸƒâ€â™‚ï¸ TDengine æµè®¡ç®—æ€§èƒ½æµ‹è¯•å·¥å…· | ç”Ÿæˆæ—¶é—´: {current_time}</p>
    </div>
</body>
</html>
"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"è¯¦ç»†HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")


        
    def run_batch_tests(self, test_time_minutes=5, max_parallel=1, start_from=0, test_limit=None):
        """è¿è¡Œæ‰¹é‡æµ‹è¯•
        
        Args:
            test_time_minutes: æ¯ä¸ªæµ‹è¯•çš„è¿è¡Œæ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
            max_parallel: æœ€å¤§å¹¶è¡Œæµ‹è¯•æ•°ï¼ˆå½“å‰åªæ”¯æŒ1ï¼‰
            start_from: ä»ç¬¬å‡ ä¸ªæµ‹è¯•å¼€å§‹
            test_limit: é™åˆ¶æµ‹è¯•æ•°é‡
        """
        print("å¼€å§‹æ‰¹é‡æµè®¡ç®—æµ‹è¯•...")
        
        # è·å–æ‰€æœ‰æµ‹è¯•ç»„åˆ
        combinations = self.get_test_combinations()
        combinations = self.filter_combinations(combinations)
        
        # æ£€æŸ¥è¿‡æ»¤åæ˜¯å¦è¿˜æœ‰æµ‹è¯•
        if not combinations:
            print(f"\nâŒ é”™è¯¯: è¿‡æ»¤åæ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•ç»„åˆ!")
            print(f"å½“å‰è¿‡æ»¤æ¨¡å¼: {self.filter_mode}")
            print(f"æŒ‡å®šçš„SQLç±»å‹: {self.specified_sql_types}")
            print(f"\nğŸ’¡ å»ºè®®:")
            print(f"  1. æ£€æŸ¥ --batch-filter-mode å‚æ•°è®¾ç½®")
            print(f"  2. æ£€æŸ¥ --batch-sql-types å‚æ•°æ˜¯å¦æ­£ç¡®")
            print(f"  3. å°è¯•ä½¿ç”¨ --batch-filter-mode all è¿è¡Œæ‰€æœ‰æµ‹è¯•")
            print(f"  4. ä½¿ç”¨ --batch-filter-mode skip-known-failures è¿è¡ŒæˆåŠŸçš„æµ‹è¯•")
            
            # ä»ç„¶åˆ›å»ºç»“æœç›®å½•å’Œç”ŸæˆæŠ¥å‘Š
            result_dir = self.create_batch_result_dir()
            print(f"ç»“æœç›®å½•å·²åˆ›å»º: {result_dir}")
            self.start_time = datetime.datetime.now().isoformat()
            self.total_tests = 0
            self.generate_final_report(result_dir)
            return
        
        
        # åº”ç”¨å¯åŠ¨ä½ç½®å’Œé™åˆ¶
        if test_limit:
            combinations = combinations[start_from:start_from + test_limit]
        else:
            combinations = combinations[start_from:]
        
        self.total_tests = len(combinations)
        self.start_time = datetime.datetime.now().isoformat()
        
        print(f"æ€»æµ‹è¯•ç»„åˆæ•°: {self.total_tests}")
        print(f"æ¯ä¸ªæµ‹è¯•è¿è¡Œæ—¶é—´: {test_time_minutes}åˆ†é’Ÿ")
        
        # é˜²æ­¢é™¤é›¶é”™è¯¯
        if self.total_tests > 0:
            print(f"é¢„è®¡æ€»è€—æ—¶: {(self.total_tests * test_time_minutes / 60):.1f}å°æ—¶")
        else:
            print(f"é¢„è®¡æ€»è€—æ—¶: 0å°æ—¶ (æ²¡æœ‰æµ‹è¯•è¦æ‰§è¡Œ)")
        
        print(f"\næ‰¹é‡æµ‹è¯•å›ºå®šé…ç½®:")
        print(f"  æµå»¶è¿Ÿæ£€æŸ¥: {'å¯ç”¨' if self.fixed_params.get('check_stream_delay', False) else 'ç¦ç”¨'}")
        if self.fixed_params.get('check_stream_delay', False):
            print(f"  å»¶è¿Ÿæ£€æŸ¥é—´éš”: {self.fixed_params.get('delay_check_interval', 10)}ç§’")
            print(f"  æœ€å¤§å»¶è¿Ÿé˜ˆå€¼: {self.fixed_params.get('max_delay_threshold', 30000)}ms")
        print(f"  å­è¡¨æ•°é‡: {self.fixed_params.get('table_count', 1000)}")
        print(f"  æ¯è½®å†™å…¥è®°å½•æ•°: {self.fixed_params.get('real_time_batch_rows', 200)}")
        print(f"  æ•°æ®å†™å…¥é—´éš”: {self.fixed_params.get('real_time_batch_sleep', 0)}ç§’")
        print(f"  éƒ¨ç½²æ¨¡å¼: {self.fixed_params.get('deployment_mode', 'single')}")
        
        if start_from > 0:
            print(f"ä»ç¬¬ {start_from + 1} ä¸ªæµ‹è¯•å¼€å§‹")
        if test_limit:
            print(f"é™åˆ¶æµ‹è¯•æ•°é‡: {test_limit}")
        
        # åˆ›å»ºç»“æœç›®å½•
        result_dir = self.create_batch_result_dir()
        print(f"æµ‹è¯•ç»“æœå°†ä¿å­˜åˆ°: {result_dir}")
        
        # å¦‚æœæ²¡æœ‰æµ‹è¯•è¦æ‰§è¡Œï¼Œç›´æ¥ç”ŸæˆæŠ¥å‘Šå¹¶è¿”å›
        if self.total_tests == 0:
            print(f"\nâš ï¸ æ²¡æœ‰æµ‹è¯•éœ€è¦æ‰§è¡Œï¼Œç›´æ¥ç”ŸæˆæŠ¥å‘Š")
            self.generate_final_report(result_dir)
            return
        
        # ç¡®è®¤å¼€å§‹æµ‹è¯•
        if self.total_tests > 10:
            response = input(f"\nå°†è¦æ‰§è¡Œ {self.total_tests} ä¸ªæµ‹è¯•ï¼Œé¢„è®¡è€—æ—¶ {(self.total_tests * test_time_minutes / 60):.1f} å°æ—¶ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ(y/N): ")
            if response.lower() != 'y':
                print("æµ‹è¯•å·²å–æ¶ˆ")
                return
        
        # æ‰§è¡Œæµ‹è¯•
        for i, combination in enumerate(combinations, 1):
            self.current_test_index = start_from + i
            
            # ç”Ÿæˆæµ‹è¯•é…ç½®
            config = self.generate_test_config(combination, self.current_test_index, result_dir)
            config['time'] = test_time_minutes
            
            # ä¿å­˜æµ‹è¯•é…ç½®
            self.save_test_config(config, result_dir)
            
            # æ‰§è¡Œæµ‹è¯•
            result = self.execute_single_test(config)
            self.test_results.append(result)
            
            # ç”Ÿæˆè¿›åº¦æŠ¥å‘Š
            self.generate_progress_report(result_dir)
            
            # æ˜¾ç¤ºè¿›åº¦
            remaining_tests = self.total_tests - len(self.test_results)
            estimated_remaining_time = remaining_tests * test_time_minutes
            
            # ç»Ÿè®¡å„ç§çŠ¶æ€
            success_count = len([r for r in self.test_results if r['status'] == 'SUCCESS'])
            failed_count = len([r for r in self.test_results if r['status'] == 'FAILED'])
            skipped_count = len([r for r in self.test_results if r['status'] == 'SKIPPED'])
            
            print(f"\nè¿›åº¦æ€»ç»“:")
            print(f"  å·²å®Œæˆ: {len(self.test_results)}/{self.total_tests}")
            print(f"  æˆåŠŸ: {success_count}")
            print(f"  å¤±è´¥: {failed_count}")
            if skipped_count > 0:
                print(f"  è·³è¿‡: {skipped_count}")
            print(f"  å‰©ä½™ä¼°è®¡æ—¶é—´: {estimated_remaining_time:.0f}åˆ†é’Ÿ ({estimated_remaining_time/60:.1f}å°æ—¶)")
            
            # åœ¨æµ‹è¯•ä¹‹é—´æ·»åŠ çŸ­æš‚ä¼‘æ¯
            if i < len(combinations):
                print("ç­‰å¾…2ç§’åå¼€å§‹ä¸‹ä¸€ä¸ªæµ‹è¯•...")
                time.sleep(2)
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self.generate_final_report(result_dir)
        
        print(f"\n{'='*80}")
        print("æ‰¹é‡æµ‹è¯•å®Œæˆ!")
        print(f"æ€»æµ‹è¯•æ•°: {self.total_tests}")
        
        if self.total_tests > 0:
            success_count = len([r for r in self.test_results if r['status'] == 'SUCCESS'])
            failed_count = len([r for r in self.test_results if r['status'] == 'FAILED'])
            skipped_count = len([r for r in self.test_results if r['status'] == 'SKIPPED'])
            
            print(f"æˆåŠŸ: {success_count}")
            print(f"å¤±è´¥: {failed_count}")
            if skipped_count > 0:
                print(f"è·³è¿‡: {skipped_count}")
            print(f"æˆåŠŸç‡: {(success_count/self.total_tests*100):.1f}%")
            
            if self.filter_mode != 'all':
                print(f"è¿‡æ»¤æ¨¡å¼: {self.filter_mode}")
                if skipped_count > 0:
                    print(f"è¿‡æ»¤æ•ˆæœ: æˆåŠŸè·³è¿‡ {skipped_count} ä¸ªæµ‹è¯•")
        else:
            print(f"æ²¡æœ‰æ‰§è¡Œä»»ä½•æµ‹è¯•")
            
        print(f"ç»“æœç›®å½•: {result_dir}")
        print(f"{'='*80}")

        
        
class StreamStarter:
    def __init__(self, runtime=None, perf_file=None, table_count=500, 
                histroy_rows=1, real_time_batch_rows=200, disorder_ratio=0, vgroups=10,
                stream_sql=None, sql_type='select_stream', stream_num=1, stream_perf_test_dir=None, monitor_interval=1,
                create_data=False, restore_data=False, deployment_mode='single',
                debug_flag=131, num_of_log_lines=500000, 
                agg_or_select='agg', tbname_or_trows_or_sourcetable='sourcetable', custom_columns=None,
                check_stream_delay=False, max_delay_threshold=30000, delay_check_interval=10,
                real_time_batch_sleep=0, delay_log_file=None, auto_combine=False, monitor_warm_up_time=None) -> None:
        
        self.stream_perf_test_dir = stream_perf_test_dir if stream_perf_test_dir else '/home/taos_stream_cluster'        
        self.table_count = table_count      
        self.histroy_rows = histroy_rows      
        self.real_time_batch_rows = real_time_batch_rows    
        self.real_time_batch_sleep = real_time_batch_sleep  
        self.disorder_ratio = disorder_ratio 
        self.vgroups = vgroups 
        self.monitor_interval = monitor_interval
        self.taosd_processes = [] 
        self.create_data = create_data
        self.restore_data = restore_data
        self.backup_dir = os.path.join(self.stream_perf_test_dir, 'data_bak')
        self.deployment_mode = deployment_mode
        self.debug_flag = debug_flag
        self.num_of_log_lines = num_of_log_lines
        self.auto_combine = auto_combine
        self.monitor_warm_up_time = monitor_warm_up_time
        
        self.sql_type = sql_type
        self.stream_num = stream_num
        self.agg_or_select = agg_or_select
        self.tbname_or_trows_or_sourcetable = tbname_or_trows_or_sourcetable
        self.custom_columns = custom_columns
        print(f"è°ƒè¯•ä¿¡æ¯: tbname_or_trows_or_sourcetable = {tbname_or_trows_or_sourcetable}")
        print(f"è°ƒè¯•ä¿¡æ¯: sql_type = {sql_type}")
        print(f"è°ƒè¯•ä¿¡æ¯: agg_or_select = {agg_or_select}")
        print(f"è°ƒè¯•ä¿¡æ¯:æ•°æ®å†™å…¥é—´éš”: {real_time_batch_sleep}ç§’")
        print(f"è°ƒè¯•ä¿¡æ¯: auto_combine = {auto_combine}")
        self.stream_sql = stream_sql if stream_sql else StreamSQLTemplates.get_sql(
            sql_type, 
            stream_num=stream_num, 
            auto_combine=auto_combine, 
            agg_or_select=agg_or_select,
            tbname_or_trows_or_sourcetable=tbname_or_trows_or_sourcetable,
            custom_columns=custom_columns
        )
        #print(f"ç”Ÿæˆçš„SQL:\n{self.stream_sql}")
        
        self.check_stream_delay = check_stream_delay
        self.max_delay_threshold = max_delay_threshold  # æ¯«ç§’
        self.delay_check_interval = delay_check_interval  # ç§’
        
        # å»¶è¿Ÿæ—¥å¿—æ–‡ä»¶è·¯å¾„è®¾ç½®
        if delay_log_file:
            # å¦‚æœå¤–éƒ¨æŒ‡å®šäº†å»¶è¿Ÿæ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨æŒ‡å®šçš„è·¯å¾„
            self.delay_log_file = delay_log_file
            print(f"è°ƒè¯•: ä½¿ç”¨å¤–éƒ¨æŒ‡å®šçš„å»¶è¿Ÿæ—¥å¿—æ–‡ä»¶: {self.delay_log_file}")
        else:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
            self.delay_log_file = f"{os.path.splitext(perf_file)[0]}-stream-delay.log"
            print(f"è°ƒè¯•: ä½¿ç”¨é»˜è®¤å»¶è¿Ÿæ—¥å¿—æ–‡ä»¶: {self.delay_log_file}")
        
        print(f"æµå»¶è¿Ÿæ£€æŸ¥: {'å¯ç”¨' if check_stream_delay else 'ç¦ç”¨'}")
        if check_stream_delay:
            print(f"æœ€å¤§å»¶è¿Ÿé˜ˆå€¼: {max_delay_threshold}ms")
            print(f"å»¶è¿Ÿæ£€æŸ¥é—´éš”: {delay_check_interval}ç§’")
            print(f"å»¶è¿Ÿæ—¥å¿—æ–‡ä»¶: {self.delay_log_file}")
        
        # æ ¹æ®éƒ¨ç½²æ¨¡å¼è°ƒæ•´å®ä¾‹é…ç½®
        if self.deployment_mode == 'single':
            # å•èŠ‚ç‚¹æ¨¡å¼åªä½¿ç”¨ç¬¬ä¸€ä¸ªå®ä¾‹
            self.instances = [
                {
                    'name': 'dnode1',
                    'host': 'localhost',
                    'port': 6030,
                    'user': 'root',
                    'passwd': 'taosdata',
                    'data_dir': f'{self.stream_perf_test_dir}/dnode1/data',
                    'log_dir': f'{self.stream_perf_test_dir}/dnode1/log',
                }
            ]
            
            # å•èŠ‚ç‚¹æ¨¡å¼çš„æ•°æ®åº“é…ç½®
            self.db_config = {
                'stream_from': {
                    'name': 'stream_from',
                    'vgroups': self.vgroups
                },
                'stream_to': {
                    'name': 'stream_to', 
                    'vgroups': self.vgroups
                }
            }
        else:
            # å®šä¹‰3ä¸ªå®ä¾‹çš„é…ç½®
            self.instances = [
                {
                    'name': 'dnode1',
                    'host': 'localhost',
                    'port': 6030,
                    'user': 'root',
                    'passwd': 'taosdata',
                    'data_dir': f'{self.stream_perf_test_dir}/dnode1/data',
                    'log_dir': f'{self.stream_perf_test_dir}/dnode1/log',
                },
                {
                    'name': 'dnode2',
                    'host': 'localhost',
                    'port': 7030,
                    'user': 'root',
                    'passwd': 'taosdata',
                    'data_dir': f'{self.stream_perf_test_dir}/dnode2/data',
                    'log_dir': f'{self.stream_perf_test_dir}/dnode2/log',
                },
                {
                    'name': 'dnode3',
                    'host': 'localhost',
                    'port': 8030,
                    'user': 'root',
                    'passwd': 'taosdata',
                    'data_dir': f'{self.stream_perf_test_dir}/dnode3/data/',
                    'log_dir': f'{self.stream_perf_test_dir}/dnode3/log',
                }
            ]
            
            self.db_config = {
                'stream_from': {
                    'name': 'stream_from',
                    'vgroups': self.vgroups,
                    'dnodes': '1'  # é»˜è®¤åœ¨dnode1ä¸Š
                },
                'stream_to': {
                    'name': 'stream_to', 
                    'vgroups': self.vgroups,
                    'dnodes': '2'  # é»˜è®¤åœ¨dnode2ä¸Š
                }
            }
            
        self.sql = None
        self.host = self.instances[0]['host']
        self.user = self.instances[0]['user']
        self.passwd = self.instances[0]['passwd']
        self.conf = f"{self.stream_perf_test_dir}/dnode1/conf/taos.cfg"
        self.tz = 'Asia/Shanghai'
        # è®¾ç½®è¿è¡Œæ—¶é—´å’Œæ€§èƒ½æ–‡ä»¶è·¯å¾„
        self.runtime = runtime if runtime else 600  # é»˜è®¤è¿è¡Œ10åˆ†é’Ÿ
        self.perf_file = perf_file if perf_file else '/tmp/perf.log'
        

    def get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥
        
        Returns:
            tuple: (connection, cursor)
        """
        try:
            conn = taos.connect(
                host=self.host,
                user=self.user,
                password=self.passwd,
                config=self.conf,
                timezone=self.tz
            )
            cursor = conn.cursor()
            return conn, cursor
        except Exception as e:
            print(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
            raise
        
    def stop_taosd(self):
        """åœæ­¢æ‰€æœ‰taosdè¿›ç¨‹"""
        try:
            # å…ˆå°è¯•æ­£å¸¸åœæ­¢
            subprocess.run('pkill taosd', shell=True)
            time.sleep(10)
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰è¿›ç¨‹å­˜åœ¨
            result = subprocess.run('ps -ef | grep "taosd -c" | grep -v grep', 
                                shell=True, capture_output=True, text=True)
            if result.stdout:
                print("å‘ç°é¡½å›ºè¿›ç¨‹ï¼Œå¼ºåˆ¶åœæ­¢...")
                for line in result.stdout.splitlines():
                    try:
                        pid = int(line.split()[1])
                        subprocess.run(f'kill -9 {pid}', shell=True)
                        print(f"å¼ºåˆ¶ç»ˆæ­¢è¿›ç¨‹ PID: {pid}")
                    except:
                        continue
                
            print("æ‰€æœ‰taosdè¿›ç¨‹å·²åœæ­¢")
            
        except Exception as e:
            print(f"åœæ­¢è¿›ç¨‹å‡ºé”™: {str(e)}")
            raise

    def check_taosd_status(self):
        """æ£€æŸ¥taosdè¿›ç¨‹çŠ¶æ€"""
        try:
            result = subprocess.run('ps -ef | grep "taosd -c" | grep -v grep', 
                                shell=True, capture_output=True, text=True)
            if result.stdout:
                print("\nå½“å‰è¿è¡Œçš„taosdè¿›ç¨‹:")
                for line in result.stdout.splitlines():
                    print(line)
                return True
            else:
                print("è­¦å‘Š: æœªå‘ç°è¿è¡Œä¸­çš„taosdè¿›ç¨‹")
                return False
                
        except Exception as e:
            print(f"æ£€æŸ¥è¿›ç¨‹çŠ¶æ€å‡ºé”™: {str(e)}")
            return False

        
    def create_database(self, db_name, vgroups=None, dnodes=None):
        """åˆ›å»ºæ•°æ®åº“
        
        Args:
            db_name: æ•°æ®åº“åç§°
            vgroups: vgroupsæ•°é‡,å¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            dnodes: æŒ‡å®šæ•°æ®åº“æ‰€åœ¨çš„dnode,å¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼
        """
        try:
            # è·å–æ•°æ®åº“é…ç½®
            db_config = self.db_config.get(db_name, {})
            vgroups = vgroups or db_config.get('vgroups', self.vgroups)
            dnodes = dnodes or db_config.get('dnodes', '1')
            
            # åˆ›å»ºæ•°æ®åº“
            conn, cursor = self.get_connection()
            create_db_sql = f"create database {db_name} vgroups {vgroups}"
            if dnodes:
                create_db_sql += f" dnodes '{dnodes}'"
                
            print(f"\nåˆ›å»ºæ•°æ®åº“: {create_db_sql}")
            cursor.execute(create_db_sql)
            
            # å…³é—­è¿æ¥
            cursor.close()
            conn.close()
            
            print(f"æ•°æ®åº“ {db_name} åˆ›å»ºæˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"åˆ›å»ºæ•°æ®åº“ {db_name} å¤±è´¥: {str(e)}")
            return False
        
    def start_taosd_processes(self):
        """å¯åŠ¨æ‰€æœ‰ taosd è¿›ç¨‹ - æ”¯æŒå•èŠ‚ç‚¹å’Œé›†ç¾¤æ¨¡å¼"""
        try:
            mode_desc = "å•èŠ‚ç‚¹" if self.deployment_mode == 'single' else "é›†ç¾¤"
            print(f"\n=== å¼€å§‹å¯åŠ¨ {mode_desc} taosd è¿›ç¨‹ ===")
            
            # æ ¹æ®éƒ¨ç½²æ¨¡å¼ç¡®å®šè¦å¯åŠ¨çš„èŠ‚ç‚¹
            nodes_to_start = []
            if self.deployment_mode == 'single':
                nodes_to_start = ['dnode1']
            else:
                nodes_to_start = ['dnode1', 'dnode2', 'dnode3']
            
            for dnode in nodes_to_start:
                cfg_file = os.path.join(self.stream_perf_test_dir, dnode, 'conf', 'taos.cfg')
                cmd = f'nohup taosd -c {cfg_file} > /dev/null 2>&1 &'
                print(f"æ‰§è¡Œå¯åŠ¨å‘½ä»¤: {cmd}")
                
                # æ‰§è¡Œå¯åŠ¨å‘½ä»¤
                result = subprocess.run(cmd, shell=True)
                if result.returncode == 0:
                    print(f"å·²æ‰§è¡Œ {dnode} çš„å¯åŠ¨å‘½ä»¤")
                else:
                    print(f"è­¦å‘Š: {dnode} å¯åŠ¨å‘½ä»¤æ‰§è¡Œå¤±è´¥")
                    
                # éªŒè¯è¿›ç¨‹æ˜¯å¦å¯åŠ¨
                time.sleep(2)
                check_cmd = f"pgrep -f 'taosd -c {cfg_file}'"
                if subprocess.run(check_cmd, shell=True, stdout=subprocess.PIPE).stdout:
                    print(f"{dnode} è¿›ç¨‹å·²æˆåŠŸå¯åŠ¨")
                else:
                    print(f"è­¦å‘Š: {dnode} è¿›ç¨‹å¯èƒ½æœªæ­£å¸¸å¯åŠ¨")
            
            # ç­‰å¾…æœåŠ¡å®Œå…¨å¯åŠ¨
            wait_time = 3 if self.deployment_mode == 'single' else 10
            print(f"\nç­‰å¾…æœåŠ¡å¯åŠ¨ ({wait_time}ç§’)...")
            time.sleep(wait_time)             
            self.check_taosd_status()
            
            # é…ç½®é›†ç¾¤ï¼ˆå¦‚æœæ˜¯é›†ç¾¤æ¨¡å¼ï¼‰
            if self.deployment_mode == 'cluster':
                try:
                    # è¿æ¥åˆ°ç¬¬ä¸€ä¸ªèŠ‚ç‚¹
                    conn = taos.connect(
                        host=self.host,
                        user=self.user,
                        password=self.passwd,
                        config=self.conf
                    )
                    cursor = conn.cursor()
                    
                    print("\n=== æ£€æŸ¥é›†ç¾¤é…ç½® ===")
                    
                    # æŸ¥è¯¢å¹¶æ˜¾ç¤ºèŠ‚ç‚¹çŠ¶æ€
                    try:
                        # æŸ¥è¯¢ dnodes ä¿¡æ¯
                        print("\nDNode ä¿¡æ¯:")
                        cursor.execute("show dnodes")
                        result = cursor.fetchall()
                        for row in result:
                            print(f"ID: {row[0]}, endpoint: {row[1]}, status: {row[4]}")
                        
                        # æŸ¥è¯¢ mnodes ä¿¡æ¯
                        print("\nMNode ä¿¡æ¯:")
                        cursor.execute("show mnodes")
                        result = cursor.fetchall()
                        if result:
                            for row in result:
                                print(f"ID: {row[0]}, endpoint: {row[1]}, role: {row[2]}, status: {row[3]}")
                        else:
                            print("å½“å‰ç³»ç»Ÿä¸­æ²¡æœ‰é¢å¤–çš„mnode")
                        
                        # æŸ¥è¯¢ snodes ä¿¡æ¯
                        print("\nSNode ä¿¡æ¯:")
                        cursor.execute("show snodes")
                        result = cursor.fetchall()
                        if result:
                            for row in result:
                                print(f"ID: {row[0]}, endpoint: {row[1]}, create_time: {row[2]}")
                        else:
                            print("å½“å‰ç³»ç»Ÿä¸­æ²¡æœ‰snode")
                            
                    except Exception as e:
                        print(f"æŸ¥è¯¢èŠ‚ç‚¹ä¿¡æ¯å¤±è´¥: {str(e)}")
                    
                    # å…³é—­è¿æ¥
                    cursor.close()
                    conn.close()
                    
                except Exception as e:
                    print(f"æ£€æŸ¥é›†ç¾¤é…ç½®å¤±è´¥: {str(e)}")
                    
            print(f"\n{mode_desc}æ¨¡å¼ taosd è¿›ç¨‹å¯åŠ¨å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"å¯åŠ¨ taosd è¿›ç¨‹æ—¶å‡ºé”™: {str(e)}")
            return False
        return True
        
    def backup_cluster_data(self):
        """å¤‡ä»½é›†ç¾¤æ•°æ® - æ”¯æŒå•èŠ‚ç‚¹å’Œé›†ç¾¤æ¨¡å¼"""
        try:
            mode_desc = "å•èŠ‚ç‚¹" if self.deployment_mode == 'single' else "é›†ç¾¤"
            print(f"å¼€å§‹å¤‡ä»½{mode_desc}æ•°æ®...")
            
            # å…ˆåœæ­¢æ‰€æœ‰ taosd è¿›ç¨‹
            print("åœæ­¢æ‰€æœ‰ taosd è¿›ç¨‹...")
            subprocess.run('pkill -15 taosd', shell=True)
            time.sleep(5)  # ç­‰å¾…è¿›ç¨‹å®Œå…¨åœæ­¢
            
            # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å®Œå…¨åœæ­¢
            if subprocess.run('pgrep -x taosd', shell=True, stdout=subprocess.PIPE).stdout:
                print("ç­‰å¾… taosd è¿›ç¨‹åœæ­¢...")
                time.sleep(5)
                # å†æ¬¡æ£€æŸ¥ï¼Œå¦‚æœè¿˜æœ‰è¿›ç¨‹åˆ™å¼ºåˆ¶ç»ˆæ­¢
                if subprocess.run('pgrep -x taosd', shell=True, stdout=subprocess.PIPE).stdout:
                    print("å¼ºåˆ¶ç»ˆæ­¢ taosd è¿›ç¨‹...")
                    subprocess.run('pkill -9 taosd', shell=True)
                    time.sleep(2)
                    
            # åˆ›å»ºå¤‡ä»½ç›®å½•
            if os.path.exists(self.backup_dir):
                print(f"æ¸…ç†å·²å­˜åœ¨çš„å¤‡ä»½ç›®å½•: {self.backup_dir}")
                shutil.rmtree(self.backup_dir)
            os.makedirs(self.backup_dir)
            print(f"åˆ›å»ºå¤‡ä»½ç›®å½•: {self.backup_dir}")
            
            # æ ¹æ®éƒ¨ç½²æ¨¡å¼ç¡®å®šéœ€è¦å¤‡ä»½çš„èŠ‚ç‚¹
            nodes_to_backup = []
            if self.deployment_mode == 'single':
                nodes_to_backup = ['dnode1']
                print("å•èŠ‚ç‚¹æ¨¡å¼: ä»…å¤‡ä»½ dnode1 æ•°æ®")
            else:
                nodes_to_backup = ['dnode1', 'dnode2', 'dnode3']
                print("é›†ç¾¤æ¨¡å¼: å¤‡ä»½ dnode1, dnode2, dnode3 æ•°æ®")
                
            # åªå¤‡ä»½dataç›®å½•ï¼Œä¸å¤‡ä»½logå’Œconf
            for dnode in nodes_to_backup:
                src_data_dir = os.path.join(self.stream_perf_test_dir, dnode, 'data')
                dst_data_dir = os.path.join(self.backup_dir, dnode, 'data')
                
                if os.path.exists(src_data_dir):
                    print(f"\nå¤‡ä»½ {dnode}/data ç›®å½•...")
                    try:
                        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
                        os.makedirs(os.path.dirname(dst_data_dir), exist_ok=True)
                        
                        # ä½¿ç”¨ rsync æ’é™¤ä¸´æ—¶æ–‡ä»¶å’Œsocketæ–‡ä»¶
                        cmd = f'rsync -av --exclude="*.sock*" --exclude="*.tmp" --exclude="*.lock" {src_data_dir}/ {dst_data_dir}/'
                        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
                        
                        if result.returncode == 0:
                            print(f"å®Œæˆå¤‡ä»½: {dst_data_dir}")
                            
                            # æ˜¾ç¤ºå¤‡ä»½æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
                            try:
                                # è·å–å¤‡ä»½ç›®å½•å¤§å°
                                du_result = subprocess.run(f'du -sh {dst_data_dir}', shell=True, 
                                                        capture_output=True, text=True)
                                if du_result.returncode == 0:
                                    size_info = du_result.stdout.strip().split()[0]
                                    print(f"  å¤‡ä»½å¤§å°: {size_info}")
                                    
                                # ç»Ÿè®¡æ–‡ä»¶æ•°é‡
                                find_result = subprocess.run(f'find {dst_data_dir} -type f | wc -l', 
                                                            shell=True, capture_output=True, text=True)
                                if find_result.returncode == 0:
                                    file_count = find_result.stdout.strip()
                                    print(f"  æ–‡ä»¶æ•°é‡: {file_count}")
                            except Exception as e:
                                print(f"  ç»Ÿè®¡å¤‡ä»½ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
                        else:
                            # rsync å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ cp å‘½ä»¤
                            print(f"rsync å¤±è´¥ï¼Œä½¿ç”¨ cp å‘½ä»¤å¤‡ä»½...")
                            cmd = f'cp -r {src_data_dir} {dst_data_dir}'
                            subprocess.run(cmd, shell=True, check=True)
                            print(f"å®Œæˆå¤‡ä»½: {dst_data_dir}")
                            
                    except subprocess.CalledProcessError as e:
                        print(f"å¤‡ä»½ {dnode}/data å¤±è´¥: {str(e)}")
                        return False
                    except Exception as e:
                        print(f"å¤‡ä»½ {dnode}/data æ—¶å‡ºé”™: {str(e)}")
                        return False
                else:
                    print(f"è­¦å‘Š: {dnode}/data ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡å¤‡ä»½")
            
            # åˆ›å»ºå¤‡ä»½ä¿¡æ¯æ–‡ä»¶
            backup_info_file = os.path.join(self.backup_dir, 'backup_info.txt')
            try:
                with open(backup_info_file, 'w') as f:
                    f.write(f"TDengine æ•°æ®å¤‡ä»½ä¿¡æ¯\n")
                    f.write(f"=" * 50 + "\n")
                    f.write(f"å¤‡ä»½æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"éƒ¨ç½²æ¨¡å¼: {self.deployment_mode}\n")
                    f.write(f"å¤‡ä»½èŠ‚ç‚¹: {', '.join(nodes_to_backup)}\n")
                    f.write(f"å¤‡ä»½å†…å®¹: ä»…æ•°æ®ç›®å½• (data)\n")
                    f.write(f"å­è¡¨æ•°é‡: {self.table_count}\n")
                    f.write(f"æ¯è¡¨è®°å½•æ•°: {self.histroy_rows}\n")
                    f.write(f"æ•°æ®ä¹±åºç‡: {self.disorder_ratio}\n")
                    f.write(f"vgroupsæ•°: {self.vgroups}\n")
                    f.write(f"=" * 50 + "\n")
                    
                    # è®°å½•æ¯ä¸ªèŠ‚ç‚¹çš„å¤‡ä»½çŠ¶æ€
                    for dnode in nodes_to_backup:
                        dst_data_dir = os.path.join(self.backup_dir, dnode, 'data')
                        if os.path.exists(dst_data_dir):
                            f.write(f"{dnode}: å¤‡ä»½æˆåŠŸ\n")
                        else:
                            f.write(f"{dnode}: å¤‡ä»½å¤±è´¥\n")
                            
                print(f"å¤‡ä»½ä¿¡æ¯å·²ä¿å­˜åˆ°: {backup_info_file}")
            except Exception as e:
                print(f"åˆ›å»ºå¤‡ä»½ä¿¡æ¯æ–‡ä»¶å¤±è´¥: {str(e)}")
            
            print(f"\n=== {mode_desc}æ•°æ®å¤‡ä»½å®Œæˆ! ===")
            print(f"å¤‡ä»½ç›®å½•: {self.backup_dir}")
            return True
            
            # # ç›´æ¥å¤åˆ¶æ•´ä¸ªèŠ‚ç‚¹ç›®å½•
            # for dnode in nodes_to_backup:
            #     src_dir = os.path.join(self.stream_perf_test_dir, dnode, 'data')
            #     dst_dir = os.path.join(self.backup_dir, dnode, 'data')
                
            #     print(f"\nå¤‡ä»½ {dnode} ç›®å½•...")
            #     try:
            #         # ä½¿ç”¨ rsync æ’é™¤ socket æ–‡ä»¶
            #         cmd = f'rsync -av --exclude="*.sock*" {src_dir}/ {dst_dir}/'
            #         subprocess.run(cmd, shell=True, check=True)
            #         print(f"å®Œæˆå¤‡ä»½: {dst_dir}")
            #     except subprocess.CalledProcessError:
            #         # å¦‚æœ rsync å¤±è´¥,ä½¿ç”¨ cp å‘½ä»¤
            #         print(f"rsync å¤±è´¥,ä½¿ç”¨ cp å‘½ä»¤å¤‡ä»½...")
            #         shutil.copytree(src_dir, dst_dir, symlinks=True, 
            #                     ignore=shutil.ignore_patterns('*.sock*'))
            #         print(f"å®Œæˆå¤‡ä»½: {dst_dir}")
            
            # print("\n=== é›†ç¾¤æ•°æ®å¤‡ä»½å®Œæˆ! ===")
            # print(f"å¤‡ä»½ç›®å½•: {self.backup_dir}")
            # return True
    
        except Exception as e:
            print(f"å¤‡ä»½æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return False
        finally:
            # é‡æ–°å¯åŠ¨ taosd è¿›ç¨‹
            if not self.start_taosd_processes():
                print("è­¦å‘Š: taosd è¿›ç¨‹å¯åŠ¨å¤±è´¥")
        
    def restore_cluster_data(self):
        """ä»å¤‡ä»½æ¢å¤é›†ç¾¤æ•°æ® - æ”¯æŒå•èŠ‚ç‚¹å’Œé›†ç¾¤æ¨¡å¼"""
        try:
            if not os.path.exists(self.backup_dir):
                raise Exception(f"é”™è¯¯: å¤‡ä»½ç›®å½•ä¸å­˜åœ¨: {self.backup_dir}")
                
            # è¯»å–å¤‡ä»½ä¿¡æ¯
            backup_info_file = os.path.join(self.backup_dir, 'backup_info.txt')
            backup_mode = None
            if os.path.exists(backup_info_file):
                try:
                    with open(backup_info_file, 'r') as f:
                        content = f.read()
                        # ä»å¤‡ä»½ä¿¡æ¯ä¸­æå–éƒ¨ç½²æ¨¡å¼
                        for line in content.split('\n'):
                            if line.startswith('éƒ¨ç½²æ¨¡å¼:'):
                                backup_mode = line.split(':')[1].strip()
                                break
                except Exception as e:
                    print(f"è¯»å–å¤‡ä»½ä¿¡æ¯å¤±è´¥: {str(e)}")
            
            current_mode_desc = "å•èŠ‚ç‚¹" if self.deployment_mode == 'single' else "é›†ç¾¤"
            backup_mode_desc = "å•èŠ‚ç‚¹" if backup_mode == 'single' else "é›†ç¾¤" if backup_mode == 'cluster' else "æœªçŸ¥"
            
            print(f"å¼€å§‹æ¢å¤æ•°æ®...")
            print(f"å½“å‰æ¨¡å¼: {current_mode_desc}")
            if backup_mode:
                print(f"å¤‡ä»½æ¨¡å¼: {backup_mode_desc}")
                if backup_mode != self.deployment_mode:
                    print(f"è­¦å‘Š: å½“å‰æ¨¡å¼({self.deployment_mode})ä¸å¤‡ä»½æ¨¡å¼({backup_mode})ä¸åŒ¹é…!")
                    response = input("æ˜¯å¦ç»§ç»­æ¢å¤? (y/N): ")
                    if response.lower() != 'y':
                        print("æ¢å¤æ“ä½œå·²å–æ¶ˆ")
                        return False
            
            # åœæ­¢ç°æœ‰taosdè¿›ç¨‹
            subprocess.run('pkill -15 taosd', shell=True)
            time.sleep(5)
            # ç¡®ä¿è¿›ç¨‹å®Œå…¨åœæ­¢
            while subprocess.run('pgrep -x taosd', shell=True, stdout=subprocess.PIPE).stdout:
                print("ç­‰å¾… taosd è¿›ç¨‹åœæ­¢...")
                time.sleep(2)
                subprocess.run('pkill -9 taosd', shell=True)
                
            # æ ¹æ®å½“å‰éƒ¨ç½²æ¨¡å¼ç¡®å®šéœ€è¦æ¢å¤çš„èŠ‚ç‚¹
            nodes_to_restore = []
            if self.deployment_mode == 'single':
                nodes_to_restore = ['dnode1']
                print("å•èŠ‚ç‚¹æ¨¡å¼: ä»…æ¢å¤ dnode1 æ•°æ®")
            else:
                nodes_to_restore = ['dnode1', 'dnode2', 'dnode3']
                print("é›†ç¾¤æ¨¡å¼: æ¢å¤ dnode1, dnode2, dnode3 æ•°æ®")
                
            # æ¢å¤æ¯ä¸ªèŠ‚ç‚¹çš„æ•°æ®
            restored_count = 0
            for dnode in nodes_to_restore:
                backup_data_dir = os.path.join(self.backup_dir, dnode, 'data')
                cluster_data_dir = os.path.join(self.stream_perf_test_dir, dnode, 'data')
                
                if not os.path.exists(backup_data_dir):
                    print(f"è­¦å‘Š: å¤‡ä»½ç›®å½•ä¸­æœªæ‰¾åˆ° {dnode}/data")
                    # å¯¹äºé›†ç¾¤æ¨¡å¼ï¼Œå¦‚æœæŸäº›èŠ‚ç‚¹çš„å¤‡ä»½ä¸å­˜åœ¨ï¼Œè·³è¿‡ä½†ç»§ç»­
                    if self.deployment_mode == 'cluster':
                        continue
                    else:
                        # å¯¹äºå•èŠ‚ç‚¹æ¨¡å¼ï¼Œå¦‚æœdnode1çš„å¤‡ä»½ä¸å­˜åœ¨ï¼Œåˆ™å¤±è´¥
                        if dnode == 'dnode1':
                            raise Exception(f"å•èŠ‚ç‚¹æ¨¡å¼ä¸‹å¿…é¡»çš„ {dnode}/data å¤‡ä»½ä¸å­˜åœ¨")
                        continue
                        
                print(f"\nè¿˜åŸ {dnode}/data ...")
                
                # æ˜¾ç¤ºå¤‡ä»½ç›®å½•ä¿¡æ¯
                try:
                    du_result = subprocess.run(f'du -sh {backup_data_dir}', shell=True, 
                                            capture_output=True, text=True)
                    if du_result.returncode == 0:
                        backup_size = du_result.stdout.strip().split()[0]
                        print(f"å¤‡ä»½æ•°æ®å¤§å°: {backup_size}")
                except:
                    pass
                
                # æ¸…ç†ç°æœ‰æ•°æ®ç›®å½•
                if os.path.exists(cluster_data_dir):
                    print(f"æ¸…ç†ç°æœ‰æ•°æ®ç›®å½•: {cluster_data_dir}")
                    shutil.rmtree(cluster_data_dir)
                
                # ç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(cluster_data_dir), exist_ok=True)
                
                # æ¢å¤æ•°æ®ç›®å½•
                try:
                    shutil.copytree(backup_data_dir, cluster_data_dir, symlinks=True)
                    print(f"å®Œæˆè¿˜åŸ: {cluster_data_dir}")
                    restored_count += 1
                    
                    # æ˜¾ç¤ºæ¢å¤åçš„ä¿¡æ¯
                    try:
                        du_result = subprocess.run(f'du -sh {cluster_data_dir}', shell=True, 
                                                capture_output=True, text=True)
                        if du_result.returncode == 0:
                            restored_size = du_result.stdout.strip().split()[0]
                            print(f"æ¢å¤æ•°æ®å¤§å°: {restored_size}")
                    except:
                        pass
                        
                except Exception as e:
                    print(f"è¿˜åŸ {dnode}/data æ—¶å‡ºé”™: {str(e)}")
                    return False
            
            if restored_count == 0:
                print("é”™è¯¯: æ²¡æœ‰æˆåŠŸæ¢å¤ä»»ä½•èŠ‚ç‚¹çš„æ•°æ®")
                return False
                
            print(f"\n=== æ•°æ®è¿˜åŸå®Œæˆ! ===")
            print(f"æˆåŠŸæ¢å¤ {restored_count} ä¸ªèŠ‚ç‚¹çš„æ•°æ®")
            
            # é‡å¯ taosd è¿›ç¨‹
            return self.start_taosd_processes()
            
        except Exception as e:
            print(f"\nè¿˜åŸæ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return False
        
    def do_test_stream_with_restored_data(self):
        """æ¢å¤æ•°æ®åæµ‹è¯•æŒ‡å®šçš„æµè®¡ç®—SQL"""
        
        loader = None
        monitor_thread = None
        delay_monitor_thread = None
        conn = None
        cursor = None
    
        try:
            print("=== æ¢å¤æ•°æ®åæµ‹è¯•æµè®¡ç®— ===")
            
            # å…ˆæ¢å¤æ•°æ®
            print("1. æ¢å¤å†å²æ•°æ®...")
            if not self.restore_cluster_data():
                raise Exception("æ¢å¤é›†ç¾¤æ•°æ®å¤±è´¥")
            
            print("2. åˆ›å»ºç›®æ ‡æ•°æ®åº“...")
            # åˆ›å»ºstream_toæ•°æ®åº“
            if not self.create_database('stream_to'):
                raise Exception("åˆ›å»ºstream_toæ•°æ®åº“å¤±è´¥")
            
            print("3. å¼€å§‹æµè®¡ç®—æµ‹è¯•...")
            print(f"SQLç±»å‹: {self.sql_type}")
            print(f"æµæ•°é‡: {self.stream_num}")
            print(f"è‡ªå®šä¹‰SQL: {'æ˜¯' if self.stream_sql else 'å¦'}")
            
            # è·å–æ•°æ®åº“è¿æ¥
            conn, cursor = self.get_connection()
            
            
            # è¿æ¥åˆ°æºæ•°æ®åº“
            print("è¿æ¥åˆ°æºæ•°æ®åº“ stream_from...")
            cursor.execute('use stream_from')
            
            # æ£€æŸ¥æºæ•°æ®çŠ¶æ€
            print("æ£€æŸ¥æºæ•°æ®çŠ¶æ€...")
            cursor.execute("select count(*) from information_schema.ins_tables where db_name='stream_from'")
            table_count = cursor.fetchall()[0][0]
            
            cursor.execute("select count(*) from stream_from.stb")
            record_count = cursor.fetchall()[0][0]
            
            print(f"æºæ•°æ®ç»Ÿè®¡: {table_count} å¼ è¡¨, {record_count:,} æ¡è®°å½•")
            
            # è·å– SQL æ¨¡æ¿å¹¶åˆ›å»ºæµ
            sql_templates = self.stream_sql 
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºæ‰¹é‡æ‰§è¡Œ
            if isinstance(sql_templates, dict):
                print("\n=== å¼€å§‹æ‰¹é‡åˆ›å»ºæµ ===")
                total_streams = len(sql_templates)
                success_count = 0
                failed_count = 0
                
                for index, (sql_name, sql_template) in enumerate(sql_templates.items(), 1):
                    try:
                        print(f"\n[{index}/{total_streams}] åˆ›å»ºæµ: {sql_name}")
                        
                        # æå–å®é™…çš„æµåç§°ï¼ˆä»SQLä¸­è§£æï¼‰
                        import re
                        # åŒ¹é… create stream åé¢çš„æµåç§°
                        match = re.search(r'create\s+stream\s+([^\s]+)', sql_template, re.IGNORECASE)
                        actual_stream_name = match.group(1) if match else sql_name
                        
                        print(f"  SQLæµåç§°: {actual_stream_name}")
                        # æ˜¾ç¤ºå®Œæ•´çš„æµSQLè¯­å¥
                        print(f"  æµSQLè¯­å¥:")
                        # æ ¼å¼åŒ–SQLæ˜¾ç¤ºï¼Œå»æ‰å¤šä½™çš„ç©ºè¡Œå’Œç¼©è¿›
                        formatted_sql = '\n'.join([
                            '    ' + line.strip() 
                            for line in sql_template.strip().split('\n') 
                            if line.strip()
                        ])
                        print(formatted_sql)
                        
                        print(f"  æ‰§è¡Œåˆ›å»º...")
                        cursor.execute(sql_template)
                        success_count += 1
                        print(f"  âœ“ åˆ›å»ºæˆåŠŸ")
                        
                    except Exception as e:
                        failed_count += 1
                    
                        # æå–TDengineçš„å…·ä½“é”™è¯¯ä¿¡æ¯
                        error_message = str(e)
                        tdengine_error = extract_stream_creation_error(error_message)
                        
                        print(f"æ‰§è¡Œé”™è¯¯: {tdengine_error}")
                
                # æ˜¾ç¤ºæ‰¹é‡åˆ›å»ºç»“æœæ‘˜è¦
                print(f"\n=== æ‰¹é‡åˆ›å»ºç»“æœæ‘˜è¦ ===")
                print(f"æ€»æµæ•°: {total_streams}")
                print(f"æˆåŠŸ: {success_count}")
                print(f"å¤±è´¥: {failed_count}")
                print(f"æˆåŠŸç‡: {(success_count/total_streams*100):.1f}%")
                
                if failed_count > 0:
                    print(f"âš ï¸  {failed_count} ä¸ªæµåˆ›å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
                else:
                    print("æ‰€æœ‰æµåˆ›å»ºæˆåŠŸï¼")
                    
            else:
                # å•ä¸ªæµçš„åˆ›å»º
                print("\n=== å¼€å§‹åˆ›å»ºæµ ===")
                
                # æå–å®é™…çš„æµåç§°
                import re
                match = re.search(r'create\s+stream\s+([^\s]+)', sql_templates, re.IGNORECASE)
                actual_stream_name = match.group(1) if match else "æœªçŸ¥æµåç§°"
                
                print(f"æµåç§°: {actual_stream_name}")
                print("æ‰§è¡Œæµè®¡ç®—SQL:")
                print("-" * 60)
                print(sql_templates)
                print("-" * 60)
                
                try:
                    start_time = time.time()
                    cursor.execute(sql_templates)
                    create_time = time.time() - start_time
                    
                    print(f"âœ“ æµ {actual_stream_name} åˆ›å»ºå®Œæˆ! è€—æ—¶: {create_time:.2f}ç§’")
                    
                except Exception as e:
                    # æå–TDengineçš„å…·ä½“é”™è¯¯ä¿¡æ¯
                    error_message = str(e)
                    tdengine_error = extract_stream_creation_error(error_message)
                    
                    print(f"æ‰§è¡Œé”™è¯¯: {tdengine_error}")
                    raise
            
            # å¯åŠ¨ç³»ç»Ÿç›‘æ§
            print("\nå¼€å§‹ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ...")
        
            # è®¡ç®—é¢„çƒ­æ—¶é—´
            if hasattr(self, 'monitor_warm_up_time') and self.monitor_warm_up_time is not None:
                warm_up_time = self.monitor_warm_up_time
                print(f"ä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„é¢„çƒ­æ—¶é—´: {warm_up_time}ç§’")
            else:
                warm_up_time = 60 if self.runtime >= 2 else 0
                if warm_up_time > 0:
                    print(f"ä½¿ç”¨è‡ªåŠ¨è®¡ç®—çš„é¢„çƒ­æ—¶é—´: {warm_up_time}ç§’")
                    
            loader = MonitorSystemLoad(
                name_pattern='taosd -c', 
                count=self.runtime * 60,
                perf_file='/tmp/perf-stream-test.log',
                interval=self.monitor_interval,
                deployment_mode=self.deployment_mode,
                warm_up_time=warm_up_time
            )
            
            # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œç›‘æ§
            monitor_thread = threading.Thread(
                target=loader.get_proc_status,
                name="StreamTestMonitor"
            )
            monitor_thread.daemon = True
            monitor_thread.start()
            
            # å¯åŠ¨æµå»¶è¿Ÿç›‘æ§
            delay_monitor_thread = self.start_stream_delay_monitor()
            
            # ç­‰å¾…æµè®¡ç®—å®Œæˆ
            print(f"ç­‰å¾…æµè®¡ç®—å®Œæˆ... (ç›‘æ§æ—¶é—´: {self.runtime}åˆ†é’Ÿ)")
            
            # å®šæœŸæ£€æŸ¥æµè®¡ç®—è¿›åº¦
            check_interval = 30  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
            total_checks = (self.runtime * 60) // check_interval
            
            for i in range(total_checks):
                time.sleep(check_interval)
                
                try:
                    # æ£€æŸ¥ç›®æ ‡è¡¨æ•°æ®é‡
                    cursor.execute("use stream_to")
                    
                    # æŸ¥è¯¢ç›®æ ‡è¡¨æ•°é‡å’Œè®°å½•æ•°
                    cursor.execute("select count(*) from information_schema.ins_tables where db_name='stream_to'")
                    target_table_count = cursor.fetchall()[0][0]
                    
                    if target_table_count > 0:
                        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®å†™å…¥
                        cursor.execute("show tables")
                        tables = cursor.fetchall()
                        
                        total_target_records = 0
                        for table in tables:
                            table_name = table[0]
                            try:
                                cursor.execute(f"select count(*) from stream_to.{table_name}")
                                count = cursor.fetchall()[0][0]
                                total_target_records += count
                            except:
                                continue
                        
                        progress = (i + 1) / total_checks * 100
                        print(f"è¿›åº¦: {progress:.1f}% - ç›®æ ‡è¡¨: {target_table_count}, ç›®æ ‡è®°å½•: {total_target_records:,}")
                    else:
                        progress = (i + 1) / total_checks * 100
                        print(f"è¿›åº¦: {progress:.1f}% - ç­‰å¾…æµè®¡ç®—å¼€å§‹...")
                        
                    cursor.execute('use stream_from')  # åˆ‡æ¢å›æºæ•°æ®åº“
                    
                except Exception as e:
                    print(f"æ£€æŸ¥è¿›åº¦æ—¶å‡ºé”™: {str(e)}")
                    
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è¿è¡Œæ—¶é—´é™åˆ¶
                if i >= total_checks - 1:
                    print(f"\nå·²è¾¾åˆ°è¿è¡Œæ—¶é—´é™åˆ¶ ({self.runtime} åˆ†é’Ÿ)")
                    break
            
            print("\næµè®¡ç®—æµ‹è¯•å®Œæˆ!")
            
            # æ˜¾ç¤ºæœ€ç»ˆç»“æœç»Ÿè®¡
            print("\n=== æµè®¡ç®—ç»“æœç»Ÿè®¡ ===")
            try:
                cursor.execute("use stream_to")
                cursor.execute("select count(*) from information_schema.ins_tables where db_name='stream_to'")
                final_table_count = cursor.fetchall()[0][0]
                
                cursor.execute("show tables")
                tables = cursor.fetchall()
                
                final_total_records = 0
                table_details = []
                for table in tables:
                    table_name = table[0]
                    try:
                        cursor.execute(f"select count(*) from stream_to.{table_name}")
                        count = cursor.fetchall()[0][0]
                        final_total_records += count
                        table_details.append(f"è¡¨ {table_name}: {count:,} æ¡è®°å½•")
                    except Exception as e:
                        table_details.append(f"è¡¨ {table_name}: æŸ¥è¯¢å¤±è´¥ - {str(e)}")
                
                print(f"æ€»è®¡: {final_table_count} å¼ ç›®æ ‡è¡¨, {final_total_records:,} æ¡ç»“æœè®°å½•")
                
                # æ˜¾ç¤ºå‰å‡ å¼ è¡¨çš„è¯¦æƒ…
                for detail in table_details[:5]:
                    print(f"  {detail}")
                if len(table_details) > 5:
                    print(f"  ... è¿˜æœ‰ {len(table_details) - 5} å¼ è¡¨")
                
                # è®¡ç®—å¤„ç†æ•ˆç‡
                if record_count > 0:
                    processing_ratio = final_total_records / record_count * 100
                    print(f"å¤„ç†æ•ˆç‡: {processing_ratio:.2f}% ({final_total_records:,}/{record_count:,})")
                
            except Exception as e:
                print(f"ç»Ÿè®¡ç»“æœæ—¶å‡ºé”™: {str(e)}")
                
            print("\n=== æµè®¡ç®—æµ‹è¯•å®Œæˆ ===")
            print("æ€§èƒ½ç›‘æ§æ•°æ®å·²ä¿å­˜åˆ°: /tmp/perf-stream-test-*.log")
            print("å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹ç›‘æ§æ•°æ®:")
            print("cat /tmp/perf-stream-test-all.log")
            
        except Exception as e:
            print(f"æµè®¡ç®—æµ‹è¯•å¤±è´¥: {str(e)}")
            raise
        finally:
            # å®‰å…¨åœ°å…³é—­æ•°æ®åº“è¿æ¥
            try:
                if cursor:
                    cursor.close()
                if conn:
                    conn.close()
            except Exception as e:
                print(f"å…³é—­æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {str(e)}")
            
            # å®‰å…¨åœ°åœæ­¢ç›‘æ§
            try:
                if loader:
                    print("ä¸»åŠ¨åœæ­¢ç³»ç»Ÿç›‘æ§...")
                    loader.stop()
            except Exception as e:
                print(f"åœæ­¢ç³»ç»Ÿç›‘æ§æ—¶å‡ºé”™: {str(e)}")
            
            # å®‰å…¨åœ°ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
            try:
                if monitor_thread:
                    print("ç­‰å¾…ç›‘æ§æ•°æ®æ”¶é›†å®Œæˆ...")
                    monitor_thread.join(timeout=15)
                    
                    if monitor_thread.is_alive():
                        print("ç›‘æ§çº¿ç¨‹æœªåœ¨é¢„æœŸæ—¶é—´å†…ç»“æŸï¼Œå¼ºåˆ¶ç»§ç»­...")
                    else:
                        print("ç›‘æ§çº¿ç¨‹å·²æ­£å¸¸ç»“æŸ")
            except Exception as e:
                print(f"ç­‰å¾…ç›‘æ§çº¿ç¨‹æ—¶å‡ºé”™: {str(e)}")
                
            # å®‰å…¨åœ°ç­‰å¾…å»¶è¿Ÿç›‘æ§çº¿ç¨‹ç»“æŸ
            try:
                if delay_monitor_thread:
                    print("ç­‰å¾…å»¶è¿Ÿç›‘æ§å®Œæˆ...")
                    delay_monitor_thread.join(timeout=10)
                    if delay_monitor_thread.is_alive():
                        print("å»¶è¿Ÿç›‘æ§çº¿ç¨‹æœªåœ¨é¢„æœŸæ—¶é—´å†…ç»“æŸï¼Œå¼ºåˆ¶ç»§ç»­...")
                    else:
                        print("å»¶è¿Ÿç›‘æ§çº¿ç¨‹å·²æ­£å¸¸ç»“æŸ")
            except Exception as e:
                print(f"ç­‰å¾…å»¶è¿Ÿç›‘æ§çº¿ç¨‹æ—¶å‡ºé”™: {str(e)}")
            
            # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
            try:
                if self.check_stream_delay:
                    print(f"\næµå»¶è¿Ÿç›‘æ§æŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.delay_log_file}")
                    self.print_final_delay_summary()
            except Exception as e:
                print(f"æ‰“å°æœ€ç»ˆæŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}")

        
    def create_test_data(self):
        """åˆ›å»ºå¹¶å¤‡ä»½æµ‹è¯•æ•°æ®"""
        try:
            print("å¼€å§‹ç”Ÿæˆæµ‹è¯•æ•°æ®...")
            self.prepare_env()
            self.prepare_source_from_data()
            # è¿è¡Œæ•°æ®ç”Ÿæˆ
            proc = subprocess.Popen('taosBenchmark --f /tmp/stream_from.json',
                                    stdout=subprocess.PIPE, shell=True, text=True)
            
            # ç­‰å¾…æ•°æ®å†™å…¥å®Œæˆ
            conn = taos.connect(host=self.host, user=self.user, 
                                password=self.passwd, config=self.conf)
            cursor = conn.cursor()
            
            if not self.wait_for_data_ready(cursor,self.table_count, self.histroy_rows):
                print("æ•°æ®ç”Ÿæˆå¤±è´¥")
                return False
                
            # å¤‡ä»½æ•°æ®
            if self.backup_cluster_data():
                print("æµ‹è¯•æ•°æ®å·²ç”Ÿæˆå¹¶å¤‡ä»½")
                return True
                
            print("æµ‹è¯•æ•°æ®åˆ›å»ºå®Œæˆ")
            return True
            
        except Exception as e:
            print(f"åˆ›å»ºæµ‹è¯•æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            return False
        
    def prepare_env(self):
        """
        æ¸…ç†ç¯å¢ƒå¹¶å¯åŠ¨TDengineæœåŠ¡
        æ”¯æŒå•èŠ‚ç‚¹(single)å’Œé›†ç¾¤(cluster)ä¸¤ç§æ¨¡å¼
        """
        try:
            print_title(f"\n=== å¼€å§‹å‡†å¤‡ç¯å¢ƒ (æ¨¡å¼: {self.deployment_mode}) ===")
            # åœæ­¢å·²å­˜åœ¨çš„taosdè¿›ç¨‹
            print_info("åœæ­¢ç°æœ‰taosdè¿›ç¨‹")
            self.stop_taosd()
            
            # æ£€æŸ¥å¹¶å¤„ç†é›†ç¾¤æ ¹ç›®å½•
            if os.path.exists(self.stream_perf_test_dir):
                print_info(f"æ¸…ç†å·²å­˜åœ¨çš„é›†ç¾¤ç›®å½•: {self.stream_perf_test_dir}")
                subprocess.run(f'rm -rf {self.stream_perf_test_dir}/*', shell=True)
            else:
                print_info(f"åˆ›å»ºæ–°çš„é›†ç¾¤ç›®å½•: {self.stream_perf_test_dir}")
                subprocess.run(f'mkdir -p {self.stream_perf_test_dir}', shell=True)
            
                
            for instance in self.instances:
                # åˆ›å»ºå¿…è¦çš„ç›®å½•
                for dir_type in ['data', 'log', 'conf']:
                    dir_path = f"{self.stream_perf_test_dir}/{instance['name']}/{dir_type}"
                    subprocess.run(f'mkdir -p {dir_path}', shell=True)
                
                # æ¸…ç†æ•°æ®ç›®å½•
                data_dir = f"{self.stream_perf_test_dir}/{instance['name']}/data"
                subprocess.run(f'rm -rf {data_dir}', shell=True)
                print(f"åˆ›å»ºç›®å½•: {dir_path}")
                
                # æ ¹æ®éƒ¨ç½²æ¨¡å¼ç”Ÿæˆä¸åŒçš„é…ç½®æ–‡ä»¶
                if self.deployment_mode == 'single':
                    # å•èŠ‚ç‚¹é…ç½®ç”Ÿæˆé…ç½®æ–‡ä»¶
                    cfg_content = f"""
firstEP         localhost:6030
fqdn            localhost
serverPort      {instance['port']}
supportVnodes   100
dataDir         {instance['data_dir']}
logDir          {instance['log_dir']}
asyncLog        1
debugFlag       {self.debug_flag}
numOfLogLines   {self.num_of_log_lines}
"""
                else:
                    # é›†ç¾¤é…ç½®ç”Ÿæˆé…ç½®æ–‡ä»¶
                    cfg_content = f"""
firstEP         localhost:6030
secondEP        localhost:7030
fqdn            localhost
serverPort      {instance['port']}
supportVnodes   50
dataDir         {instance['data_dir']}
logDir          {instance['log_dir']}
asyncLog        1
debugFlag       {self.debug_flag}
numOfLogLines   {self.num_of_log_lines}
"""
                cfg_file = f"{self.stream_perf_test_dir}/{instance['name']}/conf/taos.cfg"
                
                # ä½¿ç”¨ EOF æ–¹å¼å†™å…¥é…ç½®æ–‡ä»¶
                subprocess.run(f"""
cat << 'EOF' > {cfg_file}
{cfg_content}
EOF
""", shell=True)
            print_success("ç¯å¢ƒå‡†å¤‡å®Œæˆï¼Œé…ç½®æ–‡ä»¶å·²ç”Ÿæˆ")
            
            
            # å¯åŠ¨æ‰€æœ‰taosdå®ä¾‹
            self.taosd_processes = []  
            for instance in self.instances:
                cfg_file = f"{self.stream_perf_test_dir}/{instance['name']}/conf/taos.cfg"
                cmd = f'nohup taosd -c {cfg_file} > /dev/null 2>&1 &'
                
                try:
                    process = subprocess.Popen(cmd, shell=True)
                    self.taosd_processes.append({
                        'name': instance,
                        'pid': process.pid,
                        'cfg': cfg_file
                    })
                    print(f"å¯åŠ¨taosdè¿›ç¨‹: {instance}, PID: {process.pid}")
                except Exception as e:
                    print(f"å¯åŠ¨ {instance} å¤±è´¥: {str(e)}")
                    raise
            
            # ç­‰å¾…æœåŠ¡å®Œå…¨å¯åŠ¨
            wait_time = 3 if self.deployment_mode == 'single' else 10
            print(f"ç­‰å¾…æœåŠ¡å¯åŠ¨ ({wait_time}ç§’)...")
            time.sleep(wait_time)             
            self.check_taosd_status()
            
            # é…ç½®é›†ç¾¤
            try:
                # è¿æ¥åˆ°ç¬¬ä¸€ä¸ªèŠ‚ç‚¹
                conn = taos.connect(
                    host=self.host,
                    user=self.user,
                    password=self.passwd,
                    config=self.conf
                )
                cursor = conn.cursor()
                
                if self.deployment_mode == 'cluster':
                    # é›†ç¾¤æ¨¡å¼é…ç½®
                    print("\n=== å¼€å§‹é…ç½®é›†ç¾¤ ===")
                    cluster_cmds = [
                        'create dnode "localhost:7030"',
                        'create dnode "localhost:8030"',
                        'create mnode on dnode 2',
                        'create mnode on dnode 3',
                        'create snode on dnode 3',
                        
                        'create snode on dnode 2',
                        'create snode on dnode 1'
                    ]
                
                    for cmd in cluster_cmds:
                        try:
                            cursor.execute(cmd)
                            time.sleep(1)
                            print(f"æ‰§è¡ŒæˆåŠŸ: {cmd}")
                        except Exception as e:
                            print(f"æ‰§è¡Œå¤±è´¥: {cmd}")
                            print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
                
                    # æŸ¥è¯¢å¹¶æ˜¾ç¤ºé›†ç¾¤çŠ¶æ€
                    print("\né›†ç¾¤èŠ‚ç‚¹ä¿¡æ¯:")
                    print("-" * 50)
                    
                else:
                    # å•èŠ‚ç‚¹æ¨¡å¼é…ç½®
                    print("\n=== é…ç½®å•èŠ‚ç‚¹ç¯å¢ƒ ===")
                    try:
                        cursor.execute('create snode on dnode 1')
                        print("æ‰§è¡ŒæˆåŠŸ: create snode on dnode 1")
                        time.sleep(1)
                    except Exception as e:
                        print(f"åˆ›å»ºsnodeå¤±è´¥: {str(e)}")
                
                # æŸ¥è¯¢å¹¶æ˜¾ç¤ºèŠ‚ç‚¹çŠ¶æ€ï¼ˆå•èŠ‚ç‚¹å’Œé›†ç¾¤éƒ½æ˜¾ç¤ºï¼‰
                print(f"\n=== {self.deployment_mode.upper()}æ¨¡å¼èŠ‚ç‚¹ä¿¡æ¯ ===")
                print("-" * 60)
                
                try:
                    # æŸ¥è¯¢ dnodes ä¿¡æ¯
                    print("\nDNode ä¿¡æ¯:")
                    cursor.execute("show dnodes")
                    result = cursor.fetchall()
                    for row in result:
                        print(f"ID: {row[0]}, endpoint: {row[1]}, status: {row[4]}")
                    
                    # æŸ¥è¯¢ mnodes ä¿¡æ¯
                    print("\nMNode ä¿¡æ¯:")
                    cursor.execute("show mnodes")
                    result = cursor.fetchall()
                    if result:
                        for row in result:
                            print(f"ID: {row[0]}, endpoint: {row[1]}, role: {row[2]}, status: {row[3]}")
                    else:
                        print("å½“å‰ç³»ç»Ÿä¸­æ²¡æœ‰é¢å¤–çš„mnode")
                    
                    # æŸ¥è¯¢ snodes ä¿¡æ¯
                    print("\nSNode ä¿¡æ¯:")
                    cursor.execute("show snodes")
                    result = cursor.fetchall()
                    if result:
                        for row in result:
                            print(f"ID: {row[0]}, endpoint: {row[1]}, create_time: {row[2]}")
                    else:
                        print("å½“å‰ç³»ç»Ÿä¸­æ²¡æœ‰snode")
                        
                except Exception as e:
                    print(f"æŸ¥è¯¢èŠ‚ç‚¹ä¿¡æ¯å¤±è´¥: {str(e)}")
                
                print("-" * 50)
                
                # å…³é—­è¿æ¥
                cursor.close()
                conn.close()
                
                print(f"{self.deployment_mode.upper()}æ¨¡å¼é…ç½®å®Œæˆ")
                
            except Exception as e:
                print_error(f"èŠ‚ç‚¹é…ç½®å¤±è´¥: {str(e)}")
                raise
                
        except Exception as e:
            print_error(f"ç¯å¢ƒå‡†å¤‡å¤±è´¥: {str(e)}")
            raise
        
    def prepare_source_from_data(self) -> dict:
        json_data = {
            "filetype": "insert",
            "cfgdir": f"{self.stream_perf_test_dir}/dnode1/conf",
            "host": "localhost",
            "port": 6030,
            "rest_port": 6041,
            "user": "root",
            "password": "taosdata",
            "thread_count": 50,
            "create_table_thread_count": 5,
            "result_file": "/tmp/taosBenchmark_result.log",
            "confirm_parameter_prompt": "no",
            "insert_interval": 10,
            "num_of_records_per_req": 1000,
            "max_sql_len": 102400,
            "databases": [
                {
                    "dbinfo": {
                        "name": "stream_from",
                        "drop": "yes",
                        "replica": 1,
                        "duration": 10,
                        "precision": "ms",
                        "keep": 3650,
                        "minRows": 100,
                        "maxRows": 4096,
                        "comp": 2,
                        "dnodes": "1",
                        "vgroups": self.vgroups,
                        "stt_trigger": 2,
                        "WAL_RETENTION_PERIOD": 86400
                    },
                    "super_tables": [
                        {
                            "name": "stb",
                            "child_table_exists": "yes",
                            "childtable_count": self.table_count,
                            "childtable_prefix": "ctb0_",
                            "escape_character": "no",
                            "auto_create_table": "yes",
                            "batch_create_tbl_num": 1000,
                            "data_source": "rand",
                            "insert_mode": "taosc",
                            "interlace_rows": 1,
                            "tcp_transfer": "no",
                            "insert_rows": self.histroy_rows,
                            "partial_col_num": 0,
                            "childtable_limit": 0,
                            "childtable_offset": 0,
                            "rows_per_tbl": 0,
                            "max_sql_len": 1024000,
                            "disorder_ratio": self.disorder_ratio,
                            "disorder_range": 1000,
                            "keep_trying": -1,
                            "timestamp_step": 50,
                            "trying_interval": 10,
                            "start_timestamp": "2025-06-01 00:00:00",
                            "sample_format": "csv",
                            "sample_file": "./sample.csv",
                            "tags_file": "",
                            "columns": [
                                {
                                    "type": "INT",
                                    "count": 1
                                },
                                {
                                    "type": "BIGINT",
                                    "count": 1
                                },
                                {
                                    "type": "DOUBLE",
                                    "count": 1
                                },
                                {
                                    "type": "FLOAT",
                                    "count": 1
                                }
                            ],
                            "tags": [
                                {
                                    "type": "INT",
                                    "count": 1
                                },
                                {
                                    "type": "VARCHAR",
                                    "count": 1,
                                    "len": 32
                                }
                            ]
                        }
                    ]
                }
            ],
            "prepare_rand": 10000,
            "chinese": "no",
            "test_log": "/tmp/testlog/"
        }

        with open('/tmp/stream_from.json', 'w+') as f:
            json.dump(json_data, f, indent=4)
            

    def insert_source_from_data(self) -> dict:
        json_data = {
            "filetype": "insert",
            "cfgdir": f"{self.stream_perf_test_dir}/dnode1/conf",
            "host": "localhost",
            "port": 6030,
            "rest_port": 6041,
            "user": "root",
            "password": "taosdata",
            "thread_count": 50,
            "create_table_thread_count": 5,
            "result_file": "/tmp/taosBenchmark_result.log",
            "confirm_parameter_prompt": "no",
            "insert_interval": 1,
            "num_of_records_per_req": 1000,
            "max_sql_len": 102400,
            "databases": [
                {
                    "dbinfo": {
                        "name": "stream_from",
                        "drop": "no",
                        "replica": 1,
                        "duration": 10,
                        "precision": "ms",
                        "keep": 3650,
                        "minRows": 100,
                        "maxRows": 4096,
                        "comp": 2,
                        "dnodes": "1",
                        "vgroups": self.vgroups,
                        "stt_trigger": 2,
                        "WAL_RETENTION_PERIOD": 86400
                    },
                    "super_tables": [
                        {
                            "name": "stb",
                            "child_table_exists": "yes",
                            "childtable_count": self.table_count,
                            "childtable_prefix": "ctb0_",
                            "escape_character": "no",
                            "auto_create_table": "yes",
                            "batch_create_tbl_num": 1000,
                            "data_source": "rand",
                            "insert_mode": "taosc",
                            "interlace_rows": 1,
                            "tcp_transfer": "no",
                            "insert_rows": self.real_time_batch_rows,
                            "partial_col_num": 0,
                            "childtable_limit": 0,
                            "childtable_offset": 0,
                            "rows_per_tbl": 0,
                            "max_sql_len": 1024000,
                            "disorder_ratio": self.disorder_ratio,
                            "disorder_range": 1000,
                            "keep_trying": -1,
                            "timestamp_step": 50,
                            "trying_interval": 10,
                            "start_timestamp": "2025-06-01 00:00:00",
                            "sample_format": "csv",
                            "sample_file": "./sample.csv",
                            "tags_file": "",
                            "columns": [
                                {
                                    "type": "INT",
                                    "count": 1
                                },
                                {
                                    "type": "BIGINT",
                                    "count": 1
                                },
                                {
                                    "type": "DOUBLE",
                                    "count": 1
                                },
                                {
                                    "type": "FLOAT",
                                    "count": 1
                                }
                            ],
                            "tags": [
                                {
                                    "type": "INT",
                                    "count": 1
                                },
                                {
                                    "type": "VARCHAR",
                                    "count": 1,
                                    "len": 32
                                }
                            ]
                        }
                    ]
                }
            ],
            "prepare_rand": 10000,
            "chinese": "no",
            "test_log": "/tmp/testlog/"
        }

        with open('/tmp/stream_from_insertdata.json', 'w+') as f:
            json.dump(json_data, f, indent=4)
            
    def update_insert_config(self):
        """
        æ›´æ–°æ•°æ®æ’å…¥é…ç½®
        Args:
            real_time_batch_rows: ä¸‹ä¸€è½®è¦æ’å…¥çš„æ•°æ®è¡Œæ•°
        """
        try:
            print("\n=== æ›´æ–°æ•°æ®æ’å…¥é…ç½® ===")
            #print(f"å½“å‰SQLç±»å‹: {self.sql_type}")
            
            # è·å–æœ€æ–°æ—¶é—´æˆ³
            conn = taos.connect(
                host=self.host,
                user=self.user,
                password=self.passwd,
                config=self.conf
            )
            cursor = conn.cursor()
            
            try:
                # æ ¹æ® SQL ç±»å‹å†³å®šæ—¶é—´æˆ³æ›´æ–°æ–¹å¼
                # æ£€æŸ¥æ˜¯å¦ä¸º period ç±»å‹çš„æµè®¡ç®—
                is_period_type = (
                    self.sql_type.startswith('period_') or 
                    'period_' in self.sql_type or
                    self.sql_type == 's2_5' or 
                    self.sql_type == 's2_11' or
                    (isinstance(self.stream_sql, dict) and 
                    any('period_' in key for key in self.stream_sql.keys())) or
                    (isinstance(self.stream_sql, str) and 
                    'period(' in self.stream_sql.lower())
                )
                
                if is_period_type:
                    next_start_time = time.strftime("%Y-%m-%d %H:%M:%S")
                    print(f"æ£€æµ‹åˆ° PERIOD ç±»å‹æµè®¡ç®—ï¼Œä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºèµ·å§‹æ—¶é—´: {next_start_time}")
                    print(f"PERIOD ç±»å‹è¯´æ˜: ä½¿ç”¨ cast(_tprev_localtime/1000000 as timestamp) å’Œ cast(_tnext_localtime/1000000 as timestamp) ä½œä¸ºæ—¶é—´èŒƒå›´å˜é‡")
                
                else:
                    # æŸ¥è¯¢æœ€æ–°æ—¶é—´æˆ³
                    cursor.execute("select last(ts) from stream_from.stb")
                    last_ts = cursor.fetchall()[0][0]
                
                    if not last_ts:
                        raise Exception("æœªèƒ½è·å–åˆ°æœ€æ–°æ—¶é—´æˆ³")
                        
                    # å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼Œå¹¶åŠ ä¸Š1ç§’
                    next_start_time = (last_ts + datetime.timedelta(seconds=1)).strftime("%Y-%m-%d %H:%M:%S")
                    print(f"å½“å‰æœ€æ–°æ—¶é—´æˆ³: {last_ts}")
                    print(f"æ›´æ–°èµ·å§‹æ—¶é—´ä¸º: {next_start_time}")
                
                # è¯»å–ç°æœ‰é…ç½®
                config_file = '/tmp/stream_from_insertdata.json'
                with open(config_file, 'r') as f:
                    content = f.read()
                
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢æ—¶é—´æˆ³
                import re
                new_content = re.sub(
                    r'"start_timestamp":\s*"[^"]*"',
                    f'"start_timestamp": "{next_start_time}"',
                    content,
                    count=1  # åªæ›¿æ¢ç¬¬ä¸€æ¬¡å‡ºç°çš„æ—¶é—´æˆ³
                )
                
                # æ ¼å¼åŒ–å†™å…¥ä»¥ç¡®ä¿ JSON æ ¼å¼æ­£ç¡®
                with open(config_file, 'w') as f:
                    f.write(new_content)
                
                print("é…ç½®æ—¶é—´æˆ³å·²æ›´æ–°")
                return True
                
            except Exception as e:
                print(f"æ›´æ–°é…ç½®æ—¶å‡ºé”™: {str(e)}")
                return False
            finally:
                cursor.close()
                conn.close()
                
        except Exception as e:
            print(f"æ‰§è¡Œæ›´æ–°é…ç½®æ—¶å‡ºé”™: {str(e)}")
            return False

    def wait_for_data_ready(self, cursor, expected_tables, expected_records):
        """ç­‰å¾…æ•°æ®å†™å…¥å®Œæˆ   
        Args:
            cursor: æ•°æ®åº“æ¸¸æ ‡
            expected_tables: é¢„æœŸçš„å­è¡¨æ•°é‡
            expected_records: æ¯ä¸ªå­è¡¨çš„è®°å½•æ•°
            
        Returns:
            bool: æ•°æ®æ˜¯å¦å‡†å¤‡å°±ç»ª
        """
        max_wait_time = 3600  # æœ€å¤§ç­‰å¾…æ—¶é—´(ç§’)
        check_interval = 2   # æ£€æŸ¥é—´éš”(ç§’)
        start_time = time.time()
        
        while True:
            try:
                # æ£€æŸ¥å­è¡¨æ•°é‡
                cursor.execute("select count(*) from information_schema.ins_tables "
                            "where db_name='stream_from'")
                table_count = cursor.fetchall()[0][0]
                
                if table_count < expected_tables:
                    print(f"\rç­‰å¾…å­è¡¨åˆ›å»ºå®Œæˆ... å½“å‰: {table_count}/{expected_tables}", end='')
                    if time.time() - start_time > max_wait_time:
                        print(f"\nç­‰å¾…è¶…æ—¶! å­è¡¨æ•°é‡ä¸è¶³: {table_count}/{expected_tables}")
                        return False
                    time.sleep(check_interval)
                    continue
                
                # ä½¿ç”¨è¶…çº§è¡¨æŸ¥è¯¢æ€»è®°å½•æ•°
                cursor.execute("select count(*) from stream_from.stb")
                total_records = cursor.fetchall()[0][0]
                expected_total = expected_tables * expected_records
                
                if total_records < expected_total:
                    print(f"\rç­‰å¾…æ•°æ®å†™å…¥å®Œæˆ... å½“å‰: {total_records}/{expected_total}", end='')
                    
                    # å¦‚æœæ¥è¿‘è¶…æ—¶ï¼Œæ£€æŸ¥æ¯ä¸ªå­è¡¨çš„è®°å½•æ•°
                    if time.time() - start_time > max_wait_time - 30:  # ç•™å‡º30ç§’ç”¨äºè¯¦ç»†æ£€æŸ¥
                        print("\nå³å°†è¶…æ—¶ï¼Œæ£€æŸ¥å„å­è¡¨æ•°æ®æƒ…å†µ:")
                        insufficient_tables = []
                        
                        for i in range(expected_tables):
                            cursor.execute(f"select count(*) from stream_from.ctb0_{i}")
                            count = cursor.fetchall()[0][0]
                            if count < expected_records:
                                insufficient_tables.append({
                                    'table': f'ctb0_{i}',
                                    'current': count,
                                    'expected': expected_records,
                                    'missing': expected_records - count
                                })
                        
                        if insufficient_tables:
                            print("\nä»¥ä¸‹å­è¡¨æ•°æ®ä¸è¶³:")
                            for table in insufficient_tables:
                                print(f"è¡¨ {table['table']}: "
                                    f"å½“å‰ {table['current']}/{table['expected']}, "
                                    f"ç¼ºå°‘ {table['missing']} æ¡è®°å½•")
                        return False
                    
                    time.sleep(check_interval)
                    continue
                
                print(f"\næ•°æ®å‡†å¤‡å°±ç»ª! å…± {table_count} å¼ å­è¡¨ï¼Œ{total_records} æ¡è®°å½•")
                return True
                
            except Exception as e:
                print(f"\næ£€æŸ¥æ•°æ®æ—¶å‡ºé”™: {str(e)}")
                if time.time() - start_time > max_wait_time:
                    print("ç­‰å¾…è¶…æ—¶!")
                    return False
                time.sleep(check_interval)
                
    def _parse_target_table_from_sql(self, sql):
        """ä»æµSQLä¸­è§£æç›®æ ‡è¡¨å
        
        Args:
            sql: æµçš„SQLè¯­å¥
            
        Returns:
            str: ç›®æ ‡è¡¨åï¼Œå¦‚æœè§£æå¤±è´¥åˆ™è¿”å›None
            
        Example SQL:
            create stream qdb.s18  interval(58s) intervalsliding(11m, 4m)  
                    from qdb.v1   stream_options(watermark(43m)) 
                    into qdb.st18 
                    as select _twstart ts, c2 as c2_val, c1 as c1_val 
                    from stream_trigger;
            
            åº”è¯¥è¿”å›: qdb.st18
        """
        try:
            import re
            
            # æ¸…ç†SQLï¼šç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦å’Œæ¢è¡Œç¬¦
            cleaned_sql = ' '.join(sql.split())
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… into å…³é”®å­—åé¢çš„è¡¨å
            # æ¨¡å¼è¯´æ˜:
            # \binto\s+ : åŒ¹é…å•è¯è¾¹ç•Œçš„intoï¼Œåè·Ÿä¸€ä¸ªæˆ–å¤šä¸ªç©ºç™½å­—ç¬¦
            # ([^\s]+)  : æ•è·ä¸€ä¸ªæˆ–å¤šä¸ªéç©ºç™½å­—ç¬¦ï¼ˆè¡¨åï¼‰
            # \s+as\s+  : åŒ¹é…ç©ºç™½å­—ç¬¦ + as + ç©ºç™½å­—ç¬¦
            pattern = r'\binto\s+([^\s]+)\s+as\s+'
            
            match = re.search(pattern, cleaned_sql, re.IGNORECASE)
            
            if match:
                target_table = match.group(1)
                # ç§»é™¤å¯èƒ½çš„åˆ†å·æˆ–å…¶ä»–æ ‡ç‚¹ç¬¦å·
                target_table = target_table.rstrip(';').rstrip(',')
                #print(f"ä»SQLä¸­è§£æå‡ºç›®æ ‡è¡¨: {target_table}")
                return target_table
            else:
                # å¦‚æœç¬¬ä¸€ä¸ªæ¨¡å¼æ²¡åŒ¹é…åˆ°ï¼Œå°è¯•æ›´å®½æ¾çš„æ¨¡å¼
                # æœ‰äº›SQLå¯èƒ½æ ¼å¼ä¸åŒï¼Œå°è¯•åªåŒ¹é…intoåé¢çš„ç¬¬ä¸€ä¸ªæ ‡è¯†ç¬¦
                pattern2 = r'\binto\s+([^\s\n]+)'
                match2 = re.search(pattern2, cleaned_sql, re.IGNORECASE)
                
                if match2:
                    target_table = match2.group(1)
                    target_table = target_table.rstrip(';').rstrip(',')
                    print(f"ä½¿ç”¨å¤‡ç”¨æ¨¡å¼è§£æå‡ºç›®æ ‡è¡¨: {target_table}")
                    return target_table
                else:
                    print(f"æ— æ³•ä»SQLä¸­è§£æç›®æ ‡è¡¨: {cleaned_sql[:200]}...")
                    return None
                    
        except Exception as e:
            print(f"è§£æSQLæ—¶å‡ºé”™: {str(e)}")
            return None
                
    def get_stream_target_tables(self):
        """è·å–æµè®¡ç®—çš„ç›®æ ‡è¡¨åˆ—è¡¨
        
        Returns:
            list: ç›®æ ‡è¡¨ååˆ—è¡¨
        """
        try:
            conn, cursor = self.get_connection()
            
            # æŸ¥è¯¢æ‰€æœ‰æµä¿¡æ¯
            cursor.execute("select stream_name, sql from information_schema.ins_streams")
            streams = cursor.fetchall()
            print(f"ä» information_schema.ins_streams æŸ¥è¯¢åˆ° {len(streams)} ä¸ªæµ")
        
            
            target_tables = []
            for stream in streams:
                stream_name = stream[0]
                sql = stream[1]
            
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æSQLä¸­çš„ç›®æ ‡è¡¨
                target_table = self._parse_target_table_from_sql(sql)
                
                if target_table:
                    target_tables.append({
                        'stream_name': stream_name,
                        'target_table': target_table,
                        'target_db': target_table.split('.')[0] if '.' in target_table else 'stream_to',
                        'target_table_name': target_table.split('.')[-1],
                        'sql': sql  # ä¿å­˜åŸå§‹SQLç”¨äºè°ƒè¯•
                    })
                    #print(f"  æµ: {stream_name} -> ç›®æ ‡è¡¨: {target_table}")
                else:
                    print(f"  è­¦å‘Š: æ— æ³•è§£ææµ {stream_name} çš„ç›®æ ‡è¡¨ï¼ŒSQL: {sql[:100]}...")
            
            cursor.close()
            conn.close()
            
            if len(target_tables) == 0:
                print("è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•æµæˆ–æ— æ³•è§£æç›®æ ‡è¡¨")
            else:
                print(f"æˆåŠŸè§£æ {len(target_tables)} ä¸ªæµçš„ç›®æ ‡è¡¨")
                
            return target_tables
            
        except Exception as e:
            print(f"è·å–æµç›®æ ‡è¡¨å¤±è´¥: {str(e)}")
            return []

    def check_stream_computation_delay(self):
        """æ£€æŸ¥æµè®¡ç®—å»¶è¿Ÿ
        
        Returns:
            dict: å»¶è¿Ÿæ£€æŸ¥ç»“æœ
        """
        try:
            conn, cursor = self.get_connection()
            
            # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨
            try:
                cursor.execute("show databases")
                databases = cursor.fetchall()
                db_names = [db[0] for db in databases]
                #print(f"å‘ç°æ•°æ®åº“: {db_names}")
                
                if 'stream_from' not in db_names:
                    print("é”™è¯¯: æºæ•°æ®åº“ stream_from ä¸å­˜åœ¨")
                    cursor.close()
                    conn.close()
                    return None
            except Exception as e:
                print(f"æŸ¥è¯¢æ•°æ®åº“åˆ—è¡¨å¤±è´¥: {str(e)}")
                cursor.close()
                conn.close()
                return None
        
            # è·å–æºè¡¨æœ€æ–°æ—¶é—´æˆ³
            try:
                cursor.execute("select last(ts) from stream_from.stb")
                source_result = cursor.fetchall()
                #print(f"æºè¡¨æŸ¥è¯¢ç»“æœ: {source_result}")
            except Exception as e:
                print(f"æŸ¥è¯¢æºè¡¨å¤±è´¥: {str(e)}")
                cursor.close()
                conn.close()
                return None
            
            if not source_result or not source_result[0][0]:
                print("è­¦å‘Š: æ— æ³•è·å–æºè¡¨æœ€æ–°æ—¶é—´æˆ³")
                cursor.close()
                conn.close()
                return None
                
            source_last_ts = source_result[0][0]
            source_ts_ms = int(source_last_ts.timestamp() * 1000)
            print(f"æºè¡¨æœ€æ–°æ—¶é—´æˆ³: {source_last_ts} ({source_ts_ms}ms)")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æµå­˜åœ¨
            print("æŸ¥è¯¢æµçš„è¯¦ç»†ä¿¡æ¯...")
            try:
                cursor.execute("select stream_name, sql from information_schema.ins_streams")
                streams_info = cursor.fetchall()
                print(f"å‘ç° {len(streams_info)} ä¸ªæµ:")
                for stream in streams_info:
                    print(f"  æµåç§°: {stream[0]}")
                    #print(f"  SQL: {stream[1][:100]}...")  # åªæ˜¾ç¤ºå‰100ä¸ªå­—ç¬¦
            except Exception as e:
                print(f"æŸ¥è¯¢æµè¯¦ç»†ä¿¡æ¯å¤±è´¥: {str(e)}")
            
            # è·å–æµç›®æ ‡è¡¨
            target_tables = self.get_stream_target_tables()
            if not target_tables:
                print("è­¦å‘Š: æœªæ‰¾åˆ°æµç›®æ ‡è¡¨")
                cursor.close()
                conn.close()
                return None
            
            delay_results = {
                'check_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                'source_last_ts': source_last_ts,
                'source_ts_ms': source_ts_ms,
                'streams': []
            }
            
            # æ£€æŸ¥æ¯ä¸ªæµçš„å»¶è¿Ÿ
            for table_info in target_tables:
                try:
                    target_table = table_info['target_table']
                    stream_name = table_info['stream_name']
                    
                    #print(f"æ£€æŸ¥æµ {stream_name} çš„ç›®æ ‡è¡¨ {target_table}")
                    
                    # å…ˆæ£€æŸ¥ç›®æ ‡è¡¨æ˜¯å¦å­˜åœ¨
                    try:
                        cursor.execute(f"describe {target_table}")
                        #print(f"ç›®æ ‡è¡¨ {target_table} ç»“æ„æ­£å¸¸")
                    except Exception as e:
                        print(f"ç›®æ ‡è¡¨ {target_table} ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®")
                        
                        stream_result = {
                            'stream_name': stream_name,
                            'target_table': target_table,
                            'error': f"ç›®æ ‡è¡¨ä¸å­˜åœ¨: {str(e)}",
                            'status': 'ERROR'
                        }
                        delay_results['streams'].append(stream_result)
                        continue
                    
                    # æŸ¥è¯¢ç›®æ ‡è¡¨æœ€æ–°æ—¶é—´æˆ³
                    cursor.execute(f"select last(ts) from {target_table}")
                    target_result = cursor.fetchall()
                    #print(f"ç›®æ ‡è¡¨ {target_table} æŸ¥è¯¢ç»“æœ: {target_result}")
                    
                    if target_result and target_result[0][0]:
                        target_last_ts = target_result[0][0]
                        target_ts_ms = int(target_last_ts.timestamp() * 1000)
                        
                        # è®¡ç®—å»¶è¿Ÿ(æ¯«ç§’)
                        delay_ms = source_ts_ms - target_ts_ms
                        
                        stream_result = {
                            'stream_name': stream_name,
                            'target_table': target_table,
                            'target_last_ts': target_last_ts,
                            'delay_ms': delay_ms,
                            'delay_seconds': delay_ms / 1000.0,
                            'is_lagging': delay_ms > self.max_delay_threshold,
                            'status': 'LAGGING' if delay_ms > self.max_delay_threshold else 'OK'
                        }
                        
                        print(f"æµ {stream_name} å»¶è¿Ÿ: {delay_ms}ms ({delay_ms/1000.0:.2f}ç§’)")
                        
                    else:
                        # ç›®æ ‡è¡¨æ— æ•°æ®
                        stream_result = {
                            'stream_name': stream_name,
                            'target_table': target_table,
                            'target_last_ts': None,
                            'delay_ms': None,
                            'delay_seconds': None,
                            'is_lagging': True,
                            'status': 'NO_DATA'
                        }
                    
                    delay_results['streams'].append(stream_result)
                    
                except Exception as e:
                    error_msg = f"æ£€æŸ¥æµ {stream_name} å»¶è¿Ÿæ—¶å‡ºé”™: {str(e)}"
                    print(error_msg)
                    stream_result = {
                        'stream_name': stream_name,
                        'target_table': target_table,
                        'error': str(e),
                        'status': 'ERROR'
                    }
                    delay_results['streams'].append(stream_result)
            
            cursor.close()
            conn.close()
            
            return delay_results
            
        except Exception as e:
            print(f"æ£€æŸ¥æµè®¡ç®—å»¶è¿Ÿæ—¶å‡ºé”™: {str(e)}")
            return None

    def log_stream_delay_results(self, delay_results):
        """è®°å½•æµå»¶è¿Ÿæ£€æŸ¥ç»“æœåˆ°æ—¥å¿—æ–‡ä»¶"""
        if not delay_results:
            return
            
        try:           
            # åŸºäº max_delay_threshold è®¡ç®—åŠ¨æ€é˜ˆå€¼
            base_threshold_ms = self.max_delay_threshold
            
            # å®šä¹‰å€æ•°é˜ˆå€¼
            excellent_threshold = base_threshold_ms * 0.1    # 0.1å€æ£€æŸ¥é—´éš” - ä¼˜ç§€
            good_threshold = base_threshold_ms * 0.5         # 0.5å€æ£€æŸ¥é—´éš” - è‰¯å¥½  
            normal_threshold = base_threshold_ms * 1.0       # 1å€æ£€æŸ¥é—´éš” - æ­£å¸¸
            mild_delay_threshold = base_threshold_ms * 6.0   # 6å€æ£€æŸ¥é—´éš” - è½»å¾®å»¶è¿Ÿ
            obvious_delay_threshold = base_threshold_ms * 30.0  # 30å€æ£€æŸ¥é—´éš” - æ˜æ˜¾å»¶è¿Ÿ
            
            with open(self.delay_log_file, 'a') as f:
                # å†™å…¥æ£€æŸ¥æ—¶é—´å’Œæºè¡¨ä¿¡æ¯
                f.write(f"\n{'='*80}\n")
                f.write(f"æ£€æŸ¥æ—¶é—´: {delay_results['check_time']}\n")
                f.write(f"æºè¡¨æœ€æ–°æ—¶é—´: {delay_results['source_last_ts']}\n")
                f.write(f"æºè¡¨æ—¶é—´æˆ³(ms): {delay_results['source_ts_ms']}\n")
                f.write(f"å»¶è¿Ÿé˜ˆå€¼: {format_delay_time(self.max_delay_threshold)}\n")
                f.write(f"æ£€æŸ¥é¢‘ç‡: {self.delay_check_interval}ç§’\n")
                f.write(f"-" * 80 + "\n")
                
                # å†™å…¥æ¯ä¸ªæµçš„å»¶è¿Ÿä¿¡æ¯
                for stream in delay_results['streams']:
                    f.write(f"æµåç§°: {stream['stream_name']}\n")
                    f.write(f"ç›®æ ‡è¡¨: {stream['target_table']}\n")
                    f.write(f"çŠ¶æ€: {stream['status']}\n")
                    
                    if stream['status'] == 'OK' or stream['status'] == 'LAGGING':
                        f.write(f"ç›®æ ‡è¡¨æœ€æ–°æ—¶é—´: {stream['target_last_ts']}\n")
                        delay_ms = stream['delay_ms']
                        f.write(f"å»¶è¿Ÿ: {format_delay_time(delay_ms)}\n")
                        
                        # è®¡ç®—å»¶è¿Ÿå€æ•°å¹¶æ·»åŠ ä¸­æ–‡å»¶è¿Ÿç­‰çº§åˆ¤æ–­å’Œè®°å½•
                        delay_multiplier = delay_ms / base_threshold_ms
                        
                        # æ·»åŠ ä¸­æ–‡å»¶è¿Ÿç­‰çº§åˆ¤æ–­å’Œè®°å½•
                        if delay_ms < excellent_threshold:  # ä¼˜ç§€
                            delay_level = "ä¼˜ç§€"
                            delay_desc = f"(< 0.1å€é—´éš”, {delay_multiplier:.2f}å€)"
                        elif delay_ms < good_threshold:  # è‰¯å¥½
                            delay_level = "è‰¯å¥½"
                            delay_desc = f"(0.1-0.5å€é—´éš”, {delay_multiplier:.2f}å€)"
                        elif delay_ms < normal_threshold:  # æ­£å¸¸
                            delay_level = "æ­£å¸¸"
                            delay_desc = f"(0.5-1å€é—´éš”, {delay_multiplier:.2f}å€)"
                        elif delay_ms < mild_delay_threshold:  # è½»å¾®å»¶è¿Ÿ
                            delay_level = "è½»å¾®å»¶è¿Ÿ"
                            delay_desc = f"(1-6å€é—´éš”, {delay_multiplier:.2f}å€)"
                        elif delay_ms < obvious_delay_threshold:  # æ˜æ˜¾å»¶è¿Ÿ
                            delay_level = "æ˜æ˜¾å»¶è¿Ÿ"
                            delay_desc = f"(6-30å€é—´éš”, {delay_multiplier:.2f}å€)"
                        else:  # ä¸¥é‡å»¶è¿Ÿ
                            delay_level = "ä¸¥é‡å»¶è¿Ÿ"
                            delay_desc = f"(> 30å€é—´éš”, {delay_multiplier:.2f}å€)"
                        
                        # å†™å…¥ä¸­æ–‡å»¶è¿Ÿç­‰çº§
                        f.write(f"å»¶è¿Ÿç­‰çº§: {delay_level}\n")
                        f.write(f"å»¶è¿Ÿå€æ•°: {delay_desc}\n")
                        
                        if stream['is_lagging']:
                            f.write(f"è­¦å‘Š: å»¶è¿Ÿè¶…è¿‡é…ç½®é˜ˆå€¼ {format_delay_time(self.max_delay_threshold)}!\n")
                            
                    elif stream['status'] == 'NO_DATA':
                        f.write(f"è­¦å‘Š: ç›®æ ‡è¡¨æ— æ•°æ®\n")
                        f.write(f"å»¶è¿Ÿç­‰çº§: ä¸¥é‡å»¶è¿Ÿ\n")  # æ— æ•°æ®ä¹Ÿç®—ä¸¥é‡å»¶è¿Ÿ
                        f.write(f"å»¶è¿Ÿå€æ•°: (æ— æ•°æ®çŠ¶æ€)\n")
                        
                    elif stream['status'] == 'ERROR':
                        f.write(f"é”™è¯¯: {stream.get('error', 'æœªçŸ¥é”™è¯¯')}\n")
                        f.write(f"å»¶è¿Ÿç­‰çº§: ä¸¥é‡å»¶è¿Ÿ\n")  # é”™è¯¯ä¹Ÿç®—ä¸¥é‡å»¶è¿Ÿ
                        f.write(f"å»¶è¿Ÿå€æ•°: (é”™è¯¯çŠ¶æ€)\n")
                    
                    f.write(f"-" * 40 + "\n")
                    
        except Exception as e:
            print(f"å†™å…¥å»¶è¿Ÿæ—¥å¿—å¤±è´¥: {str(e)}")

    def print_stream_delay_summary(self, delay_results):
        """æ‰“å°æµå»¶è¿Ÿæ£€æŸ¥æ‘˜è¦"""
        if not delay_results:
            return
        
        c = Colors.get_colors()
        
        # åŸºäº delay_check_interval è®¡ç®—åŠ¨æ€é˜ˆå€¼
        base_threshold_ms = self.max_delay_threshold
        
        # å®šä¹‰å€æ•°é˜ˆå€¼
        excellent_threshold = base_threshold_ms * 0.1    # 0.1å€æ£€æŸ¥é—´éš” - ä¼˜ç§€
        good_threshold = base_threshold_ms * 0.5         # 0.5å€æ£€æŸ¥é—´éš” - è‰¯å¥½  
        normal_threshold = base_threshold_ms * 1.0       # 1å€æ£€æŸ¥é—´éš” - æ­£å¸¸
        mild_delay_threshold = base_threshold_ms * 6.0   # 6å€æ£€æŸ¥é—´éš” - è½»å¾®å»¶è¿Ÿ
        obvious_delay_threshold = base_threshold_ms * 30.0  # 30å€æ£€æŸ¥é—´éš” - æ˜æ˜¾å»¶è¿Ÿ
            
        print(f"\n=== æµè®¡ç®—å»¶è¿Ÿæ£€æŸ¥ ({delay_results['check_time']}) ===")
        print(f"æºè¡¨æœ€æ–°æ—¶é—´: {delay_results['source_last_ts']}")
        print(f"é…ç½®ä¿¡æ¯: å­è¡¨æ•°é‡({self.table_count}) | æ¯è½®æ’å…¥è®°å½•æ•°({self.real_time_batch_rows}) | vgroups({self.vgroups}) | æ•°æ®ä¹±åº({self.disorder_ratio})")
        print(f"å»¶è¿Ÿåˆ¤æ–­åŸºå‡†: æœ€å¤§å»¶è¿Ÿé˜ˆå€¼ {format_delay_time(self.max_delay_threshold)} | æ£€æŸ¥é¢‘ç‡ {self.delay_check_interval}s | éƒ¨ç½²æ¨¡å¼({self.deployment_mode}) | SQLç±»å‹({self.sql_type})")
            
        ok_count = 0
        lagging_count = 0
        no_data_count = 0
        error_count = 0
        
        # æŒ‰å»¶è¿Ÿçº§åˆ«åˆ†ç±»ç»Ÿè®¡
        excellent_count = 0
        good_count = 0
        normal_count = 0
        mild_delay_count = 0
        obvious_delay_count = 0
        severe_delay_count = 0
    
        for stream in delay_results['streams']:
            status = stream['status']
            stream_name = stream['stream_name']
            
            if status == 'OK':
                ok_count += 1
                target_time = stream['target_last_ts']
                delay_ms = stream['delay_ms']
                # æ ¹æ®å»¶è¿Ÿæ—¶é—´é€‰æ‹©å›¾æ ‡å’Œæè¿°
                if delay_ms < excellent_threshold:  #ä¼˜ç§€
                    excellent_count += 1
                    status_icon = "ğŸŸ¢" if Colors.supports_color() else "âœ“"
                    delay_desc = f"ä¼˜ç§€(<{format_delay_time(excellent_threshold)})"
                    color = c.GREEN
                elif delay_ms < good_threshold:  # è‰¯å¥½
                    good_count += 1
                    status_icon = "ğŸŸ¢" if Colors.supports_color() else "âœ“"
                    delay_desc = f"è‰¯å¥½(<{format_delay_time(good_threshold)})"
                    color = c.GREEN
                else:  # delay_ms < normal_threshold (åœ¨é˜ˆå€¼å†…) - æ­£å¸¸
                    normal_count += 1
                    status_icon = "ğŸŸ¡" if Colors.supports_color() else "âœ“"
                    delay_desc = f"æ­£å¸¸(<{format_delay_time(normal_threshold)})"
                    color = c.YELLOW
                    
                print(f"{color}{status_icon} {stream_name}: å»¶è¿Ÿ {format_delay_time(delay_ms)} - {delay_desc}{c.END}")
                print(f"  {c.BLUE}ç›®æ ‡è¡¨æœ€æ–°æ—¶é—´: {target_time}{c.END}")
            
            elif status == 'LAGGING':
                lagging_count += 1
                target_time = stream['target_last_ts']
                delay_ms = stream['delay_ms']
            
                # æ ¹æ®å»¶è¿Ÿç¨‹åº¦é€‰æ‹©é¢œè‰²å’Œå›¾æ ‡
                if delay_ms < mild_delay_threshold:  # è½»å¾®å»¶è¿Ÿ
                    mild_delay_count += 1
                    status_icon = "ğŸŸ¡" if Colors.supports_color() else "âš "
                    color = c.YELLOW
                    delay_desc = f"è½»å¾®å»¶è¿Ÿ(<{format_delay_time(mild_delay_threshold)})"
                elif delay_ms < obvious_delay_threshold:  # æ˜æ˜¾å»¶è¿Ÿ
                    obvious_delay_count += 1
                    status_icon = "ğŸŸ " if Colors.supports_color() else "âš "
                    color = c.YELLOW
                    delay_desc = f"æ˜æ˜¾å»¶è¿Ÿ(<{format_delay_time(obvious_delay_threshold)})"
                else:  # ä¸¥é‡å»¶è¿Ÿ
                    severe_delay_count += 1
                    status_icon = "ğŸ”´" if Colors.supports_color() else "âš "
                    color = c.RED
                    delay_desc = f"ä¸¥é‡å»¶è¿Ÿ(>{format_delay_time(obvious_delay_threshold)})"
                    
                print(f"{color}{c.BOLD}{status_icon} {stream_name}: å»¶è¿Ÿ {format_delay_time(delay_ms)} - {delay_desc}!{c.END}")
                print(f"  {c.BLUE}ç›®æ ‡è¡¨æœ€æ–°æ—¶é—´: {target_time}{c.END}")
                
            elif status == 'NO_DATA':
                no_data_count += 1
                severe_delay_count += 1
                status_icon = "âŒ" if Colors.supports_color() else "âœ—"
                print(f"{c.RED}{status_icon} {stream_name}: ç›®æ ‡è¡¨æœªç”Ÿæˆæ•°æ®{c.END}")
                
            elif status == 'ERROR':
                error_count += 1
                severe_delay_count += 1
                status_icon = "ğŸ’¥" if Colors.supports_color() else "âœ—"
                print(f"{c.RED}{c.BOLD}{status_icon} {stream_name}: ç›®æ ‡è¡¨ä¸å­˜åœ¨æˆ–æ£€æŸ¥å‡ºé”™{c.END}")
                error_msg = stream.get('error', 'æœªçŸ¥é”™è¯¯')
                print(f"  {c.RED}é”™è¯¯ä¿¡æ¯: {error_msg}{c.END}")
 
    
        # è®¡ç®—æ€»æ•°å’Œç™¾åˆ†æ¯”
        total_streams = len(delay_results['streams'])
        
        def get_percentage(count, total):
            return f"{(count/total*100):.1f}%" if total > 0 else "0.0%"
           
        # æ‘˜è¦ä¿¡æ¯å¸¦é¢œè‰²å’Œå›¾æ ‡
        summary_parts = []
        # æ­£å¸¸æµ
        if ok_count > 0:
            ok_icon = "ğŸŸ¢" if Colors.supports_color() else ""
            ok_percent = get_percentage(ok_count, total_streams)
            summary_parts.append(f"{c.GREEN}{ok_icon}æ­£å¸¸({ok_count}/{ok_percent}){c.END}")
            
        # å»¶è¿Ÿæµ
        if lagging_count > 0:
            lag_icon = "ğŸŸ¡" if Colors.supports_color() else ""
            lag_percent = get_percentage(lagging_count, total_streams)
            summary_parts.append(f"{c.YELLOW}{lag_icon}å»¶è¿Ÿ({lagging_count}/{lag_percent}){c.END}")
            
        # æœªç”Ÿæˆæ•°æ®çš„æµ     
        if no_data_count > 0:
            no_data_icon = "âŒ" if Colors.supports_color() else ""
            no_data_percent = get_percentage(no_data_count, total_streams)
            summary_parts.append(f"{c.RED}{no_data_icon}æœªç”Ÿæˆæ•°æ®({no_data_count}/{no_data_percent}){c.END}")
            
        # é”™è¯¯çŠ¶æ€çš„æµ    
        if error_count > 0:
            error_icon = "ğŸ’¥" if Colors.supports_color() else ""
            error_percent = get_percentage(error_count, total_streams)
            summary_parts.append(f"{c.RED}{error_icon}è¡¨ä¸å­˜åœ¨({error_count}/{error_percent}){c.END}")
        
        print(f"\n{c.BOLD}ğŸ“Š æ‘˜è¦ (æ€»æµæ•°: {total_streams}): {' '.join(summary_parts)}{c.END}")

        # è¯¦ç»†åˆ†å¸ƒç»Ÿè®¡
        if ok_count > 0 or lagging_count > 0:
            print(f"\n{c.CYAN}ğŸ“ˆ å»¶è¿Ÿåˆ†å¸ƒè¯¦æƒ…:{c.END}")
            
            # æ­£å¸¸çŠ¶æ€åˆ†å¸ƒ
            if ok_count > 0:
                print(f"  {c.GREEN}æ­£å¸¸çŠ¶æ€åˆ†å¸ƒ:{c.END}")
                if excellent_count > 0:
                    excellent_percent = get_percentage(excellent_count, total_streams)
                    print(f"    ğŸŸ¢ ä¼˜ç§€: {excellent_count} ({excellent_percent})")
                if good_count > 0:
                    good_percent = get_percentage(good_count, total_streams)
                    print(f"    ğŸŸ¢ è‰¯å¥½: {good_count} ({good_percent})")
                if normal_count > 0:
                    normal_percent = get_percentage(normal_count, total_streams)
                    print(f"    ğŸŸ¡ æ­£å¸¸: {normal_count} ({normal_percent})")
            
            # å»¶è¿ŸçŠ¶æ€åˆ†å¸ƒ
            if lagging_count > 0:
                print(f"  {c.YELLOW}å»¶è¿ŸçŠ¶æ€åˆ†å¸ƒ:{c.END}")
                if mild_delay_count > 0:
                    mild_percent = get_percentage(mild_delay_count, total_streams)
                    print(f"    ğŸŸ¡ è½»å¾®å»¶è¿Ÿ: {mild_delay_count} ({mild_percent})")
                if obvious_delay_count > 0:
                    obvious_percent = get_percentage(obvious_delay_count, total_streams)
                    print(f"    ğŸŸ  æ˜æ˜¾å»¶è¿Ÿ: {obvious_delay_count} ({obvious_percent})")
                if severe_delay_count > 0:
                    severe_percent = get_percentage(severe_delay_count, total_streams)
                    print(f"    ğŸ”´ ä¸¥é‡å»¶è¿Ÿ: {severe_delay_count} ({severe_percent})")
    
        # é—®é¢˜æµç»Ÿè®¡
        if no_data_count > 0 or error_count > 0:
            print(f"\n{c.RED}âš ï¸ é—®é¢˜æµç»Ÿè®¡:{c.END}")
            if no_data_count > 0:
                no_data_percent = get_percentage(no_data_count, total_streams)
                print(f"  âŒ ç›®æ ‡è¡¨æœªç”Ÿæˆæ•°æ®: {no_data_count} ({no_data_percent}) ")
            if error_count > 0:
                error_percent = get_percentage(error_count, total_streams)
                print(f"  ğŸ’¥ ç›®æ ‡è¡¨ä¸å­˜åœ¨: {error_count} ({error_percent}) ")
           
        # æ˜¾ç¤ºé˜ˆå€¼å‚è€ƒä¿¡æ¯
        print(f"\n{c.CYAN}å»¶è¿Ÿç­‰çº§å‚è€ƒ (åŸºäºæ£€æŸ¥é—´éš” {format_delay_time(self.max_delay_threshold)}):{c.END}")
        print(f"  ğŸŸ¢ ä¼˜ç§€: < {format_delay_time(excellent_threshold)} (0.1å€é—´éš”)")
        print(f"  ğŸŸ¢ è‰¯å¥½: {format_delay_time(excellent_threshold)} - {format_delay_time(good_threshold)} (0.1-0.5å€é—´éš”)")
        print(f"  ğŸŸ¡ æ­£å¸¸: {format_delay_time(good_threshold)} - {format_delay_time(normal_threshold)} (0.5-1å€é—´éš”)")
        print(f"  ğŸŸ¡ è½»å¾®å»¶è¿Ÿ: {format_delay_time(normal_threshold)} - {format_delay_time(mild_delay_threshold)} (1-6å€é—´éš”)")
        print(f"  ğŸŸ  æ˜æ˜¾å»¶è¿Ÿ: {format_delay_time(mild_delay_threshold)} - {format_delay_time(obvious_delay_threshold)} (6-30å€é—´éš”)")
        print(f"  ğŸ”´ ä¸¥é‡å»¶è¿Ÿ: > {format_delay_time(obvious_delay_threshold)} (>30å€é—´éš”)")
        print(f"\n{c.CYAN}ç›‘æ§é¢‘ç‡: æ¯ {self.delay_check_interval}ç§’ æ£€æŸ¥ä¸€æ¬¡å»¶è¿ŸçŠ¶æ€{c.END}")
        
        # çŠ¶æ€è¯„ä¼°
        healthy_count = ok_count  # åªæœ‰æ­£å¸¸çŠ¶æ€æ‰ç®—å¥åº·
        problem_count = lagging_count + no_data_count + error_count # å»¶è¿Ÿã€æ— æ•°æ®ã€é”™è¯¯éƒ½æ˜¯é—®é¢˜
        
        # è­¦å‘Šå’Œå»ºè®®ä¿¡æ¯
        if problem_count > 0:
            if no_data_count > 0 or error_count > 0:
                # æœ‰ä¸¥é‡é—®é¢˜ï¼ˆæ— æ•°æ®æˆ–é”™è¯¯ï¼‰
                critical_count = no_data_count + error_count
                critical_percent = get_percentage(critical_count, total_streams)
                warning_icon = "ğŸš¨" if Colors.supports_color() else "âš "
                print_error(f"{warning_icon} ä¸¥é‡: {critical_count} ä¸ªæµå­˜åœ¨ä¸¥é‡é—®é¢˜ ({critical_percent})!")
                
            if lagging_count > 0:
                # æœ‰å»¶è¿Ÿé—®é¢˜
                lag_percent = get_percentage(lagging_count, total_streams)
                warning_icon = "âš ï¸" if Colors.supports_color() else "âš "
                print_warning(f"{warning_icon} å»¶è¿Ÿ: {lagging_count} ä¸ªæµè®¡ç®—å‡ºç°å»¶è¿Ÿ ({lag_percent})!")
            
        # æ•´ä½“çŠ¶æ€è¯„ä¼°
        if error_count > 0 or no_data_count > 0:
            # å­˜åœ¨ä¸¥é‡é—®é¢˜
            advice_icon = "ğŸš¨" if Colors.supports_color() else "ğŸ’¡"
            problem_ratio = (error_count + no_data_count) / total_streams * 100
            print_error(f"{advice_icon} å‘Šè­¦: {problem_ratio:.1f}% çš„æµå­˜åœ¨ä¸¥é‡é—®é¢˜ - éœ€è¦ç«‹å³æ£€æŸ¥")
        elif lagging_count > healthy_count:
            # å»¶è¿Ÿé—®é¢˜è¾ƒå¤š
            advice_icon = "ğŸ’¡" if Colors.supports_color() else "ğŸ’¡"
            lag_ratio = lagging_count / total_streams * 100
            print_error(f"{advice_icon} è­¦å‘Š: {lag_ratio:.1f}% çš„æµå‡ºç°å»¶è¿Ÿ - å»ºè®®ä¼˜åŒ–æ€§èƒ½")
        elif lagging_count > 0:
            # å°‘é‡å»¶è¿Ÿ
            tip_icon = "ğŸ’¡" if Colors.supports_color() else "ğŸ’¡"
            lag_ratio = lagging_count / total_streams * 100
            print_warning(f"{tip_icon} æç¤º: {lag_ratio:.1f}% çš„æµå‡ºç°å»¶è¿Ÿ - æŒç»­å…³æ³¨")
        elif healthy_count == total_streams:
            # å…¨éƒ¨æ­£å¸¸
            success_icon = "âœ…" if Colors.supports_color() else "âœ“"
            print_success(f"{success_icon} çŠ¶æ€ä¼˜ç§€: æ‰€æœ‰æµè®¡ç®—éƒ½æ­£å¸¸è¿è¡Œ (100%)")
            
        # è°ƒä¼˜å»ºè®®
        if problem_count > 0:
            print(f"\n{c.CYAN}ğŸ’¡ é—®é¢˜åˆ†æä¸å»ºè®®:{c.END}")
            
            if error_count > 0:
                print(f"  ğŸ”´ ç›®æ ‡è¡¨ä¸å­˜åœ¨é—®é¢˜:")
                print(f"    - {error_count} ä¸ªæµçš„ç›®æ ‡è¡¨æ— æ³•è®¿é—®")
                print(f"    - å¯èƒ½åŸå› : æµåˆ›å»ºå¤±è´¥ã€ç›®æ ‡è¡¨è¿˜æœªç”Ÿæˆ")
                
            if no_data_count > 0:
                print(f"  âŒ ç›®æ ‡è¡¨æ— æ•°æ®é—®é¢˜:")
                print(f"    - {no_data_count} ä¸ªæµçš„ç›®æ ‡è¡¨å­˜åœ¨ä½†æ— æ•°æ®")
                print(f"    - å¯èƒ½åŸå› : æµè®¡ç®—é€»è¾‘é—®é¢˜ã€æºæ•°æ®ä¸åŒ¹é…ã€æµæœªå¯åŠ¨")
                print(f"    - å»ºè®®: æ£€æŸ¥æµçŠ¶æ€ã€éªŒè¯æºæ•°æ®ã€æ£€æŸ¥æµè®¡ç®—é€»è¾‘")
                
            if severe_delay_count > 0:
                severe_ratio = severe_delay_count / total_streams * 100
                print(f"  ğŸ”´ ä¸¥é‡å»¶è¿Ÿé—®é¢˜:")
                print(f"    - {severe_delay_count} ä¸ªæµå­˜åœ¨ä¸¥é‡å»¶è¿Ÿ ({severe_ratio:.1f}%)")
                print(f"    - å»ºè®®: ç«‹å³æ£€æŸ¥ç³»ç»Ÿèµ„æºã€ä¼˜åŒ–æµè®¡ç®—é€»è¾‘")
                
            if obvious_delay_count > 0:
                obvious_ratio = obvious_delay_count / total_streams * 100
                print(f"  ğŸŸ  æ˜æ˜¾å»¶è¿Ÿé—®é¢˜:")
                print(f"    - {obvious_delay_count} ä¸ªæµå­˜åœ¨æ˜æ˜¾å»¶è¿Ÿ ({obvious_ratio:.1f}%)")
                print(f"    - å»ºè®®: æ£€æŸ¥ç³»ç»Ÿè´Ÿè½½ã€è€ƒè™‘å¢åŠ èµ„æº")
                
            print(f"  ğŸ“Š ç³»ç»Ÿè°ƒä¼˜å»ºè®®:")
            if problem_count > total_streams * 0.5:
                print(f"    - è¶…è¿‡ä¸€åŠçš„æµå­˜åœ¨é—®é¢˜ï¼Œå»ºè®®å…¨é¢æ£€æŸ¥ç³»ç»Ÿé…ç½®")
            print(f"    - å½“å‰å†™å…¥é—´éš”: {self.real_time_batch_sleep}sï¼Œå¯é€‚å½“å¢åŠ ä»¥å‡è½»å‹åŠ›")
            print(f"    - å¯è°ƒæ•´ --delay-check-interval å‚æ•°æ”¹å˜ç›‘æ§é¢‘ç‡")
            
                        

    def start_stream_delay_monitor(self):
        """å¯åŠ¨æµå»¶è¿Ÿç›‘æ§çº¿ç¨‹"""
        if not self.check_stream_delay:
            return None
            
        def delay_monitor():
            """å»¶è¿Ÿç›‘æ§çº¿ç¨‹å‡½æ•°"""
            print(f"å¯åŠ¨æµå»¶è¿Ÿç›‘æ§ (é—´éš”: {self.delay_check_interval}ç§’)")
            print(f"å»¶è¿Ÿæ—¥å¿—æ–‡ä»¶: {self.delay_log_file}")
            
            # ç¡®ä¿å»¶è¿Ÿæ—¥å¿—æ–‡ä»¶çš„ç›®å½•å­˜åœ¨
            delay_log_dir = os.path.dirname(self.delay_log_file)
            if not os.path.exists(delay_log_dir):
                try:
                    os.makedirs(delay_log_dir, exist_ok=True)
                    print(f"åˆ›å»ºå»¶è¿Ÿæ—¥å¿—ç›®å½•: {delay_log_dir}")
                except Exception as e:
                    print(f"åˆ›å»ºå»¶è¿Ÿæ—¥å¿—ç›®å½•å¤±è´¥: {str(e)}")
                    return
            
            # åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶
            try:
                with open(self.delay_log_file, 'w') as f:
                    f.write(f"TDengine æµè®¡ç®—å»¶è¿Ÿç›‘æ§æ—¥å¿—\n")
                    f.write(f"å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"æœ€å¤§å»¶è¿Ÿé˜ˆå€¼: {self.max_delay_threshold}ms\n")
                    f.write(f"æ£€æŸ¥é—´éš”: {self.delay_check_interval}ç§’\n")
                    f.write(f"æ•°æ®å†™å…¥é—´éš”: {self.real_time_batch_sleep}ç§’\n") 
                    f.write(f"æ¯è½®å†™å…¥è®°å½•æ•°: {self.real_time_batch_rows}\n")
                    f.write(f"å­è¡¨æ•°é‡: {self.table_count}\n")
                    f.write(f"è¿è¡Œæ—¶é—´: {self.runtime}åˆ†é’Ÿ\n")
                    f.write(f"SQLç±»å‹: {self.sql_type}\n")
                    f.write(f"="*80 + "\n")
            except Exception as e:
                print(f"åˆå§‹åŒ–å»¶è¿Ÿæ—¥å¿—æ–‡ä»¶å¤±è´¥: {str(e)}")
            
            start_time = time.time()
            check_count = 0
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©æµåˆ›å»ºå®Œæˆ
            initial_wait = 15  # ç­‰å¾…15ç§’
            print(f"ç­‰å¾… {initial_wait} ç§’è®©æµè®¡ç®—å¯åŠ¨...")
            time.sleep(initial_wait)
        
            while time.time() - start_time < self.runtime * 60:
                check_count += 1
                try:
                    print(f"\n--- ç¬¬ {check_count} æ¬¡å»¶è¿Ÿæ£€æŸ¥ ---")
                    
                    # è®°å½•æ£€æŸ¥å¼€å§‹
                    with open(self.delay_log_file, 'a') as f:
                        f.write(f"\nç¬¬ {check_count} æ¬¡æ£€æŸ¥å¼€å§‹: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    
                    # æ‰§è¡Œå»¶è¿Ÿæ£€æŸ¥
                    delay_results = self.check_stream_computation_delay()
                    
                    if delay_results:
                        # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
                        self.log_stream_delay_results(delay_results)
                        
                        # æ‰“å°æ‘˜è¦
                        self.print_stream_delay_summary(delay_results)
                    else:
                        error_msg = f"ç¬¬ {check_count} æ¬¡æ£€æŸ¥å¤±è´¥: æ— æ³•è·å–å»¶è¿Ÿç»“æœ"
                        print(error_msg)
                        with open(self.delay_log_file, 'a') as f:
                            f.write(f"{error_msg}\n")
                    
                    # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
                    print(f"ç­‰å¾… {self.delay_check_interval} ç§’åè¿›è¡Œä¸‹æ¬¡æ£€æŸ¥...")
                    time.sleep(self.delay_check_interval)
                    
                except Exception as e:
                    error_msg = f"ç¬¬ {check_count} æ¬¡å»¶è¿Ÿç›‘æ§å‡ºé”™: {str(e)}"
                    print(error_msg)
                    with open(self.delay_log_file, 'a') as f:
                        f.write(f"{error_msg}\n")
                    time.sleep(self.delay_check_interval)
            
            final_msg = f"æµå»¶è¿Ÿç›‘æ§ç»“æŸï¼Œå…±æ‰§è¡Œäº† {check_count} æ¬¡æ£€æŸ¥"
            print(final_msg)
            try:
                with open(self.delay_log_file, 'a') as f:
                    f.write(f"\n{final_msg}\n")
                    f.write(f"ç»“æŸæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            except Exception as e:
                print(f"å†™å…¥å»¶è¿Ÿæ—¥å¿—ç»“æŸä¿¡æ¯å¤±è´¥: {str(e)}")
        
        # åˆ›å»ºå¹¶å¯åŠ¨ç›‘æ§çº¿ç¨‹
        monitor_thread = threading.Thread(target=delay_monitor, name="StreamDelayMonitor")
        monitor_thread.daemon = True
        monitor_thread.start()
        
        return monitor_thread
            
    def do_test_stream_with_realtime_data_ooo(self):
        self.prepare_env()
        self.prepare_source_from_data()
        self.insert_source_from_data()
        
        conn = taos.connect(
            host=self.host,
            user=self.user,
            password=self.passwd,
            config=self.conf
        )
        cursor = conn.cursor()

        try:
            # è¿è¡Œsource_fromçš„æ•°æ®ç”Ÿæˆ
            subprocess.Popen('taosBenchmark --f /tmp/stream_from.json', 
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, text=True)
            
            # åˆ›å»ºstream_toæ•°æ®åº“
            if not self.create_database('stream_to'):
                raise Exception("åˆ›å»ºstream_toæ•°æ®åº“å¤±è´¥")
            
            time.sleep(5)
            print("æ•°æ®åº“å·²åˆ›å»º,ç­‰å¾…æ•°æ®å†™å…¥...")
            
            # ç­‰å¾…æ•°æ®å‡†å¤‡å°±ç»ª
            if not self.wait_for_data_ready(cursor, self.table_count, self.histroy_rows):
                print("æ•°æ®å‡†å¤‡å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
                return
            
            
            # âœ… è®¡ç®—é¢„çƒ­æ—¶é—´å’Œæµåˆ›å»ºå»¶è¿Ÿ
            if hasattr(self, 'monitor_warm_up_time') and self.monitor_warm_up_time is not None:
                warm_up_time = self.monitor_warm_up_time
                stream_creation_delay = self.monitor_warm_up_time  # æµåˆ›å»ºå»¶è¿Ÿç­‰äºé¢„çƒ­æ—¶é—´
                print(f"ä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„é¢„çƒ­æ—¶é—´: {warm_up_time}ç§’")
                print(f"âœ¨ æµåˆ›å»ºå»¶è¿Ÿ: {stream_creation_delay}ç§’ (å°†åœ¨é¢„çƒ­ç»“æŸååˆ›å»ºæµ)")
            else:
                warm_up_time = 60 if self.runtime >= 2 else 0
                stream_creation_delay = warm_up_time  # æµåˆ›å»ºå»¶è¿Ÿç­‰äºé¢„çƒ­æ—¶é—´
                if warm_up_time > 0:
                    print(f"ä½¿ç”¨è‡ªåŠ¨è®¡ç®—çš„é¢„çƒ­æ—¶é—´: {warm_up_time}ç§’")
                    print(f"âœ¨ æµåˆ›å»ºå»¶è¿Ÿ: {stream_creation_delay}ç§’ (å°†åœ¨é¢„çƒ­ç»“æŸååˆ›å»ºæµ)")
                else:
                    print(f"è¿è¡Œæ—¶é—´è¾ƒçŸ­({self.runtime}åˆ†é’Ÿ)ï¼Œè·³è¿‡é¢„çƒ­ï¼Œç«‹å³åˆ›å»ºæµ")
                    
                    
            # è·å–æ–°è¿æ¥æ‰§è¡Œæµå¼æŸ¥è¯¢
            conn, cursor = self.get_connection()
            
            print("å¼€å§‹è¿æ¥æ•°æ®åº“")
            cursor.execute('use stream_from')
            
            # # æ‰§è¡Œæµå¼æŸ¥è¯¢
            # print(f"æ‰§è¡Œæµå¼æŸ¥è¯¢SQL:\n{self.stream_sql}")
            # cursor.execute(self.stream_sql)
            
            # è·å– SQL æ¨¡æ¿
            sql_templates = self.stream_sql 
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºæ‰¹é‡æ‰§è¡Œ
            if isinstance(sql_templates, dict):
                print("\n=== å¼€å§‹æ‰¹é‡åˆ›å»ºæµ ===")
                total_streams = len(sql_templates)
                success_count = 0
                failed_count = 0
                failed_errors = [] 
                
                for index, (sql_name, sql_template) in enumerate(sql_templates.items(), 1):
                    try:
                        print(f"\n[{index}/{total_streams}] åˆ›å»ºæµ: {sql_name}")
                        
                        # æå–å®é™…çš„æµåç§°ï¼ˆä»SQLä¸­è§£æï¼‰
                        import re
                        match = re.search(r'create\s+stream\s+([^\s]+)', sql_template, re.IGNORECASE)
                        actual_stream_name = match.group(1) if match else sql_name
                        
                        print(f"  SQLæµåç§°: {actual_stream_name}")
                        # æ˜¾ç¤ºå®Œæ•´çš„æµSQLè¯­å¥
                        print(f"  æµSQLè¯­å¥:")
                        # æ ¼å¼åŒ–SQLæ˜¾ç¤ºï¼Œå»æ‰å¤šä½™çš„ç©ºè¡Œå’Œç¼©è¿›
                        formatted_sql = '\n'.join([
                            '    ' + line.strip() 
                            for line in sql_template.strip().split('\n') 
                            if line.strip()
                        ])
                        print(formatted_sql)
                        
                        print(f"  æ‰§è¡Œåˆ›å»º...")
                        cursor.execute(sql_template)
                        success_count += 1
                        print(f"  âœ“ åˆ›å»ºæˆåŠŸ")
                        
                    except Exception as e:
                        failed_count += 1
                        
                        # æå–TDengineçš„å…·ä½“é”™è¯¯ä¿¡æ¯
                        error_message = str(e)
                        tdengine_error = extract_stream_creation_error(error_message)
                        
                        print(f"æ‰§è¡Œé”™è¯¯: {tdengine_error}")
                        failed_errors.append(f"{sql_name}: {tdengine_error}")
                
                # æ˜¾ç¤ºæ‰¹é‡åˆ›å»ºç»“æœæ‘˜è¦
                print(f"\n=== æ‰¹é‡åˆ›å»ºç»“æœæ‘˜è¦ ===")
                print(f"æ€»æµæ•°: {total_streams}")
                print(f"æˆåŠŸ: {success_count}")
                print(f"å¤±è´¥: {failed_count}")
                print(f"æˆåŠŸç‡: {(success_count/total_streams*100):.1f}%")
            
                if failed_count > 0:
                    # æ„å»ºè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                    error_summary = f"æµåˆ›å»ºå¤±è´¥ç»Ÿè®¡: {failed_count}/{total_streams} ä¸ªæµå¤±è´¥"
                    if len(failed_errors) > 0:
                        # åªä¿ç•™å‰5ä¸ªé”™è¯¯è¯¦æƒ…ï¼Œé¿å…ä¿¡æ¯è¿‡é•¿
                        error_details = "; ".join(failed_errors[:5])
                        if len(failed_errors) > 5:
                            error_details += f"; è¿˜æœ‰{len(failed_errors)-5}ä¸ªé”™è¯¯..."
                        error_summary += f" | é”™è¯¯è¯¦æƒ…: {error_details}"
                    print(f"âš ï¸  {failed_count} ä¸ªæµåˆ›å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
                    
                    if success_count == 0:
                        # å¦‚æœæ‰€æœ‰æµéƒ½å¤±è´¥äº†ï¼ŒæŠ›å‡ºå¼‚å¸¸
                        raise Exception(f"æ‰€æœ‰ {total_streams} ä¸ªæµåˆ›å»ºéƒ½å¤±è´¥äº†")
                    else:
                        # å¦‚æœéƒ¨åˆ†å¤±è´¥ï¼Œè®°å½•è­¦å‘Šä½†ç»§ç»­æ‰§è¡Œ
                        print(f"âš ï¸  æ³¨æ„: {failed_count}/{total_streams} ä¸ªæµåˆ›å»ºå¤±è´¥ï¼Œå°†ä½¿ç”¨ {success_count} ä¸ªæˆåŠŸçš„æµç»§ç»­æµ‹è¯•")
                else:
                    print("æ‰€æœ‰æµåˆ›å»ºæˆåŠŸï¼")
                
            else:
                # å•ä¸ªæµçš„åˆ›å»º
                print("\n=== å¼€å§‹åˆ›å»ºæµ ===")
                
                # æå–å®é™…çš„æµåç§°
                import re
                match = re.search(r'create\s+stream\s+([^\s]+)', sql_templates, re.IGNORECASE)
                actual_stream_name = match.group(1) if match else "æœªçŸ¥æµåç§°"
                
                print(f"æµåç§°: {actual_stream_name}")
                print("æµSQLè¯­å¥:")
                print("-" * 80)
                print(sql_templates)
                print("-" * 80)
                
                try:
                    start_time = time.time()
                    cursor.execute(sql_templates)
                    create_time = time.time() - start_time
                    
                    print(f"âœ“ æµ {actual_stream_name} åˆ›å»ºå®Œæˆ! è€—æ—¶: {create_time:.2f}ç§’")
                    
                except Exception as e:
                    # æå–TDengineçš„å…·ä½“é”™è¯¯ä¿¡æ¯
                    error_message = str(e)
                    tdengine_error = extract_stream_creation_error(error_message)
                    
                    print(f"realtime_dataæå–TDengineæ‰§è¡Œé”™è¯¯2: {tdengine_error}")
                    raise Exception(tdengine_error)
            
            print("æµå¼æŸ¥è¯¢å·²åˆ›å»º,å¼€å§‹ç›‘æ§ç³»ç»Ÿè´Ÿè½½")
            cursor.close()
            conn.close()
            
            # ç›‘æ§ç³»ç»Ÿè´Ÿè½½ - åŒæ—¶ç›‘æ§ä¸‰ä¸ªèŠ‚ç‚¹
            print(f"è°ƒè¯•: ä½¿ç”¨æ€§èƒ½æ–‡ä»¶è·¯å¾„: {self.perf_file}")
            
            # ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„å€¼
            if hasattr(self, 'monitor_warm_up_time') and self.monitor_warm_up_time is not None:
                # å¦‚æœç”¨æˆ·æ˜ç¡®è®¾ç½®äº†é¢„çƒ­æ—¶é—´ï¼Œä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„å€¼
                warm_up_time = self.monitor_warm_up_time
                print(f"ä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„é¢„çƒ­æ—¶é—´: {warm_up_time}ç§’")
            else:
                # å¦‚æœç”¨æˆ·æœªè®¾ç½®ï¼Œä½¿ç”¨è‡ªåŠ¨è®¡ç®—é€»è¾‘
                warm_up_time = 60 if self.runtime >= 2 else 0
                if warm_up_time > 0:
                    print(f"ä½¿ç”¨è‡ªåŠ¨è®¡ç®—çš„é¢„çƒ­æ—¶é—´: {warm_up_time}ç§’ (è¿è¡Œæ—¶é—´>=2åˆ†é’Ÿ)")
                else:
                    print(f"è¿è¡Œæ—¶é—´è¾ƒçŸ­({self.runtime}åˆ†é’Ÿ)ï¼Œè·³è¿‡é¢„çƒ­ç›´æ¥å¼€å§‹ç›‘æ§")
            
            if warm_up_time > 0:
                print(f"\n=== å¯åŠ¨æ€§èƒ½ç›‘æ§ (åŒ…å«{warm_up_time}ç§’é¢„çƒ­æœŸ) ===")
                print(f"ç›‘æ§é˜¶æ®µåˆ’åˆ†:")
                print(f"  ğŸ“Š é¢„çƒ­æœŸ (0-{warm_up_time}ç§’): çº¯æ•°æ®å†™å…¥ + ç³»ç»Ÿç¨³å®š")
                print(f"  ğŸ”„ æµè®¡ç®—æœŸ ({warm_up_time}ç§’-{self.runtime*60}ç§’): æ•°æ®å†™å…¥ + æµè®¡ç®—")
                print(f"æ€»è¿è¡Œæ—¶é—´: {self.runtime}åˆ†é’Ÿ")
            else:
                print(f"è¿è¡Œæ—¶é—´è¾ƒçŸ­({self.runtime}åˆ†é’Ÿ)ï¼Œè·³è¿‡é¢„çƒ­ç›´æ¥å¼€å§‹ç›‘æ§")
                
            loader = MonitorSystemLoad(
                name_pattern='taosd -c', 
                count=self.runtime * 60,
                perf_file=self.perf_file,  # åŸºç¡€æ–‡ä»¶å,ä¼šè‡ªåŠ¨æ·»åŠ dnodeç¼–å·
                interval=self.monitor_interval,
                deployment_mode=self.deployment_mode,
                warm_up_time=warm_up_time
            )
                        
            # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œç›‘æ§
            monitor_thread = threading.Thread(
                target=loader.get_proc_status,
                name="TaosdMonitor"
            )
            monitor_thread.daemon = True
            monitor_thread.start()
            print("å¼€å§‹ç›‘æ§taosdè¿›ç¨‹èµ„æºä½¿ç”¨æƒ…å†µ...")
            
            # å¯åŠ¨æµå»¶è¿Ÿç›‘æ§
            delay_monitor_thread = self.start_stream_delay_monitor()    
          
            try:            
                # å¾ªç¯æ‰§è¡Œå†™å…¥å’Œè®¡ç®—
                cycle_count = 0
                start_time = time.time()
                
                while True:
                    cycle_count += 1
                    print(f"\n=== å¼€å§‹ç¬¬ {cycle_count} è½®å†™å…¥å’Œè®¡ç®— ===")
                    
                    if cycle_count > 1:
                        # ä»ç¬¬äºŒè½®å¼€å§‹ï¼Œå…ˆå†™å…¥æ–°æ•°æ®
                        print(f"\nå†™å…¥æ–°ä¸€æ‰¹æµ‹è¯•æ•°æ® (æ¯è½® {self.real_time_batch_rows} æ¡è®°å½•)...")
                        
                        # åº”ç”¨å†™å…¥é—´éš”æ§åˆ¶
                        if self.real_time_batch_sleep > 0:
                            print(f"ç­‰å¾… {self.real_time_batch_sleep} ç§’åå¼€å§‹å†™å…¥æ•°æ®...")
                            time.sleep(self.real_time_batch_sleep)
                        
                        write_start_time = time.time()
                        
                        if not self.update_insert_config():
                            raise Exception("æ›´æ–°å†™å…¥é…ç½®å¤±è´¥")
                        
                        cmd = "taosBenchmark -f /tmp/stream_from_insertdata.json"
                        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, text=True)
                        
                        write_end_time = time.time()
                        write_duration = write_end_time - write_start_time
                        
                        if result.returncode != 0:
                            print(f"å†™å…¥æ•°æ®å¤±è´¥: {result.stderr}")
                            raise Exception("å†™å…¥æ–°æ•°æ®å¤±è´¥")
                        else:
                            total_records = self.table_count * self.real_time_batch_rows
                            write_speed = total_records / write_duration if write_duration > 0 else 0
                            print(f"æ•°æ®å†™å…¥å®Œæˆ: {total_records} æ¡è®°å½•, è€—æ—¶ {write_duration:.2f}ç§’, é€Ÿåº¦ {write_speed:.0f} æ¡/ç§’")
                            
                            # å¦‚æœè®¾ç½®äº†å†™å…¥é—´éš”ï¼Œæ˜¾ç¤ºå†™å…¥æ§åˆ¶ä¿¡æ¯
                            if self.real_time_batch_sleep > 0:
                                print(f"å†™å…¥é—´éš”æ§åˆ¶: {self.real_time_batch_sleep}ç§’ (ç”¨äºæ§åˆ¶æµè®¡ç®—å‹åŠ›)")
                                           
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è¿è¡Œæ—¶é—´é™åˆ¶
                    if time.time() - start_time >= self.runtime * 60:
                        print(f"\n\nå·²è¾¾åˆ°è¿è¡Œæ—¶é—´é™åˆ¶ ({self.runtime} åˆ†é’Ÿ)ï¼Œåœæ­¢æ‰§è¡Œ")
                        break
                        
                    print(f"\n=== ç¬¬ {cycle_count} è½®å¤„ç†å®Œæˆ ===")
             
            except Exception as e:
                print(f"æŸ¥è¯¢å†™å…¥æ“ä½œå‡ºé”™: {str(e)}")
            finally:
                cursor.close()
                conn.close()
                print("æŸ¥è¯¢å†™å…¥æ“ä½œå®Œæˆ")
        
                # ä¸»åŠ¨åœæ­¢ç›‘æ§
                print("ä¸»åŠ¨åœæ­¢ç³»ç»Ÿç›‘æ§...")
                loader.stop()
                
            # ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
            print("ç­‰å¾…ç›‘æ§æ•°æ®æ”¶é›†å®Œæˆ...")
            monitor_thread.join(timeout=15)  # æœ€å¤šç­‰å¾…15ç§’
                
            if monitor_thread.is_alive():
                print("ç›‘æ§çº¿ç¨‹æœªåœ¨é¢„æœŸæ—¶é—´å†…ç»“æŸï¼Œå¼ºåˆ¶ç»§ç»­...")
            else:
                print("ç›‘æ§çº¿ç¨‹å·²æ­£å¸¸ç»“æŸ")
        
            if delay_monitor_thread:
                print("ç­‰å¾…å»¶è¿Ÿç›‘æ§å®Œæˆ...")
                delay_monitor_thread.join(timeout=10)  # æœ€å¤šç­‰å¾…10ç§’
                if delay_monitor_thread.is_alive():
                    print("å»¶è¿Ÿç›‘æ§çº¿ç¨‹æœªåœ¨é¢„æœŸæ—¶é—´å†…ç»“æŸï¼Œå¼ºåˆ¶ç»§ç»­...")
                else:
                    print("å»¶è¿Ÿç›‘æ§çº¿ç¨‹å·²æ­£å¸¸ç»“æŸ")
                
            # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
            if self.check_stream_delay:
                print(f"\næµå»¶è¿Ÿç›‘æ§æŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.delay_log_file}")
                self.print_final_delay_summary()
                
                            
            try:
                loader.get_proc_status()
            except KeyboardInterrupt:
                print("\nç›‘æ§è¢«ä¸­æ–­")
            finally:
                # æ£€æŸ¥taosdè¿›ç¨‹
                result = subprocess.run('ps -ef | grep taosd | grep -v grep', 
                                    shell=True, capture_output=True, text=True)
                if result.stdout:
                    print("\ntaosdè¿›ç¨‹ä»åœ¨è¿è¡Œ")
                    print("å¦‚éœ€åœæ­¢taosdè¿›ç¨‹ï¼Œè¯·æ‰‹åŠ¨æ‰§è¡Œ: pkill taosd")
                    
                    
        
            print("\næµè®¡ç®—æµ‹è¯•å®Œæˆ!")
            
            # å¦‚æœæœ‰éƒ¨åˆ†æµåˆ›å»ºå¤±è´¥ï¼Œåœ¨æµ‹è¯•å®Œæˆæ—¶æŠ¥å‘Š
            if hasattr(self, 'partial_stream_creation_errors'):
                print(f"\nâš ï¸  æµ‹è¯•å®Œæˆï¼Œä½†å­˜åœ¨æµåˆ›å»ºé—®é¢˜:")
                print(f"    {self.partial_stream_creation_errors}")
                # ä½œä¸ºè­¦å‘Šè€Œä¸æ˜¯é”™è¯¯ï¼Œä¸ä¸­æ–­æµ‹è¯•ä½†è®°å½•é—®é¢˜
                # å¯ä»¥é€‰æ‹©æ˜¯å¦æŠ›å‡ºå¼‚å¸¸
                # raise Exception(self.partial_stream_creation_errors)
                
        except Exception as e:
            print(f"æµè®¡ç®—æµ‹è¯•å¤±è´¥: {str(e)}")      
            raise      


    def do_test_stream_with_realtime_data(self):
        self.prepare_env()
        self.prepare_source_from_data()
        self.insert_source_from_data()
        
        conn = taos.connect(
            host=self.host,
            user=self.user,
            password=self.passwd,
            config=self.conf
        )
        cursor = conn.cursor()

        try:
            # âœ… å¯åŠ¨åˆå§‹æ•°æ®ç”Ÿæˆè¿›ç¨‹
            print("å¯åŠ¨åˆå§‹æ•°æ®ç”Ÿæˆ...")
            data_process = subprocess.Popen('taosBenchmark --f /tmp/stream_from.json', 
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, text=True)
            
            # åˆ›å»ºstream_toæ•°æ®åº“
            if not self.create_database('stream_to'):
                raise Exception("åˆ›å»ºstream_toæ•°æ®åº“å¤±è´¥")
            
            time.sleep(5)
            print("æ•°æ®åº“å·²åˆ›å»º,ç­‰å¾…æ•°æ®å†™å…¥...")
            
            # ç­‰å¾…æ•°æ®å‡†å¤‡å°±ç»ª
            if not self.wait_for_data_ready(cursor, self.table_count, self.histroy_rows):
                print("æ•°æ®å‡†å¤‡å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
                return
            
            # âœ… è®¡ç®—é¢„çƒ­æ—¶é—´å’Œæµåˆ›å»ºå»¶è¿Ÿ
            if hasattr(self, 'monitor_warm_up_time') and self.monitor_warm_up_time is not None:
                warm_up_time = self.monitor_warm_up_time
                stream_creation_delay = self.monitor_warm_up_time
                print(f"ä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„é¢„çƒ­æ—¶é—´: {warm_up_time}ç§’")
                print(f"âœ¨ æµåˆ›å»ºå»¶è¿Ÿ: {stream_creation_delay}ç§’ (å°†åœ¨é¢„çƒ­ç»“æŸååˆ›å»ºæµ)")
            else:
                warm_up_time = 60 if self.runtime >= 2 else 0
                stream_creation_delay = warm_up_time
                if warm_up_time > 0:
                    print(f"ä½¿ç”¨è‡ªåŠ¨è®¡ç®—çš„é¢„çƒ­æ—¶é—´: {warm_up_time}ç§’")
                    print(f"âœ¨ æµåˆ›å»ºå»¶è¿Ÿ: {stream_creation_delay}ç§’ (å°†åœ¨é¢„çƒ­ç»“æŸååˆ›å»ºæµ)")
                else:
                    print(f"è¿è¡Œæ—¶é—´è¾ƒçŸ­({self.runtime}åˆ†é’Ÿ)ï¼Œè·³è¿‡é¢„çƒ­ï¼Œç«‹å³åˆ›å»ºæµ")
            
            # âœ… å¯åŠ¨ç³»ç»Ÿç›‘æ§ (åŒ…å«é¢„çƒ­æœŸ)
            print(f"è°ƒè¯•: ä½¿ç”¨æ€§èƒ½æ–‡ä»¶è·¯å¾„: {self.perf_file}")
            
            if warm_up_time > 0:
                print(f"\n=== å¯åŠ¨æ€§èƒ½ç›‘æ§ (åŒ…å«{warm_up_time}ç§’é¢„çƒ­æœŸ) ===")
                print(f"ç›‘æ§é˜¶æ®µåˆ’åˆ†:")
                print(f"  ğŸ“Š é¢„çƒ­æœŸ (0-{warm_up_time}ç§’): æŒç»­æ•°æ®å†™å…¥ + ç³»ç»Ÿç¨³å®š + ä¸åˆ›å»ºæµ")
                print(f"  ğŸ”„ æµè®¡ç®—æœŸ ({warm_up_time}ç§’-{self.runtime*60}ç§’): åˆ›å»ºæµ + æ•°æ®å†™å…¥ + æµè®¡ç®—")
                print(f"æ€»è¿è¡Œæ—¶é—´: {self.runtime}åˆ†é’Ÿ")
                print(f"âš ï¸  é¢„çƒ­æœŸç›®çš„: è§‚å¯Ÿçº¯æ•°æ®å†™å…¥æ€§èƒ½åŸºçº¿ï¼Œä¸æµè®¡ç®—æœŸå¯¹æ¯”")
            else:
                print(f"è¿è¡Œæ—¶é—´è¾ƒçŸ­({self.runtime}åˆ†é’Ÿ)ï¼Œè·³è¿‡é¢„çƒ­ç›´æ¥å¼€å§‹ç›‘æ§")
                
            loader = MonitorSystemLoad(
                name_pattern='taosd -c', 
                count=self.runtime * 60,
                perf_file=self.perf_file,
                interval=self.monitor_interval,
                deployment_mode=self.deployment_mode,
                warm_up_time=warm_up_time
            )
                        
            # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œç›‘æ§
            monitor_thread = threading.Thread(
                target=loader.get_proc_status,
                name="TaosdMonitor"
            )
            monitor_thread.daemon = True
            monitor_thread.start()
            print("âœ… ç³»ç»Ÿèµ„æºç›‘æ§å·²å¯åŠ¨...")
            
            # âœ… ç”¨äºæ ‡è®°æµåˆ›å»ºæ˜¯å¦æˆåŠŸçš„å˜é‡
            stream_creation_success = False
            stream_creation_error = None
            
            # âœ… å»¶è¿Ÿåˆ›å»ºæµçš„çº¿ç¨‹
            stream_creation_thread = None
            if stream_creation_delay > 0:
                def delayed_stream_creation():
                    """å»¶è¿Ÿåˆ›å»ºæµçš„å‡½æ•°"""
                    nonlocal stream_creation_success, stream_creation_error
                    
                    print(f"\nâ° ç­‰å¾… {stream_creation_delay} ç§’ååˆ›å»ºæµ...")
                    print(f"ğŸ“Š é¢„çƒ­æœŸ: æŒç»­æ•°æ®å†™å…¥é˜¶æ®µ ({stream_creation_delay}ç§’)")
                    print(f"   åœ¨æ­¤æœŸé—´æŒç»­å†™å…¥æ•°æ®è§‚å¯Ÿçº¯å†™å…¥æ€§èƒ½åŸºçº¿")
                    time.sleep(stream_creation_delay)
                    
                    print(f"\nğŸ”„ é¢„çƒ­æœŸç»“æŸï¼Œå¼€å§‹åˆ›å»ºæµ...")
                    print(f"=" * 80)
                    
                    try:
                        # è·å–æ–°çš„æ•°æ®åº“è¿æ¥ç”¨äºåˆ›å»ºæµ
                        stream_conn, stream_cursor = self.get_connection()
                        stream_cursor.execute('use stream_from')
                        
                        # è·å– SQL æ¨¡æ¿å¹¶åˆ›å»ºæµ
                        sql_templates = self.stream_sql 
                        
                        # åˆ¤æ–­æ˜¯å¦ä¸ºæ‰¹é‡æ‰§è¡Œ
                        if isinstance(sql_templates, dict):
                            print("\n=== å¼€å§‹æ‰¹é‡åˆ›å»ºæµ ===")
                            total_streams = len(sql_templates)
                            success_count = 0
                            failed_count = 0
                            failed_errors = [] 
                            
                            for index, (sql_name, sql_template) in enumerate(sql_templates.items(), 1):
                                try:
                                    print(f"\n[{index}/{total_streams}] åˆ›å»ºæµ: {sql_name}")
                                    
                                    # æå–å®é™…çš„æµåç§°ï¼ˆä»SQLä¸­è§£æï¼‰
                                    import re
                                    match = re.search(r'create\s+stream\s+([^\s]+)', sql_template, re.IGNORECASE)
                                    actual_stream_name = match.group(1) if match else sql_name
                                    
                                    print(f"  SQLæµåç§°: {actual_stream_name}")
                                    # æ˜¾ç¤ºå®Œæ•´çš„æµSQLè¯­å¥
                                    print(f"  æµSQLè¯­å¥:")
                                    # æ ¼å¼åŒ–SQLæ˜¾ç¤ºï¼Œå»æ‰å¤šä½™çš„ç©ºè¡Œå’Œç¼©è¿›
                                    formatted_sql = '\n'.join([
                                        '    ' + line.strip() 
                                        for line in sql_template.strip().split('\n') 
                                        if line.strip()
                                    ])
                                    print(formatted_sql)
                                    
                                    print(f"  æ‰§è¡Œåˆ›å»º...")
                                    stream_cursor.execute(sql_template)
                                    success_count += 1
                                    print(f"  âœ“ åˆ›å»ºæˆåŠŸ")
                                    
                                except Exception as e:
                                    failed_count += 1
                                    
                                    # æå–TDengineçš„å…·ä½“é”™è¯¯ä¿¡æ¯
                                    error_message = str(e)
                                    tdengine_error = extract_stream_creation_error(error_message)
                                    
                                    print(f"æ‰§è¡Œé”™è¯¯: {tdengine_error}")
                                    failed_errors.append(f"{sql_name}: {tdengine_error}")
                            
                            # æ˜¾ç¤ºæ‰¹é‡åˆ›å»ºç»“æœæ‘˜è¦
                            print(f"\n=== æ‰¹é‡åˆ›å»ºç»“æœæ‘˜è¦ ===")
                            print(f"æ€»æµæ•°: {total_streams}")
                            print(f"æˆåŠŸ: {success_count}")
                            print(f"å¤±è´¥: {failed_count}")
                            print(f"æˆåŠŸç‡: {(success_count/total_streams*100):.1f}%")
                        
                            if failed_count > 0:
                                error_summary = f"æµåˆ›å»ºå¤±è´¥ç»Ÿè®¡: {failed_count}/{total_streams} ä¸ªæµå¤±è´¥"
                                if len(failed_errors) > 0:
                                    error_details = "; ".join(failed_errors[:5])
                                    if len(failed_errors) > 5:
                                        error_details += f"; è¿˜æœ‰{len(failed_errors)-5}ä¸ªé”™è¯¯..."
                                    error_summary += f" | é”™è¯¯è¯¦æƒ…: {error_details}"
                                print(f"âš ï¸  {failed_count} ä¸ªæµåˆ›å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
                                
                                if success_count == 0:
                                    # âœ… å¦‚æœæ‰€æœ‰æµéƒ½å¤±è´¥äº†ï¼Œè®¾ç½®é”™è¯¯æ ‡è®°å¹¶è¿”å›
                                    stream_creation_error = f"æ‰€æœ‰ {total_streams} ä¸ªæµåˆ›å»ºéƒ½å¤±è´¥äº†"
                                    stream_creation_success = False
                                    print(f"âŒ æ‰€æœ‰æµåˆ›å»ºå¤±è´¥ï¼Œåœæ­¢æµ‹è¯•")
                                    return
                                else:
                                    # éƒ¨åˆ†å¤±è´¥ï¼Œä»ç„¶ç®—ä½œæˆåŠŸï¼Œç»§ç»­æµ‹è¯•
                                    stream_creation_success = True
                                    print(f"âš ï¸  æ³¨æ„: {failed_count}/{total_streams} ä¸ªæµåˆ›å»ºå¤±è´¥ï¼Œå°†ä½¿ç”¨ {success_count} ä¸ªæˆåŠŸçš„æµç»§ç»­æµ‹è¯•")
                            else:
                                stream_creation_success = True
                                print("âœ… æ‰€æœ‰æµåˆ›å»ºæˆåŠŸï¼")
                            
                        else:
                            # å•ä¸ªæµçš„åˆ›å»º
                            print("\n=== å¼€å§‹åˆ›å»ºæµ ===")
                            
                            # æå–å®é™…çš„æµåç§°
                            import re
                            match = re.search(r'create\s+stream\s+([^\s]+)', sql_templates, re.IGNORECASE)
                            actual_stream_name = match.group(1) if match else "æœªçŸ¥æµåç§°"
                            
                            print(f"æµåç§°: {actual_stream_name}")
                            print("æµSQLè¯­å¥:")
                            print("-" * 80)
                            print(sql_templates)
                            print("-" * 80)
                            
                            try:
                                start_time = time.time()
                                stream_cursor.execute(sql_templates)
                                create_time = time.time() - start_time
                                
                                print(f"âœ… æµ {actual_stream_name} åˆ›å»ºå®Œæˆ! è€—æ—¶: {create_time:.2f}ç§’")
                                stream_creation_success = True
                                
                            except Exception as e:
                                # âœ… å•ä¸ªæµåˆ›å»ºå¤±è´¥ï¼Œè®¾ç½®é”™è¯¯æ ‡è®°
                                error_message = str(e)
                                tdengine_error = extract_stream_creation_error(error_message)
                                
                                print(f"âŒ æµåˆ›å»ºå¤±è´¥: {tdengine_error}")
                                stream_creation_error = tdengine_error
                                stream_creation_success = False
                                return
                        
                        if stream_creation_success:
                            print(f"\nğŸ”„ æµè®¡ç®—æœŸå¼€å§‹ - æ•°æ®å†™å…¥ + æµè®¡ç®—")
                            print(f"=" * 80)
                        
                        # å…³é—­æµåˆ›å»ºè¿æ¥
                        stream_cursor.close()
                        stream_conn.close()
                        
                    except Exception as e:
                        print(f"âŒ å»¶è¿Ÿåˆ›å»ºæµæ—¶å‡ºé”™: {str(e)}")
                        stream_creation_error = str(e)
                        stream_creation_success = False
                
                # åˆ›å»ºå¹¶å¯åŠ¨å»¶è¿Ÿæµåˆ›å»ºçº¿ç¨‹
                stream_creation_thread = threading.Thread(
                    target=delayed_stream_creation,
                    name="DelayedStreamCreation"
                )
                stream_creation_thread.daemon = True
                stream_creation_thread.start()
                print(f"âœ… å»¶è¿Ÿæµåˆ›å»ºçº¿ç¨‹å·²å¯åŠ¨ (å°†åœ¨{stream_creation_delay}ç§’åæ‰§è¡Œ)")
                
            else:
                # ç«‹å³åˆ›å»ºæµ (åŸæœ‰é€»è¾‘ä¿æŒä¸å˜)
                print("\n=== ç«‹å³åˆ›å»ºæµ ===")
    
                try:
                    # è·å–æ–°çš„æ•°æ®åº“è¿æ¥ç”¨äºåˆ›å»ºæµ
                    stream_conn, stream_cursor = self.get_connection()
                    stream_cursor.execute('use stream_from')
                    
                    # è·å– SQL æ¨¡æ¿å¹¶åˆ›å»ºæµ
                    sql_templates = self.stream_sql 
                    
                    # åˆ¤æ–­æ˜¯å¦ä¸ºæ‰¹é‡æ‰§è¡Œ
                    if isinstance(sql_templates, dict):
                        print("\n=== å¼€å§‹æ‰¹é‡åˆ›å»ºæµ ===")
                        total_streams = len(sql_templates)
                        success_count = 0
                        failed_count = 0
                        failed_errors = [] 
                        
                        for index, (sql_name, sql_template) in enumerate(sql_templates.items(), 1):
                            try:
                                print(f"\n[{index}/{total_streams}] åˆ›å»ºæµ: {sql_name}")
                                
                                # æå–å®é™…çš„æµåç§°ï¼ˆä»SQLä¸­è§£æï¼‰
                                import re
                                match = re.search(r'create\s+stream\s+([^\s]+)', sql_template, re.IGNORECASE)
                                actual_stream_name = match.group(1) if match else sql_name
                                
                                print(f"  SQLæµåç§°: {actual_stream_name}")
                                # æ˜¾ç¤ºå®Œæ•´çš„æµSQLè¯­å¥
                                print(f"  æµSQLè¯­å¥:")
                                # æ ¼å¼åŒ–SQLæ˜¾ç¤ºï¼Œå»æ‰å¤šä½™çš„ç©ºè¡Œå’Œç¼©è¿›
                                formatted_sql = '\n'.join([
                                    '    ' + line.strip() 
                                    for line in sql_template.strip().split('\n') 
                                    if line.strip()
                                ])
                                print(formatted_sql)
                                
                                print(f"  æ‰§è¡Œåˆ›å»º...")
                                stream_cursor.execute(sql_template)
                                success_count += 1
                                print(f"  âœ“ åˆ›å»ºæˆåŠŸ")
                                
                            except Exception as e:
                                failed_count += 1
                                
                                # æå–TDengineçš„å…·ä½“é”™è¯¯ä¿¡æ¯
                                error_message = str(e)
                                tdengine_error = extract_stream_creation_error(error_message)
                                
                                print(f"âŒ æ‰§è¡Œé”™è¯¯: {tdengine_error}")
                                failed_errors.append(f"{sql_name}: {tdengine_error}")
                        
                        # æ˜¾ç¤ºæ‰¹é‡åˆ›å»ºç»“æœæ‘˜è¦
                        print(f"\n=== æ‰¹é‡åˆ›å»ºç»“æœæ‘˜è¦ ===")
                        print(f"æ€»æµæ•°: {total_streams}")
                        print(f"æˆåŠŸ: {success_count}")
                        print(f"å¤±è´¥: {failed_count}")
                        print(f"æˆåŠŸç‡: {(success_count/total_streams*100):.1f}%")
                    
                        if failed_count > 0:
                            error_summary = f"æµåˆ›å»ºå¤±è´¥ç»Ÿè®¡: {failed_count}/{total_streams} ä¸ªæµå¤±è´¥"
                            if len(failed_errors) > 0:
                                error_details = "; ".join(failed_errors[:5])
                                if len(failed_errors) > 5:
                                    error_details += f"; è¿˜æœ‰{len(failed_errors)-5}ä¸ªé”™è¯¯..."
                                error_summary += f" | é”™è¯¯è¯¦æƒ…: {error_details}"
                            print(f"âš ï¸  {failed_count} ä¸ªæµåˆ›å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
                            
                            if success_count == 0:
                                # å¦‚æœæ‰€æœ‰æµéƒ½å¤±è´¥äº†ï¼Œè®¾ç½®é”™è¯¯æ ‡è®°å¹¶æŠ›å‡ºå¼‚å¸¸
                                stream_creation_error = f"æ‰€æœ‰ {total_streams} ä¸ªæµåˆ›å»ºéƒ½å¤±è´¥äº†"
                                stream_creation_success = False
                                print(f"âŒ æ‰€æœ‰æµåˆ›å»ºå¤±è´¥ï¼Œåœæ­¢æµ‹è¯•")
                                raise Exception(stream_creation_error)
                            else:
                                # éƒ¨åˆ†å¤±è´¥ï¼Œä»ç„¶ç®—ä½œæˆåŠŸï¼Œç»§ç»­æµ‹è¯•
                                stream_creation_success = True
                                print(f"âš ï¸  æ³¨æ„: {failed_count}/{total_streams} ä¸ªæµåˆ›å»ºå¤±è´¥ï¼Œå°†ä½¿ç”¨ {success_count} ä¸ªæˆåŠŸçš„æµç»§ç»­æµ‹è¯•")
                        else:
                            stream_creation_success = True
                            print("âœ… æ‰€æœ‰æµåˆ›å»ºæˆåŠŸï¼")
                        
                    else:
                        # å•ä¸ªæµçš„åˆ›å»º
                        print("\n=== å¼€å§‹åˆ›å»ºå•ä¸ªæµ ===")
                        
                        # æå–å®é™…çš„æµåç§°
                        import re
                        match = re.search(r'create\s+stream\s+([^\s]+)', sql_templates, re.IGNORECASE)
                        actual_stream_name = match.group(1) if match else "æœªçŸ¥æµåç§°"
                        
                        print(f"æµåç§°: {actual_stream_name}")
                        print("æµSQLè¯­å¥:")
                        print("-" * 80)
                        print(sql_templates)
                        print("-" * 80)
                        
                        try:
                            start_time = time.time()
                            stream_cursor.execute(sql_templates)
                            create_time = time.time() - start_time
                            
                            print(f"âœ… æµ {actual_stream_name} åˆ›å»ºå®Œæˆ! è€—æ—¶: {create_time:.2f}ç§’")
                            stream_creation_success = True
                            
                        except Exception as e:
                            # å•ä¸ªæµåˆ›å»ºå¤±è´¥ï¼Œè®¾ç½®é”™è¯¯æ ‡è®°å¹¶æŠ›å‡ºå¼‚å¸¸
                            error_message = str(e)
                            tdengine_error = extract_stream_creation_error(error_message)
                            
                            print(f"âŒ æµåˆ›å»ºå¤±è´¥: {tdengine_error}")
                            stream_creation_error = tdengine_error
                            stream_creation_success = False
                            raise Exception(tdengine_error)
                    
                    if stream_creation_success:
                        print(f"\nğŸ”„ æµè®¡ç®—å¼€å§‹ - æ•°æ®å†™å…¥ + æµè®¡ç®—")
                        print(f"=" * 80)
                    
                    # å…³é—­æµåˆ›å»ºè¿æ¥
                    stream_cursor.close()
                    stream_conn.close()
                    
                except Exception as e:
                    print(f"âŒ ç«‹å³åˆ›å»ºæµæ—¶å‡ºé”™: {str(e)}")
                    stream_creation_error = str(e)
                    stream_creation_success = False
                    # å¯¹äºç«‹å³åˆ›å»ºæµçš„æƒ…å†µï¼Œå¦‚æœå¤±è´¥å°±ç›´æ¥æŠ›å‡ºå¼‚å¸¸
                    raise Exception(f"æµåˆ›å»ºå¤±è´¥: {str(e)}")
            
            # âœ… ç«‹å³å¯åŠ¨æ•°æ®å†™å…¥çº¿ç¨‹ï¼ˆé¢„çƒ­æœŸæ•°æ®å†™å…¥ï¼‰
            write_thread = None
            write_stop_flag = threading.Event()
            
            def continuous_data_writing():
                """æŒç»­æ•°æ®å†™å…¥çº¿ç¨‹å‡½æ•°"""
                write_cycle = 0
                while not write_stop_flag.is_set():
                    write_cycle += 1
                    current_time = time.time()
                    
                    # åˆ¤æ–­å½“å‰é˜¶æ®µ
                    if stream_creation_delay > 0:
                        if current_time - test_start_time <= stream_creation_delay:
                            stage = "é¢„çƒ­æœŸ"
                            stage_desc = "çº¯æ•°æ®å†™å…¥è´Ÿè½½ (è§‚å¯ŸåŸºçº¿æ€§èƒ½)"
                        else:
                            stage = "æµè®¡ç®—æœŸ"
                            stage_desc = "æ•°æ®å†™å…¥ + æµè®¡ç®—è´Ÿè½½"
                    else:
                        stage = "æ­£å¸¸æœŸ"
                        stage_desc = "æ•°æ®å†™å…¥ + æµè®¡ç®—"
                    
                    print(f"\n=== ç¬¬ {write_cycle} è½®æ•°æ®å†™å…¥ - {stage} ===")
                    print(f"ğŸ“ {stage_desc}")
                    
                    # åº”ç”¨å†™å…¥é—´éš”æ§åˆ¶
                    if self.real_time_batch_sleep > 0:
                        print(f"ç­‰å¾… {self.real_time_batch_sleep} ç§’åå¼€å§‹å†™å…¥æ•°æ®...")
                        if write_stop_flag.wait(self.real_time_batch_sleep):
                            break  # å¦‚æœæ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡º
                    
                    write_start_time = time.time()
                    
                    try:
                        # æ›´æ–°å†™å…¥é…ç½®
                        if not self.update_insert_config():
                            print("âŒ æ›´æ–°å†™å…¥é…ç½®å¤±è´¥")
                            break
                        
                        # æ‰§è¡Œæ•°æ®å†™å…¥
                        cmd = "taosBenchmark -f /tmp/stream_from_insertdata.json"
                        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, text=True)
                        
                        write_end_time = time.time()
                        write_duration = write_end_time - write_start_time
                        
                        if result.returncode != 0:
                            print(f"âŒ å†™å…¥æ•°æ®å¤±è´¥: {result.stderr}")
                            break
                        else:
                            total_records = self.table_count * self.real_time_batch_rows
                            write_speed = total_records / write_duration if write_duration > 0 else 0
                            print(f"âœ… æ•°æ®å†™å…¥å®Œæˆ: {total_records} æ¡è®°å½•, è€—æ—¶ {write_duration:.2f}ç§’, é€Ÿåº¦ {write_speed:.0f} æ¡/ç§’")
                            
                            # æ˜¾ç¤ºå†™å…¥æ§åˆ¶ä¿¡æ¯
                            if self.real_time_batch_sleep > 0:
                                print(f"â³ å†™å…¥é—´éš”æ§åˆ¶: {self.real_time_batch_sleep}ç§’")
                    
                    except Exception as e:
                        print(f"âŒ æ•°æ®å†™å…¥å‡ºé”™: {str(e)}")
                        break
                    
                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
                    if write_stop_flag.is_set():
                        break
            
            # å¯åŠ¨æ•°æ®å†™å…¥çº¿ç¨‹
            test_start_time = time.time()
            write_thread = threading.Thread(target=continuous_data_writing, name="ContinuousDataWriter")
            write_thread.daemon = True
            write_thread.start()
            print("âœ… æŒç»­æ•°æ®å†™å…¥çº¿ç¨‹å·²å¯åŠ¨...")
            
            # âœ… ç­‰å¾…æµåˆ›å»ºå®Œæˆï¼ˆå¦‚æœæ˜¯å»¶è¿Ÿåˆ›å»ºï¼‰
            if stream_creation_thread:
                print("ç­‰å¾…æµåˆ›å»ºå®Œæˆ...")
                # ç­‰å¾…è¶³å¤Ÿçš„æ—¶é—´è®©æµåˆ›å»ºå®Œæˆ
                max_wait_time = stream_creation_delay + 60  # é¢„çƒ­æ—¶é—´ + 60ç§’ç¼“å†²æ—¶é—´
                stream_creation_thread.join(timeout=max_wait_time)
                
                # æ£€æŸ¥æµåˆ›å»ºç»“æœ
                if not stream_creation_success:
                    if stream_creation_error:
                        print(f"âŒ æµåˆ›å»ºå¤±è´¥ï¼Œåœæ­¢æµ‹è¯•: {stream_creation_error}")
                        raise Exception(f"æµåˆ›å»ºå¤±è´¥: {stream_creation_error}")
                    else:
                        print("âŒ æµåˆ›å»ºçŠ¶æ€æœªçŸ¥ï¼Œåœæ­¢æµ‹è¯•")
                        raise Exception("æµåˆ›å»ºçŠ¶æ€æœªçŸ¥")
                
                print("âœ… æµåˆ›å»ºå®Œæˆï¼Œç»§ç»­æ•°æ®å†™å…¥å’Œæµè®¡ç®—æµ‹è¯•")
            
            # âœ… åªæœ‰åœ¨æµåˆ›å»ºæˆåŠŸåæ‰å¯åŠ¨å»¶è¿Ÿç›‘æ§
            delay_monitor_thread = None
            if stream_creation_success and self.check_stream_delay:
                print("æµåˆ›å»ºæˆåŠŸï¼Œå¯åŠ¨å»¶è¿Ÿç›‘æ§...")
                delay_monitor_thread = self.start_stream_delay_monitor()
            elif not stream_creation_success:
                print("âš ï¸  æµåˆ›å»ºå¤±è´¥ï¼Œè·³è¿‡å»¶è¿Ÿç›‘æ§")
            elif not self.check_stream_delay:
                print("âš ï¸  å»¶è¿Ÿç›‘æ§æœªå¯ç”¨")
            else:
                print("âš ï¸  æœªçŸ¥çŠ¶æ€ï¼Œè·³è¿‡å»¶è¿Ÿç›‘æ§")
        
            try:            
                # âœ… ç­‰å¾…æµ‹è¯•å®Œæˆ
                total_test_time = self.runtime * 60
                print(f"\n=== å¼€å§‹ç­‰å¾…æµ‹è¯•å®Œæˆ (æ€»æ—¶é—´: {self.runtime}åˆ†é’Ÿ) ===")
                
                time.sleep(total_test_time)
                
                print(f"\nâœ… å·²è¾¾åˆ°è¿è¡Œæ—¶é—´é™åˆ¶ ({self.runtime} åˆ†é’Ÿ)ï¼Œåœæ­¢æµ‹è¯•")
            
            except Exception as e:
                print(f"æµ‹è¯•æ‰§è¡Œå‡ºé”™: {str(e)}")
            finally:
                # åœæ­¢æ•°æ®å†™å…¥çº¿ç¨‹
                if write_thread and write_thread.is_alive():
                    print("åœæ­¢æ•°æ®å†™å…¥çº¿ç¨‹...")
                    write_stop_flag.set()
                    write_thread.join(timeout=10)
                    if write_thread.is_alive():
                        print("æ•°æ®å†™å…¥çº¿ç¨‹æœªåœ¨é¢„æœŸæ—¶é—´å†…ç»“æŸ")
                    else:
                        print("æ•°æ®å†™å…¥çº¿ç¨‹å·²åœæ­¢")
                
                print("æµ‹è¯•ä¸»ä½“æ“ä½œå®Œæˆ")
        
                # ä¸»åŠ¨åœæ­¢ç›‘æ§
                print("ä¸»åŠ¨åœæ­¢ç³»ç»Ÿç›‘æ§...")
                loader.stop()
                
            # ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
            print("ç­‰å¾…ç›‘æ§æ•°æ®æ”¶é›†å®Œæˆ...")
            monitor_thread.join(timeout=15)  # æœ€å¤šç­‰å¾…15ç§’
                
            if monitor_thread.is_alive():
                print("ç›‘æ§çº¿ç¨‹æœªåœ¨é¢„æœŸæ—¶é—´å†…ç»“æŸï¼Œå¼ºåˆ¶ç»§ç»­...")
            else:
                print("ç›‘æ§çº¿ç¨‹å·²æ­£å¸¸ç»“æŸ")
        
            if delay_monitor_thread:
                print("ç­‰å¾…å»¶è¿Ÿç›‘æ§å®Œæˆ...")
                delay_monitor_thread.join(timeout=10)  # æœ€å¤šç­‰å¾…10ç§’
                if delay_monitor_thread.is_alive():
                    print("å»¶è¿Ÿç›‘æ§çº¿ç¨‹æœªåœ¨é¢„æœŸæ—¶é—´å†…ç»“æŸï¼Œå¼ºåˆ¶ç»§ç»­...")
                else:
                    print("å»¶è¿Ÿç›‘æ§çº¿ç¨‹å·²æ­£å¸¸ç»“æŸ")
                
            # âœ… å¢å¼ºçš„æœ€ç»ˆæŠ¥å‘Š
            if self.check_stream_delay and stream_creation_success:
                print(f"\næµå»¶è¿Ÿç›‘æ§æŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.delay_log_file}")
                self.print_final_delay_summary_enhanced(stream_creation_delay)
            else:
                self.print_final_test_summary_enhanced(stream_creation_delay)
                            
            print("\næµè®¡ç®—æµ‹è¯•å®Œæˆ!")
            
        except Exception as e:
            print(f"æµè®¡ç®—æµ‹è¯•å¤±è´¥: {str(e)}")      
            raise       

    def print_final_delay_summary_enhanced(self, stream_creation_delay):
        """å¢å¼ºçš„æœ€ç»ˆå»¶è¿Ÿæµ‹è¯•æ‘˜è¦ï¼ŒåŒ…å«é˜¶æ®µåˆ’åˆ†ä¿¡æ¯"""
        try:
            c = Colors.get_colors()
            
            print_title("\n=== æµè®¡ç®—å»¶è¿Ÿæµ‹è¯•æ€»ç»“ (å¢å¼ºç‰ˆ) ===")
            
            # âœ… æ˜¾ç¤ºæµ‹è¯•é˜¶æ®µä¿¡æ¯
            if stream_creation_delay > 0:
                print(f"ğŸ”„ æµ‹è¯•é˜¶æ®µåˆ’åˆ†:")
                print(f"  ğŸ“Š é¢„çƒ­æœŸ (0-{stream_creation_delay}ç§’): æŒç»­æ•°æ®å†™å…¥ + ç³»ç»Ÿç¨³å®š + ä¸åˆ›å»ºæµ")
                print(f"      - ç³»ç»Ÿèµ„æºç›‘æ§: å¯åŠ¨")
                print(f"      - æ•°æ®å†™å…¥: å¯åŠ¨ (è§‚å¯Ÿçº¯å†™å…¥æ€§èƒ½åŸºçº¿)")
                print(f"      - æµè®¡ç®—: æœªå¯åŠ¨")
                print(f"      - å»¶è¿Ÿç›‘æ§: æœªå¼€å§‹")
                print(f"  ğŸ”„ æµè®¡ç®—æœŸ ({stream_creation_delay}-{self.runtime*60}ç§’): åˆ›å»ºæµ + æ•°æ®å†™å…¥ + æµè®¡ç®—")
                print(f"      - ç³»ç»Ÿèµ„æºç›‘æ§: ç»§ç»­")
                print(f"      - æ•°æ®å†™å…¥: ç»§ç»­")
                print(f"      - æµè®¡ç®—: å¯åŠ¨")
                print(f"      - å»¶è¿Ÿç›‘æ§: å¼€å§‹")
                print(f"  ğŸ“ æ€»è¿è¡Œæ—¶é—´: {self.runtime*60}ç§’ ({self.runtime}åˆ†é’Ÿ)")
                
                # è®¡ç®—å®é™…æµè®¡ç®—æ—¶é—´
                actual_stream_time = self.runtime * 60 - stream_creation_delay
                print(f"  â±ï¸  å®é™…æµè®¡ç®—æ—¶é—´: {actual_stream_time}ç§’ ({actual_stream_time/60:.1f}åˆ†é’Ÿ)")
                print(f"  ğŸ” æ€§èƒ½å¯¹æ¯”: é¢„çƒ­æœŸä¸ºçº¯å†™å…¥åŸºçº¿ï¼Œæµè®¡ç®—æœŸä¸ºå†™å…¥+æµè®¡ç®—è´Ÿè½½")
            else:
                print(f"ğŸ”„ æµ‹è¯•æ¨¡å¼: ç«‹å³å¯åŠ¨æµè®¡ç®— (æ— é¢„çƒ­å»¶è¿Ÿ)")
                print(f"  ğŸ“ æ€»è¿è¡Œæ—¶é—´: {self.runtime*60}ç§’ ({self.runtime}åˆ†é’Ÿ)")
                print(f"  â±ï¸  æµè®¡ç®—æ—¶é—´: {self.runtime*60}ç§’ ({self.runtime}åˆ†é’Ÿ)")
            
            # ... å…¶ä½™ä»£ç ä¿æŒä¸å˜ ...
            
        except Exception as e:
            print(f"ç”Ÿæˆå¢å¼ºæ‘˜è¦æ—¶å‡ºé”™: {str(e)}")

    def print_final_test_summary_enhanced(self, stream_creation_delay):
        """å¢å¼ºçš„æœ€ç»ˆæµ‹è¯•æ‘˜è¦ï¼ˆæ— å»¶è¿Ÿç›‘æ§æ—¶ä½¿ç”¨ï¼‰"""
        try:
            c = Colors.get_colors()
            
            print_title("\n=== æµè®¡ç®—æ€§èƒ½æµ‹è¯•æ€»ç»“ ===")
            
            # æ˜¾ç¤ºæµ‹è¯•é˜¶æ®µä¿¡æ¯
            if stream_creation_delay > 0:
                print(f"ğŸ”„ æµ‹è¯•é˜¶æ®µåˆ’åˆ†:")
                print(f"  ğŸ“Š é¢„çƒ­æœŸ (0-{stream_creation_delay}ç§’): çº¯æ•°æ®å†™å…¥é˜¶æ®µ")
                print(f"  ğŸ”„ æµè®¡ç®—æœŸ ({stream_creation_delay}-{self.runtime*60}ç§’): æ•°æ®å†™å…¥ + æµè®¡ç®—é˜¶æ®µ")
                actual_stream_time = self.runtime * 60 - stream_creation_delay
                print(f"  â±ï¸  å®é™…æµè®¡ç®—æ—¶é—´: {actual_stream_time}ç§’ ({actual_stream_time/60:.1f}åˆ†é’Ÿ)")
            else:
                print(f"ğŸ”„ æµ‹è¯•æ¨¡å¼: ç«‹å³å¯åŠ¨æµè®¡ç®—")
                print(f"  â±ï¸  æµè®¡ç®—æ—¶é—´: {self.runtime*60}ç§’ ({self.runtime}åˆ†é’Ÿ)")
            
            print(f"\nâœ… æµ‹è¯•å·²å®Œæˆ!")
            print(f"ğŸ“„ æ€§èƒ½ç›‘æ§æ•°æ®: /tmp/perf-taosd-*.log")
            print(f"ğŸ” å»ºè®®å¯ç”¨ --check-stream-delay å‚æ•°è·å¾—å»¶è¿Ÿåˆ†æ")
            
        except Exception as e:
            print(f"ç”Ÿæˆæµ‹è¯•æ‘˜è¦æ—¶å‡ºé”™: {str(e)}")
    

    def print_final_delay_summary(self):
        """æ‰“å°æœ€ç»ˆçš„å»¶è¿Ÿæµ‹è¯•æ‘˜è¦"""
        try:
            c = Colors.get_colors()
            
            print_title("\n=== æµè®¡ç®—å»¶è¿Ÿæµ‹è¯•æ€»ç»“ ===")
            print(f"æµ‹è¯•é…ç½®:")
            print(f"  å­è¡¨æ•°é‡: {self.table_count}")
            print(f"  æ¯è½®æ’å…¥è®°å½•æ•°: {self.real_time_batch_rows}")
            print(f"  æ•°æ®å†™å…¥é—´éš”: {self.real_time_batch_sleep}ç§’")
            print(f"  å»¶è¿Ÿæ£€æŸ¥é—´éš”: {self.delay_check_interval}ç§’")
            print(f"  æœ€å¤§å»¶è¿Ÿé˜ˆå€¼: {format_delay_time(self.max_delay_threshold)}")
            print(f"  è¿è¡Œæ—¶é—´: {self.runtime}åˆ†é’Ÿ")
            print(f"  SQLç±»å‹: {self.sql_type}")
            print(f"  æµæ•°é‡: {self.stream_num}")
            
            # åˆ†æå†™å…¥å‹åŠ›
            total_records_per_round = self.table_count * self.real_time_batch_rows
            estimated_rounds = (self.runtime * 60) / max(self.delay_check_interval, 1)
            total_estimated_records = total_records_per_round * estimated_rounds
            
            print(f"\nå†™å…¥å‹åŠ›åˆ†æ:")
            print(f"  æ¯è½®æ€»è®°å½•æ•°: {total_records_per_round:,}")
            print(f"  é¢„è®¡è½®æ¬¡: {estimated_rounds:.0f}")
            print(f"  é¢„è®¡æ€»è®°å½•æ•°: {total_estimated_records:,.0f}")
            
            if self.real_time_batch_sleep > 0:
                effective_write_interval = self.delay_check_interval + self.real_time_batch_sleep
                print(f"  å®é™…å†™å…¥é—´éš”: {effective_write_interval}ç§’ (æ£€æŸ¥é—´éš” + å†™å…¥ç­‰å¾…)")
                write_rate = total_records_per_round / effective_write_interval
                print(f"  å¹³å‡å†™å…¥é€Ÿç‡: {write_rate:.0f} æ¡/ç§’")
            else:
                print(f"  å†™å…¥æ¨¡å¼: æ— é—´éš”æ§åˆ¶ (æœ€å¤§é€Ÿåº¦å†™å…¥)")
            
            print(f"\nä¼˜åŒ–å»ºè®®:")
            if self.real_time_batch_sleep == 0:
                print_warning("  - å½“å‰æ— å†™å…¥é—´éš”æ§åˆ¶ï¼Œå¦‚æœå»¶è¿Ÿè¾ƒå¤§å¯ä»¥å°è¯•å¢åŠ  --real-time-batch-sleep å‚æ•°")
            else:
                print_info(f"  - å½“å‰å†™å…¥é—´éš”: {self.real_time_batch_sleep}ç§’")
                print_info("  - å¦‚æœå»¶è¿Ÿä»ç„¶è¾ƒå¤§ï¼Œå¯ä»¥è¿›ä¸€æ­¥å¢åŠ å†™å…¥é—´éš”")
                print_info("  - å¦‚æœå»¶è¿Ÿè¾ƒå°ï¼Œå¯ä»¥å‡å°‘å†™å…¥é—´éš”ä»¥å¢åŠ æµ‹è¯•å‹åŠ›")
            
            print(f"  - å¯ä»¥è°ƒæ•´ --real-time-batch-rows å‚æ•°æ”¹å˜æ¯è½®å†™å…¥é‡")
            print(f"  - å¯ä»¥è°ƒæ•´ --delay-check-interval å‚æ•°æ”¹å˜ç›‘æ§é¢‘ç‡")
            
            print(f"\nå»¶è¿Ÿç›‘æ§æ—¥å¿—: {self.delay_log_file}")
            print(f"æ€§èƒ½ç›‘æ§æ—¥å¿—: /tmp/perf-taosd-*.log")
            
        except Exception as e:
            print(f"ç”Ÿæˆæœ€ç»ˆæ‘˜è¦æ—¶å‡ºé”™: {str(e)}")
            
            
    def do_start_bak(self):
        self.prepare_env()
        self.prepare_source_from_data()
        
        conn = taos.connect(
            host=self.host,
            user=self.user,
            password=self.passwd,
            config=self.conf
        )
        cursor = conn.cursor()

        try:
            # è¿è¡Œsource_fromçš„æ•°æ®ç”Ÿæˆ
            subprocess.Popen('taosBenchmark --f /tmp/stream_from.json', 
                            stdout=subprocess.PIPE, shell=True, text=True)
            
            # åˆ›å»ºstream_toæ•°æ®åº“
            if not self.create_database('stream_to'):
                raise Exception("åˆ›å»ºstream_toæ•°æ®åº“å¤±è´¥")
            
            time.sleep(5)
            print("æ•°æ®åº“å·²åˆ›å»º,ç­‰å¾…æ•°æ®å†™å…¥...")
            
            # ç­‰å¾…æ•°æ®å‡†å¤‡å°±ç»ª
            if not self.wait_for_data_ready(cursor, self.table_count, self.histroy_rows):
                print("æ•°æ®å‡†å¤‡å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
                return
            
            # è·å–æ–°è¿æ¥æ‰§è¡Œæµå¼æŸ¥è¯¢
            conn, cursor = self.get_connection()
            
            print("å¼€å§‹è¿æ¥æ•°æ®åº“")
            cursor.execute('use stream_from')
            
            # æ‰§è¡Œæµå¼æŸ¥è¯¢
            print(f"æ‰§è¡Œæµå¼æŸ¥è¯¢SQL:\n{self.stream_sql}")
            cursor.execute(self.stream_sql)
            
            print("æµå¼æŸ¥è¯¢å·²åˆ›å»º,å¼€å§‹ç›‘æ§ç³»ç»Ÿè´Ÿè½½")
            cursor.close()
            conn.close()
            
            # ç›‘æ§ç³»ç»Ÿè´Ÿè½½ - åŒæ—¶ç›‘æ§ä¸‰ä¸ªèŠ‚ç‚¹
            loader = MonitorSystemLoad(
                name_pattern='taosd -c', 
                count=self.runtime * 60,
                perf_file='/tmp/perf-taosd.log',  # åŸºç¡€æ–‡ä»¶å,ä¼šè‡ªåŠ¨æ·»åŠ dnodeç¼–å·
                interval=self.monitor_interval
            )
        
            try:
                loader.get_proc_status()
            except KeyboardInterrupt:
                print("\nç›‘æ§è¢«ä¸­æ–­")
            finally:
                # æ£€æŸ¥taosdè¿›ç¨‹
                result = subprocess.run('ps -ef | grep taosd | grep -v grep', 
                                    shell=True, capture_output=True, text=True)
                if result.stdout:
                    print("\ntaosdè¿›ç¨‹ä»åœ¨è¿è¡Œ")
                    print("å¦‚éœ€åœæ­¢taosdè¿›ç¨‹ï¼Œè¯·æ‰‹åŠ¨æ‰§è¡Œ: pkill taosd")
                
        except Exception as e:
            print(f"æ‰§è¡Œé”™è¯¯: {str(e)}")            
                                    
    def format_timestamp(self, ts):
        """æ ¼å¼åŒ–æ—¶é—´æˆ³ä¸ºå¯è¯»å­—ç¬¦ä¸²
        Args:
            ts: æ¯«ç§’çº§æ—¶é—´æˆ³
        Returns:
            str: æ ¼å¼åŒ–åçš„æ—¶é—´å­—ç¬¦ä¸² (YYYY-MM-DD HH:mm:ss)
        """
        return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(ts/1000))

         
    def do_query_then_insert(self):
        self.prepare_env()
        self.prepare_source_from_data()
        self.insert_source_from_data()
        
        conn = taos.connect(
            host=self.host,
            user=self.user,
            password=self.passwd,
            config=self.conf
        )
        cursor = conn.cursor()

        try:
            # è¿è¡Œsource_fromçš„æ•°æ®ç”Ÿæˆ
            subprocess.Popen('taosBenchmark --f /tmp/stream_from.json', 
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, text=True)
        except subprocess.CalledProcessError as e:
            print(f"Error running Bash command: {e}")

        # åˆ›å»ºstream_toæ•°æ®åº“
        if not self.create_database('stream_to'):
            raise Exception("åˆ›å»ºstream_toæ•°æ®åº“å¤±è´¥")
        
        time.sleep(5)
        print("æ•°æ®åº“å·²åˆ›å»º,ç­‰å¾…æ•°æ®å†™å…¥...")
        # ç­‰å¾…æ•°æ®å‡†å¤‡å°±ç»ª
        if not self.wait_for_data_ready(cursor, self.table_count, self.histroy_rows):
            print("æ•°æ®å‡†å¤‡å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
            return
        
        try:
            # å¯åŠ¨æ€§èƒ½ç›‘æ§çº¿ç¨‹
            if hasattr(self, 'monitor_warm_up_time') and self.monitor_warm_up_time is not None:
                warm_up_time = self.monitor_warm_up_time
            else:
                warm_up_time = 60 if self.runtime >= 2 else 0
        
            loader = MonitorSystemLoad(
                name_pattern='taosd -c', 
                count=self.runtime * 60,
                perf_file='/tmp/perf-taosd-query.log',
                interval=self.monitor_interval,
                deployment_mode=self.deployment_mode,
                warm_up_time=warm_up_time  
            )
            
            # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œç›‘æ§
            monitor_thread = threading.Thread(
                target=loader.get_proc_status,
                name="TaosdMonitor"
            )
            monitor_thread.daemon = True
            monitor_thread.start()
            print("å¼€å§‹ç›‘æ§taosdè¿›ç¨‹èµ„æºä½¿ç”¨æƒ…å†µ...")

            # æ•°æ®åº“è¿æ¥å’ŒæŸ¥è¯¢æ“ä½œ
            conn = taos.connect(
                host=self.host, 
                user=self.user, 
                password=self.passwd, 
                config=self.conf, 
                timezone=self.tz
            )
            cursor = conn.cursor()
            cursor.execute('use stream_to')
            
            print("å¼€å§‹æ‰§è¡ŒæŸ¥è¯¢å’Œå†™å…¥æ“ä½œ...")

            cursor.execute("create stable if not exists stream_to.stb_result(wstart timestamp, avg_c0 float, avg_c1 float, avg_c2 float,avg_c3 float, max_c0 float, max_c1 float, max_c2 float,max_c3 float, min_c0 float, min_c1 float, min_c2 float,min_c3 float) tags(gid bigint unsigned)")

            try:
                t = threading.Thread(target=do_monitor, args=(self.runtime * 60, self.perf_file))
                t.daemon = True 
                t.start()
            except Exception as e:
                print("Error: unable to start thread, %s" % e)
            finally:
                print("Execution completed")

            print("start to query")

            list = get_table_list(cursor)
            print("there are %d tables" % len(list))

            try:
                # å¾ªç¯æ‰§è¡Œå†™å…¥å’Œè®¡ç®—
                cycle_count = 0
                start_time = time.time()
                
                while True:
                    cycle_count += 1
                    print(f"\n=== å¼€å§‹ç¬¬ {cycle_count} è½®å†™å…¥å’Œè®¡ç®— ===")
                    
                    if cycle_count > 1:
                        # ä»ç¬¬äºŒè½®å¼€å§‹ï¼Œå…ˆå†™å…¥æ–°æ•°æ®
                        print(f"\nå†™å…¥æ–°ä¸€æ‰¹æµ‹è¯•æ•°æ® (æ¯è½® {self.real_time_batch_rows} æ¡è®°å½•)...")
                        
                        # åº”ç”¨å†™å…¥é—´éš”æ§åˆ¶
                        if self.real_time_batch_sleep > 0:
                            print(f"ç­‰å¾… {self.real_time_batch_sleep} ç§’åå¼€å§‹å†™å…¥æ•°æ®...")
                            time.sleep(self.real_time_batch_sleep)
                
                        if not self.update_insert_config():
                            raise Exception("å†™å…¥æ–°æ•°æ®å¤±è´¥")
                        
                        cmd = "taosBenchmark -f /tmp/stream_from_insertdata.json"
                        if subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, text=True).returncode != 0:
                            raise Exception("å†™å…¥æ–°æ•°æ®å¤±è´¥")
                
                        if self.real_time_batch_sleep > 0:
                            print(f"æ•°æ®å†™å…¥å®Œæˆï¼Œå†™å…¥é—´éš”æ§åˆ¶: {self.real_time_batch_sleep}ç§’")
                
                    for index, table in enumerate(list):
                        table_name = table[0]
                        print(f"\nå¼€å§‹å¤„ç†è¡¨ {table_name} ({index+1}/{len(list)})")
                        window_count = 0
                        
                        count_sql = f"select count(*) from stream_from.{table_name}"
                        cursor.execute(count_sql)
                        count_res = cursor.fetchall()
                        if count_res and count_res[0][0] >= 0:
                            print(f"è¡¨ {table_name} åŒ…å« {count_res[0][0]} æ¡è®°å½•")
                            cursor.execute(f"create table if not exists stream_to.{table_name}_1 using stream_to.stb_result tags(1)")
                            
                            # æŸ¥è¯¢è¡¨çš„æ—¶é—´èŒƒå›´
                            range_sql = f"select first(ts), last(ts) from stream_from.{table_name}"
                            print(f"æŸ¥è¯¢æ—¶é—´èŒƒå›´SQL: {range_sql}")
                            cursor.execute(range_sql)
                            time_range = cursor.fetchall()
                            
                            if time_range and len(time_range) > 0 and time_range[0][0]:
                                start_ts = int(time_range[0][0].timestamp() * 1000)
                                end_ts = int(time_range[0][1].timestamp() * 1000)
                                step = 15 * 1000  # 15ç§’
                                
                                # è®¡ç®—æ€»æ—¶é—´çª—å£æ•°
                                total_windows = ((end_ts - start_ts) // step) + 1
                                print(f"æ•°æ®æ—¶é—´èŒƒå›´: {self.format_timestamp(start_ts)} -> {self.format_timestamp(end_ts)}")
                                print(f"é¢„è®¡å¤„ç† {total_windows} ä¸ªæ—¶é—´çª—å£")
                                
                                # ä½¿ç”¨åˆ—è¡¨ä¿å­˜æ‰€æœ‰æ—¶é—´çª—å£
                                time_windows = []
                                current_ts = start_ts
                                while current_ts < end_ts:
                                    next_ts = min(current_ts + step, end_ts)
                                    time_windows.append((current_ts, next_ts))
                                    current_ts = next_ts
                            
                                for window_idx, (window_start, window_end) in enumerate(time_windows, 1):                                
                                    window_sql = (f"select cast({current_ts} as timestamp), "
                                        f"avg(c0), avg(c1), avg(c2), avg(c3), "
                                        f"max(c0), max(c1), max(c2), max(c3), "
                                        f"min(c0), min(c1), min(c2), min(c3) "
                                        f"from stream_from.{table_name} "
                                        f"where ts >= {window_start} and ts < {window_end}")
                                    
                                    #print(f"æ‰§è¡ŒSQLæŸ¥è¯¢: {window_sql}")
                                    cursor.execute(window_sql)
                                    window_data = cursor.fetchall()
                                    
                                    if window_data and len(window_data) > 0:
                                        # å†™å…¥æ•°æ®
                                        insert_sql = f"insert into stream_to.{table_name}_1 values ({current_ts}, {window_data[0][1]}, {window_data[0][2]}, {window_data[0][3]}, {window_data[0][4]}, {window_data[0][5]}, {window_data[0][6]}, {window_data[0][7]}, {window_data[0][8]}, {window_data[0][9]}, {window_data[0][10]}, {window_data[0][11]}, {window_data[0][12]})"
                                        
                                        cursor.execute(insert_sql)
                                        window_count += 1
                                        
                                        # æ˜¾ç¤ºè¿›åº¦
                                        print(f"\rè¿›åº¦: {(window_count/total_windows)*100:.2f}% - "
                                            f"çª—å£ [{window_count}/{total_windows}]: "
                                            f"{self.format_timestamp(current_ts)} -> "
                                            f"{self.format_timestamp(window_end)}", end='')
                                    else:
                                        print(f" stream_from.{table_name} æ²¡æœ‰æŸ¥è¯¢åˆ°æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(current_ts/1000))} -> {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(query_end/1000))}")
                                        break
                                    
                                    # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´çª—å£
                                    current_ts = window_end
                                    
                                print(f"è¡¨ {table_name} å¤„ç†å®Œæˆ, å…±å†™å…¥ {window_count} ä¸ªæ—¶é—´çª—å£çš„æ•°æ®")
                                
                            else:
                                print(f"è¡¨ {table_name} æ— æ•°æ®è®°å½•ï¼Œè·³è¿‡å¤„ç†")
                
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è¿è¡Œæ—¶é—´é™åˆ¶
                    if time.time() - start_time >= self.runtime * 60:
                        print(f"\n\nå·²è¾¾åˆ°è¿è¡Œæ—¶é—´é™åˆ¶ ({self.runtime} åˆ†é’Ÿ)ï¼Œåœæ­¢æ‰§è¡Œ")
                        break
                        
                    print(f"\n=== ç¬¬ {cycle_count} è½®å¤„ç†å®Œæˆ ===")
                        
            except Exception as e:
                print(f"æŸ¥è¯¢å†™å…¥æ“ä½œå‡ºé”™: {str(e)}")
            finally:
                cursor.close()
                conn.close()
                print("æŸ¥è¯¢å†™å…¥æ“ä½œå®Œæˆ")
        
                # ä¸»åŠ¨åœæ­¢ç›‘æ§
                print("ä¸»åŠ¨åœæ­¢ç³»ç»Ÿç›‘æ§...")
                loader.stop()
                
            # ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
            print("ç­‰å¾…ç›‘æ§æ•°æ®æ”¶é›†å®Œæˆ...")
            monitor_thread.join(timeout=15)  # æœ€å¤šç­‰å¾…15ç§’
    
            if monitor_thread.is_alive():
                print("ç›‘æ§çº¿ç¨‹æœªåœ¨é¢„æœŸæ—¶é—´å†…ç»“æŸï¼Œç¨‹åºå°†ç»§ç»­...")
            else:
                print("ç›‘æ§çº¿ç¨‹å·²æ­£å¸¸ç»“æŸ")
            
        except KeyboardInterrupt:
            print("\næ”¶åˆ°ä¸­æ–­ä¿¡å·")
            print("åœæ­¢ç›‘æ§å’ŒæŸ¥è¯¢æ“ä½œ...")
        except Exception as e:
            print(f"æ‰§è¡Œå‡ºé”™: {str(e)}")
        finally:
            print("\næ‰§è¡Œå®Œæˆ")
            print("ç›‘æ§æ•°æ®å·²ä¿å­˜åˆ°: /tmp/perf-taosd-query-*.log")
            print("å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹ç›‘æ§æ•°æ®:")
            print("cat /tmp/perf-taosd-query-all.log")
                     
    def do_query_then_insert_bak(self):
        self.prepare_env()
        self.prepare_source_from_data()
        
        conn = taos.connect(
            host=self.host,
            user=self.user,
            password=self.passwd,
            config=self.conf
        )
        cursor = conn.cursor()

        try:
            # è¿è¡Œsource_fromçš„æ•°æ®ç”Ÿæˆ
            subprocess.Popen('taosBenchmark --f /tmp/stream_from.json', 
                            stdout=subprocess.PIPE, shell=True, text=True)
        except subprocess.CalledProcessError as e:
            print(f"Error running Bash command: {e}")

        # åˆ›å»ºstream_toæ•°æ®åº“
        if not self.create_database('stream_to'):
            raise Exception("åˆ›å»ºstream_toæ•°æ®åº“å¤±è´¥")
        
        time.sleep(5)
        print("æ•°æ®åº“å·²åˆ›å»º,ç­‰å¾…æ•°æ®å†™å…¥...")
        # ç­‰å¾…æ•°æ®å‡†å¤‡å°±ç»ª
        if not self.wait_for_data_ready(cursor, self.table_count, self.histroy_rows):
            print("æ•°æ®å‡†å¤‡å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
            return
        
        try:
            # å¯åŠ¨æ€§èƒ½ç›‘æ§çº¿ç¨‹
            loader = MonitorSystemLoad(
                name_pattern='taosd -c', 
                count=self.runtime * 60,
                perf_file='/tmp/perf-taosd-query.log',
                interval=self.monitor_interval
            )
            
            # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œç›‘æ§
            monitor_thread = threading.Thread(
                target=loader.get_proc_status,
                name="TaosdMonitor"
            )
            monitor_thread.daemon = True
            monitor_thread.start()
            print("å¼€å§‹ç›‘æ§taosdè¿›ç¨‹èµ„æºä½¿ç”¨æƒ…å†µ...")

            # æ•°æ®åº“è¿æ¥å’ŒæŸ¥è¯¢æ“ä½œ
            conn = taos.connect(
                host=self.host, 
                user=self.user, 
                password=self.passwd, 
                config=self.conf, 
                timezone=self.tz
            )
            cursor = conn.cursor()
            cursor.execute('use stream_to')
            
            print("å¼€å§‹æ‰§è¡ŒæŸ¥è¯¢å’Œå†™å…¥æ“ä½œ...")

            cursor.execute("create stable if not exists stream_to.stb_result(wstart timestamp, avg_c0 float, avg_c1 float, avg_c2 float,avg_c3 float, max_c0 float, max_c1 float, max_c2 float,max_c3 float, min_c0 float, min_c1 float, min_c2 float,min_c3 float) tags(gid bigint unsigned)")

            try:
                t = threading.Thread(target=do_monitor, args=(self.runtime * 60, self.perf_file))
                t.daemon = True 
                t.start()
            except Exception as e:
                print("Error: unable to start thread, %s" % e)
            finally:
                print("Execution completed")

            print("start to query")

            list = get_table_list(cursor)
            print("there are %d tables" % len(list))

            try:
                for index, table in enumerate(list):
                    table_name = table[0]
                    print(f"\nå¼€å§‹å¤„ç†è¡¨ {table_name} ({index+1}/{len(list)})")
                    window_count = 0
                    
                    count_sql = f"select count(*) from stream_from.{table_name}"
                    cursor.execute(count_sql)
                    count_res = cursor.fetchall()
                    if count_res and count_res[0][0] >= 0:
                        print(f"è¡¨ {table_name} åŒ…å« {count_res[0][0]} æ¡è®°å½•")
                        cursor.execute(f"create table if not exists stream_to.{table_name}_1 using stream_to.stb_result tags(1)")
                        
                        # æŸ¥è¯¢è¡¨çš„æ—¶é—´èŒƒå›´
                        range_sql = f"select first(ts), last(ts) from stream_from.{table_name}"
                        print(f"æŸ¥è¯¢æ—¶é—´èŒƒå›´SQL: {range_sql}")
                        cursor.execute(range_sql)
                        time_range = cursor.fetchall()
                        
                        if time_range and len(time_range) > 0 and time_range[0][0]:
                            start_ts = int(time_range[0][0].timestamp() * 1000)
                            end_ts = int(time_range[0][1].timestamp() * 1000)
                            step = 15 * 1000  # 15ç§’
                            
                            # è®¡ç®—æ€»æ—¶é—´çª—å£æ•°
                            total_windows = ((end_ts - start_ts) // step) + 1
                            print(f"æ•°æ®æ—¶é—´èŒƒå›´: {self.format_timestamp(start_ts)} -> {self.format_timestamp(end_ts)}")
                            print(f"é¢„è®¡å¤„ç† {total_windows} ä¸ªæ—¶é—´çª—å£")
                            
                            # ä½¿ç”¨åˆ—è¡¨ä¿å­˜æ‰€æœ‰æ—¶é—´çª—å£
                            time_windows = []
                            current_ts = start_ts
                            while current_ts < end_ts:
                                next_ts = min(current_ts + step, end_ts)
                                time_windows.append((current_ts, next_ts))
                                current_ts = next_ts
                        
                            for window_idx, (window_start, window_end) in enumerate(time_windows, 1):                                
                                window_sql = (f"select cast({current_ts} as timestamp), "
                                    f"avg(c0), avg(c1), avg(c2), avg(c3), "
                                    f"max(c0), max(c1), max(c2), max(c3), "
                                    f"min(c0), min(c1), min(c2), min(c3) "
                                    f"from stream_from.{table_name} "
                                    f"where ts >= {window_start} and ts < {window_end}")
                                
                                #print(f"æ‰§è¡ŒSQLæŸ¥è¯¢: {window_sql}")
                                cursor.execute(window_sql)
                                window_data = cursor.fetchall()
                                
                                if window_data and len(window_data) > 0:
                                    # å†™å…¥æ•°æ®
                                    insert_sql = f"insert into stream_to.{table_name}_1 values ({current_ts}, {window_data[0][1]}, {window_data[0][2]}, {window_data[0][3]}, {window_data[0][4]}, {window_data[0][5]}, {window_data[0][6]}, {window_data[0][7]}, {window_data[0][8]}, {window_data[0][9]}, {window_data[0][10]}, {window_data[0][11]}, {window_data[0][12]})"
                                    
                                    cursor.execute(insert_sql)
                                    window_count += 1
                                    
                                    # æ˜¾ç¤ºè¿›åº¦
                                    print(f"\rè¿›åº¦: {(window_count/total_windows)*100:.2f}% - "
                                        f"çª—å£ [{window_count}/{total_windows}]: "
                                        f"{self.format_timestamp(current_ts)} -> "
                                        f"{self.format_timestamp(window_end)}", end='')
                                else:
                                    print(f" stream_from.{table_name} æ²¡æœ‰æŸ¥è¯¢åˆ°æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(current_ts/1000))} -> {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(query_end/1000))}")
                                    break
                                
                                # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´çª—å£
                                current_ts = window_end
                                
                            print(f"è¡¨ {table_name} å¤„ç†å®Œæˆ, å…±å†™å…¥ {window_count} ä¸ªæ—¶é—´çª—å£çš„æ•°æ®")
                            
                        else:
                            print(f"è¡¨ {table_name} æ— æ•°æ®è®°å½•ï¼Œè·³è¿‡å¤„ç†")
                        
            except Exception as e:
                print(f"æŸ¥è¯¢å†™å…¥æ“ä½œå‡ºé”™: {str(e)}")
            finally:
                cursor.close()
                conn.close()
                print("æŸ¥è¯¢å†™å…¥æ“ä½œå®Œæˆ")
                
            # ç­‰å¾…ç›‘æ§çº¿ç¨‹ç»“æŸ
            print("ç­‰å¾…ç›‘æ§æ•°æ®æ”¶é›†å®Œæˆ...")
            monitor_thread.join()
            
        except KeyboardInterrupt:
            print("\næ”¶åˆ°ä¸­æ–­ä¿¡å·")
            print("åœæ­¢ç›‘æ§å’ŒæŸ¥è¯¢æ“ä½œ...")
        except Exception as e:
            print(f"æ‰§è¡Œå‡ºé”™: {str(e)}")
        finally:
            print("\næ‰§è¡Œå®Œæˆ")
            print("ç›‘æ§æ•°æ®å·²ä¿å­˜åˆ°: /tmp/perf-taosd-query-*.log")
            print("å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹ç›‘æ§æ•°æ®:")
            print("cat /tmp/perf-taosd-query-all.log")
        
    def do_query_then_insert_no_monitor(self):
        self.prepare_env()
        self.prepare_source_from_data()

        try:
            # è¿è¡Œsource_fromçš„æ•°æ®ç”Ÿæˆ
            subprocess.Popen('taosBenchmark --f /tmp/stream_from.json', 
                            stdout=subprocess.PIPE, shell=True, text=True)
        except subprocess.CalledProcessError as e:
            print(f"Error running Bash command: {e}")

        # åˆ›å»ºstream_toæ•°æ®åº“
        if not self.create_database('stream_to'):
            raise Exception("åˆ›å»ºstream_toæ•°æ®åº“å¤±è´¥")
        
        time.sleep(10)
        print("æ•°æ®åº“å·²åˆ›å»º,ç­‰å¾…æ•°æ®å†™å…¥...")

        conn = taos.connect(
            host=self.host, user=self.user, password=self.passwd, config=self.conf, timezone=self.tz
        )

        cursor = conn.cursor()
        cursor.execute('use stream_to')

        start_ts = 1748707200000
        step = 300 # å°†æ­¥é•¿æ”¹ä¸º300ç§’

        cursor.execute("create stable if not exists stream_to.stb_result(wstart timestamp, avg_c0 float, avg_c1 float, avg_c2 float,avg_c3 float, max_c0 float, max_c1 float, max_c2 float,max_c3 float, min_c0 float, min_c1 float, min_c2 float,min_c3 float) tags(gid bigint unsigned)")

        try:
            t = threading.Thread(target=do_monitor, args=(self.runtime * 60, self.perf_file))
            t.daemon = True 
            t.start()
        except Exception as e:
            print("Error: unable to start thread, %s" % e)
        finally:
            print("Execution completed")

        print("start to query")

        list = get_table_list(cursor)
        print("there are %d tables" % len(list))

        for index, n in enumerate(list):
            cursor.execute(f"create table if not exists stream_to.{n[0]}_1 using stream_to.stb_result tags(1)")
            count = 1
            while True:
                sql = (f"select cast({start_ts + step * 1000 * (count - 1)} as timestamp), "
                        f"avg(c0), avg(c1), avg(c2), avg(c3), "
                        f"max(c0), max(c1), max(c2), max(c3), "
                        f"min(c0), min(c1), min(c2), min(c3) "
                        f"from stream_from.{n[0]} "
                        f"where ts >= {start_ts + step * 1000 * (count - 1)} "
                        f"and ts < {start_ts + step * 1000 * count}")
                print(f"æ‰§è¡ŒSQLæŸ¥è¯¢: {sql}")
                cursor.execute(sql)

                res = cursor.fetchall()
                if not res or len(res) == 0:  # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ
                    print(f"æ²¡æœ‰æŸ¥è¯¢åˆ°æ•°æ®ï¼Œæ—¶é—´èŒƒå›´: {start_ts + step * 1000 * (count - 1)} -> {start_ts + step * 1000 * count}")
                    break

                insert = f"insert into stream_to.{n[0]}_1 values ({start_ts + step * 1000 * (count - 1)}, {res[0][1]}, {res[0][2]}, {res[0][3]}, {res[0][4]}, {res[0][5]}, {res[0][6]}, {res[0][7]}, {res[0][8]}, {res[0][9]}, {res[0][10]}, {res[0][11]}, {res[0][12]})"
                print(f"Inserting: {insert}")
                cursor.execute(insert)
                count += 1
        conn.close()

    def multi_insert(self):
        self.prepare_env()
        self.prepare_source_from_data()

        try:
            subprocess.Popen('taosBenchmark --f /tmp/stream_from.json', stdout=subprocess.PIPE, shell=True, text=True)
            subprocess.Popen('taosBenchmark --f /tmp/stream_to.json', stdout=subprocess.PIPE, shell=True, text=True)
        except subprocess.CalledProcessError as e:
            print(f"Error running Bash command: {e}")

        time.sleep(10)

        for n in range(5):
            try:
                print(f"start query_insert thread {n}")
                t = threading.Thread(target=do_multi_insert, args=(n, 100, self.host, self.user, self.passwd, self.conf, self.tz))
                t.start()
            except Exception as e:
                print("Error: unable to start thread, %s" % e)

        loader = MonitorSystemLoad('taosd', self.runtime * 60, self.perf_file)
        loader.get_proc_status()

    
def main():
    def signal_handler(signum, frame):
        """ä¸»ç¨‹åºçš„ä¿¡å·å¤„ç†å™¨"""
        print("\næ”¶åˆ°ä¸­æ–­ä¿¡å·")
        print("æ­£åœ¨åœæ­¢ç›‘æ§,ä½†ä¿æŒtaosdè¿›ç¨‹è¿è¡Œ...")
        return  # ç›´æ¥è¿”å›ï¼Œä¸æ‰§è¡Œä»»ä½•åœæ­¢æ“ä½œ

    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # ä½¿ç”¨è‡ªå®šä¹‰æ ¼å¼åŒ–å™¨æ¥æ”¹å–„å¸®åŠ©ä¿¡æ¯æ˜¾ç¤º
    class CustomHelpFormatter(argparse.RawDescriptionHelpFormatter):
        def __init__(self, prog):
            super().__init__(prog, max_help_position=50, width=150)
            
        def _format_action_invocation(self, action):
            # ä¿æŒåŸæœ‰çš„è°ƒç”¨æ ¼å¼
            return super()._format_action_invocation(action)
        
        def _format_usage(self, usage, actions, groups, prefix):
            """é‡å†™ usage æ ¼å¼åŒ–ï¼Œæ”¯æŒå¤šè¡Œæ˜¾ç¤º"""
            if prefix is None:
                prefix = 'usage: '
            
            # è·å–ç¨‹åºå
            prog = usage or self._prog
            
            # æ„å»ºå‚æ•°åˆ—è¡¨
            parts = [prog]
            
            # åˆ†ç»„æ˜¾ç¤ºå‚æ•°
            optional_parts = []
            positional_parts = []
            
            for action in actions:
                if action.option_strings:
                    # å¯é€‰å‚æ•°
                    if action.option_strings:
                        opt_str = '/'.join(action.option_strings)
                        if action.nargs != 0:
                            opt_str += f' {action.dest.upper()}'
                        optional_parts.append(f'[{opt_str}]')
                else:
                    # ä½ç½®å‚æ•°
                    positional_parts.append(action.dest.upper())
            
            # ç»„åˆæ‰€æœ‰éƒ¨åˆ†
            all_parts = parts + positional_parts + optional_parts
            
            # æŒ‰è¡Œåˆ†ç»„æ˜¾ç¤º
            lines = [prefix + prog]
            current_line = ''
            max_line_length = 80
            
            for part in optional_parts:
                if len(current_line + ' ' + part) > max_line_length:
                    if current_line:
                        lines.append(' ' * len(prefix) + current_line)
                        current_line = part
                    else:
                        lines.append(' ' * len(prefix) + part)
                else:
                    current_line = current_line + ' ' + part if current_line else part
            
            if current_line:
                lines.append(' ' * len(prefix) + current_line)
                
            return '\n'.join(lines) + '\n\n'
        
        def _fill_text(self, text, width, indent):
            # ä¿æŒæ–‡æœ¬çš„åŸå§‹æ ¼å¼ï¼Œä¸è¿›è¡Œè‡ªåŠ¨æ¢è¡Œ
            return ''.join(indent + line for line in text.splitlines(keepends=True))
        
        def _split_lines(self, text, width):
            # æŒ‰ç…§ \n åˆ†å‰²è¡Œï¼Œä¿æŒæ‰‹åŠ¨æ¢è¡Œ
            if '\n' in text:
                return text.splitlines()
            else:
                # å¯¹äºæ²¡æœ‰æ¢è¡Œç¬¦çš„æ–‡æœ¬ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†
                return argparse.HelpFormatter._split_lines(self, text, width)
    
    parser = argparse.ArgumentParser(
        prog=' python3 stream_perf_test.py -m 1 --stream-perf-test-dir /home/stream_perf_test_dir',
        description='TDengine Stream Perf Test',
        formatter_class=CustomHelpFormatter,
        epilog="""
ä¸Šé¢å‚æ•°å¯ä»¥è¿›è¡Œæ­é…ç»„åˆè¿›è¡Œæµ‹è¯•
        """
    )
    
    # åŸºç¡€è¿è¡Œå‚æ•°
    basic_group = parser.add_argument_group('åŸºç¡€å‚æ•°ï¼Œæ§åˆ¶æµ‹è¯•æ¨¡å¼ã€è¿è¡Œæ—¶é—´ç­‰æ ¸å¿ƒå‚æ•°')
    # basic_group.add_argument('-m', '--mode', type=int, default=0,
    #                         help='è¿è¡Œæ¨¡å¼:\n'
    #                             '  1: do_test_stream_with_realtime_data (å†™å…¥æ•°æ®å¹¶æ‰§è¡Œå®æ—¶æ•°æ®æµè®¡ç®—)\n'
    #                             '  2: do_query_then_insert (æŒç»­æŸ¥è¯¢å†™å…¥)\n'
    #                             '  3: multi_insert (å¤šçº¿ç¨‹æ’å…¥)')
    basic_group.add_argument('-m', '--mode', type=int, default=0,
                            # help='è¿è¡Œæ¨¡å¼:\n'
                            #     '  1: do_test_stream_with_realtime_data (å†™å…¥æ•°æ®å¹¶æ‰§è¡Œå®æ—¶æ•°æ®æµè®¡ç®—)\n'
                            #     '  2: do_query_then_insert (æŒç»­æŸ¥è¯¢å†™å…¥)(å’Œæµè®¡ç®—æ— å…³)\n'
                            #     '  3: do_test_stream_with_restored_data (æ¢å¤å†å²æ•°æ®å¹¶æµ‹è¯•æŒ‡å®šæµè®¡ç®—)')
                            help='''è¿è¡Œæ¨¡å¼ (å¿…é€‰):
  1 = do_test_stream_with_realtime_data: åˆ›å»ºæ•°æ®å¹¶æ‰§è¡Œå®æ—¶æµè®¡ç®—æµ‹è¯•
      â”œâ”€ è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•æ•°æ®
      â”œâ”€ åˆ›å»ºå¹¶å¯åŠ¨æµè®¡ç®—  
      â”œâ”€ æ¨¡æ‹ŸæŒç»­å®æ—¶å†™å…¥æ•°æ®
      â””â”€ å…¨ç¨‹æ€§èƒ½ç›‘æ§
      
  2 = do_test_stream_with_restored_data: åŠ è½½å†å²æ•°æ®å¹¶æ‰§è¡Œæµè®¡ç®—æµ‹è¯•
      â”œâ”€ ä»å¤‡ä»½æ¢å¤å†å²æ•°æ®
      â”œâ”€ æµ‹è¯•æŒ‡å®šçš„æµè®¡ç®—SQL  
      â””â”€ é¿å…é‡å¤æ•°æ®ç”Ÿæˆï¼ŒèŠ‚çœæ—¶é—´
      
  3 = do_query_then_insert: æŒç»­æŸ¥è¯¢å†™å…¥(å’Œæµè®¡ç®—æ— å…³)
      â”œâ”€ æŒç»­å†™å…¥æ•°æ®
      â”œâ”€ æŒç»­æ‰§è¡ŒèšåˆæŸ¥è¯¢
      â””â”€ ç”¨äºå¯¹æ¯”æµè®¡ç®—ä¸æŸ¥è¯¢æ€§èƒ½å·®å¼‚
      
é»˜è®¤: 0 (æ˜¾ç¤ºå¸®åŠ©)\n''')
    basic_group.add_argument('-t', '--time', type=int, default=10,
                            help='è¿è¡Œæ—¶é—´(åˆ†é’Ÿ), é»˜è®¤10åˆ†é’Ÿ\n')
    basic_group.add_argument('-f', '--file', type=str, default='/tmp/perf.log',
                            help='æ€§èƒ½æ•°æ®è¾“å‡ºæ–‡ä»¶è·¯å¾„, é»˜è®¤/tmp/perf.log')
    
    # æ•°æ®ç›¸å…³å‚æ•°
    data_group = parser.add_argument_group('æ•°æ®å‚æ•°, æ§åˆ¶æ•°æ®è§„æ¨¡å’Œå†™å…¥é€Ÿåº¦')
    data_group.add_argument('--table-count', type=int, default=500,
                            help='å­è¡¨æ•°é‡, é»˜è®¤500, æµ‹è¯•å¤§æ•°æ®æ—¶éœ€è°ƒå¤§æ­¤å‚æ•°')
    data_group.add_argument('--histroy-rows', type=int, default=1,
                            help='æ¯ä¸ªå­è¡¨æ’å…¥å†å²æ•°æ®æ•°, é»˜è®¤1')
    data_group.add_argument('--real-time-batch-rows', type=int, default=200,
                            help='åç»­æ¯ä¸ªå­è¡¨æ¯è½®æ’å…¥å®æ—¶æ•°æ®æ•°, é»˜è®¤200ã€timestamp_step = 50ã€‘')
    data_group.add_argument('--real-time-batch-sleep', type=float, default=0,
                            help='æ•°æ®å†™å…¥é—´éš”æ—¶é—´(ç§’), é»˜è®¤0ç§’\n'
                                '    æ§åˆ¶æ¯è½®æ•°æ®å†™å…¥ä¹‹é—´çš„ç­‰å¾…æ—¶é—´\n'
                                '    ç”¨äºè°ƒèŠ‚å†™å…¥å‹åŠ›ï¼Œè§‚å¯Ÿæµè®¡ç®—å»¶è¿Ÿå˜åŒ–\n'
                                '    å»ºè®®å€¼: 0(æ— æ§åˆ¶), 1-10(è½»åº¦æ§åˆ¶), 10+(é‡åº¦æ§åˆ¶)\n'
                                '    ä¾‹å¦‚: --real-time-batch-sleep 5')
    data_group.add_argument('--disorder-ratio', type=int, default=0,
                            help='æ•°æ®ä¹±åºç‡, é»˜è®¤0æ— ä¹±åº, ä¹±åºè¿‡å¤šå½±å“æµè®¡ç®—é€Ÿåº¦')
    data_group.add_argument('--vgroups', type=int, default=10,
                            help='''vgroupsæ•°é‡, é»˜è®¤10\n
ç¤ºä¾‹ç”¨æ³•:%(prog)s --table-count 10000 --histroy-rows 1000 \n --real-time-batch-rows 500 --real-time-batch-sleep 5 --disorder-ratio 1 --vgroups 20 --sql-type intervalsliding_stb --time 60\n\n''')
    
    # SQLç›¸å…³å‚æ•°
    sql_group = parser.add_argument_group('æµè®¡ç®—SQLé…ç½®')
    
    # old for phase 1
    # sql_group.add_argument('--sql-type', type=str, default='s2_7',
    #                     choices=['s2_2', 's2_3', 's2_4', 's2_5', 's2_6', 
    #                              's2_7', 's2_8', 's2_9', 's2_10', 's2_11', 
    #                              's2_12', 's2_13', 's2_14', 's2_15', 'all'],
    #                      help='å®æ—¶æµè®¡ç®—SQLç±»å‹:\n'
    #                           '  s2_2: trigger at_once\n'
    #                           '  s2_3: trigger window_close\n'
    #                           '  s2_4: trigger max_delay 5s\n'
    #                           '  s2_5: trigger FORCE_WINDOW_CLOSE\n'
    #                           '  s2_6: trigger CONTINUOUS_WINDOW_CLOSE\n'
    #                           '  s2_7: INTERVAL(15s) çª—å£\n'
    #                           '  s2_8: INTERVAL with %%trows\n'
    #                           '  s2_9: INTERVAL with MAX_DELAY\n'
    #                           '  s2_10: INTERVAL with MAX_DELAY and %%trows\n'
    #                           '  s2_11: PERIOD(15s) å®šæ—¶è§¦å‘\n'
    #                           '  s2_12: SESSION(ts,10a) ä¼šè¯çª—å£\n'
    #                           '  s2_13: COUNT_WINDOW(1000) è®¡æ•°çª—å£\n'
    #                           '  s2_14: EVENT_WINDOW äº‹ä»¶çª—å£\n'
    #                           '  s2_15: STATE_WINDOW çŠ¶æ€çª—å£\n'
    #                           '  all: æ‰¹é‡æ‰§è¡Œå¤šç§æµç±»å‹')
    
    def validate_sql_type(value):
        """éªŒè¯ SQL ç±»å‹å‚æ•°"""
        valid_choices = [
            'select_stream', 'all_detailed',
            'tbname_agg', 'tbname_select', 'trows_agg', 'trows_select',
            'sourcetable_agg', 'sourcetable_select',
            'intervalsliding_stb', 'intervalsliding_stb_partition_by_tbname', 'intervalsliding_stb_partition_by_tag', 'intervalsliding_tb',
            'sliding_stb', 'sliding_stb_partition_by_tbname', 'sliding_stb_partition_by_tag', 'sliding_tb', 
            'session_stb', 'session_stb_partition_by_tbname', 'session_stb_partition_by_tag', 'session_tb',
            'count_stb', 'count_stb_partition_by_tbname', 'count_stb_partition_by_tag', 'count_tb',
            'event_stb', 'event_stb_partition_by_tbname', 'event_stb_partition_by_tag', 'event_tb',
            'state_stb', 'state_stb_partition_by_tbname', 'state_stb_partition_by_tag', 'state_tb',
            'period_stb', 'period_stb_partition_by_tbname', 'period_stb_partition_by_tag', 'period_tb',
            'intervalsliding_detailed', 'sliding_detailed', 'session_detailed', 'count_detailed',
            'event_detailed', 'state_detailed', 'period_detailed'
        ]
        
        if value not in valid_choices:
            # æŒ‰ç±»åˆ«æ ¼å¼åŒ–é”™è¯¯ä¿¡æ¯
            error_msg = f"invalid choice: '{value}'\n\næœ‰æ•ˆé€‰æ‹©é¡¹:\n"
            error_msg += "è¯´æ˜: stb(è¶…çº§è¡¨), tb(å­è¡¨), by_tbname(tbnameåˆ†ç»„), by_tag(tagåˆ†ç»„,ç¬¬ä¸€åˆ—tag)\n"
            error_msg += "æŸ¥è¯¢ç±»å‹: select_stream æŸ¥è¯¢æ‰€æœ‰æµä¿¡æ¯\n"
            error_msg += "å›ºå®šå‚æ•°ç»„åˆ: tbname_agg, tbname_select, trows_agg, trows_select, sourcetable_agg, sourcetable_select,\n"
            error_msg += "æ—¶é—´çª—å£: intervalsliding_stb, intervalsliding_stb_partition_by_tbname, intervalsliding_stb_partition_by_tag, intervalsliding_tb\n"
            error_msg += "æ»‘åŠ¨çª—å£: sliding_stb, sliding_stb_partition_by_tbname, sliding_stb_partition_by_tag, sliding_tb\n"
            error_msg += "ä¼šè¯çª—å£: session_stb, session_stb_partition_by_tbname, session_stb_partition_by_tag, session_tb\n"
            error_msg += "è®¡æ•°çª—å£: count_stb, count_stb_partition_by_tbname, count_stb_partition_by_tag, count_tb\n"
            error_msg += "äº‹ä»¶çª—å£: event_stb, event_stb_partition_by_tbname, event_stb_partition_by_tag, event_tb\n"
            error_msg += "çŠ¶æ€çª—å£: state_stb, state_stb_partition_by_tbname, state_stb_partition_by_tag, state_tb\n"
            error_msg += "å®šæ—¶è§¦å‘: period_stb, period_stb_partition_by_tbname, period_stb_partition_by_tag, period_tb\n"
            error_msg += "ç»„åˆæ¨¡æ¿: intervalsliding_detailed, session_detailed, count_detailed, event_detailed, state_detailed, period_detailed, all_detailed"
            
            raise argparse.ArgumentTypeError(error_msg)
        return value

    # ä½¿ç”¨æ—¶ä¸è®¾ç½® choicesï¼Œè€Œæ˜¯ä½¿ç”¨ type å‚æ•°è¿›è¡ŒéªŒè¯
    sql_group.add_argument('--sql-type', type=validate_sql_type, default='select_stream',
                        help='''å®æ—¶æµè®¡ç®—SQLç±»å‹:
    è¯´æ˜:
        stb(è¶…çº§è¡¨), tb(å­è¡¨), by_tbname(tbnameåˆ†ç»„), by_tag(tagåˆ†ç»„,ç¬¬ä¸€åˆ—tag)
    æŸ¥è¯¢ç±»å‹:
        select_stream: æŸ¥è¯¢æ‰€æœ‰æµä¿¡æ¯ (é»˜è®¤æŸ¥è¯¢information_schema.ins_streamsçš„æ•°æ®)
    å›ºå®šå‚æ•°ç»„åˆæ¨¡æ¿(å¿½ç•¥ --tbname-or-trows-or-sourcetable å’Œ --agg-or-select å‚æ•°):
        tbname_agg:    æ‰€æœ‰çª—å£ç±»å‹ + tbname + èšåˆæŸ¥è¯¢
        tbname_select: æ‰€æœ‰çª—å£ç±»å‹ + tbname + æŠ•å½±æŸ¥è¯¢  
        trows_agg:     æ‰€æœ‰çª—å£ç±»å‹ + trows +  èšåˆæŸ¥è¯¢
        trows_select:  æ‰€æœ‰çª—å£ç±»å‹ + trows +  æŠ•å½±æŸ¥è¯¢
        sourcetable_agg:   æ‰€æœ‰çª—å£ç±»å‹ + sourcetable + èšåˆæŸ¥è¯¢
        sourcetable_select: æ‰€æœ‰çª—å£ç±»å‹ + sourcetable + æŠ•å½±æŸ¥è¯¢
    æ—¶é—´çª—å£æ¨¡æ¿(INTERVAL(15s) SLIDING(15s)):
        intervalsliding_stb, intervalsliding_stb_partition_by_tbname, intervalsliding_stb_partition_by_tag, intervalsliding_tb
    æ»‘åŠ¨çª—å£æ¨¡æ¿(SLIDING(15s)):
        sliding_stb, sliding_stb_partition_by_tbname, sliding_stb_partition_by_tag, sliding_tb
    ä¼šè¯çª—å£æ¨¡æ¿(SESSION(ts,10a)):
        session_stb, session_stb_partition_by_tbname, session_stb_partition_by_tag, session_tb
    è®¡æ•°çª—å£æ¨¡æ¿(COUNT_WINDOW(1000)):
        count_stb, count_stb_partition_by_tbname, count_stb_partition_by_tag, count_tb
    äº‹ä»¶çª—å£æ¨¡æ¿(EVENT_WINDOW(START WITH c0 > -10000000 END WITH c0 < 10000000)):
        event_stb, event_stb_partition_by_tbname, event_stb_partition_by_tag, event_tb
    çŠ¶æ€çª—å£æ¨¡æ¿(STATE_WINDOW(c0)):
        state_stb, state_stb_partition_by_tbname, state_stb_partition_by_tag, state_tb
    å®šæ—¶è§¦å‘æ¨¡æ¿(PERIOD(15s)):
        period_stb, period_stb_partition_by_tbname, period_stb_partition_by_tag, period_tb
    ç»„åˆæ¨¡æ¿(æ¯ç»„åŒ…å«ä¸Šè¿°4ä¸ªç»„åˆ,allåŒ…å«ä¸Šè¿°æ‰€æœ‰ç»„åˆ):
        intervalsliding_detailed, sliding_detailed, session_detailed, count_detailed, event_detailed, 
        state_detailed, period_detailed, all_detailed''')
    
    sql_group.add_argument('--stream-num', type=int, default=1,
                        help='æµæ•°é‡: å½“ sql-type ä¸ºéç»„åˆæ¨¡ç‰ˆæ—¶ï¼Œåˆ›å»ºæŒ‡å®šæ•°é‡çš„ç›¸åŒæµ\n'
                                '    å¹¶å‘æµæ•°é‡, é»˜è®¤1ä¸ªæµ\n'
                                '    è®¾ç½®ä¸ºNæ—¶ï¼Œä¼šåˆ›å»ºNä¸ªç¼–å·ä¸º1åˆ°Nçš„ç›¸åŒæµï¼Œå¦‚æœæ˜¯ä»å­è¡¨è·å–æ•°æ®åˆ™ä»ç¬¬ä¸€ä¸ªå­è¡¨åˆ°ç¬¬Nä¸ªå­è¡¨\n'
                                '    éœ€è¦å’Œ --sql-type æ­é…ä½¿ç”¨ï¼Œé€‚åˆæµ‹è¯•å¤šä¸ªæµçš„å‹åŠ›\n'
                                '    ç¤ºä¾‹: --sql-type intervalsliding_stb --stream-num 100')
    
    sql_group.add_argument('--stream-sql', type=str,
                        help='è‡ªå®šä¹‰æµè®¡ç®—SQL (ä¼˜å…ˆçº§æœ€é«˜)\n'
                                '    ç¤ºä¾‹: "create stream test_stream..."')
    
    sql_group.add_argument('--agg-or-select', type=str, default='agg',
                        choices=['agg', 'select'],
                        help='æŸ¥è¯¢ç±»å‹:\n'
                            '    agg: èšåˆæŸ¥è¯¢ (avg,max,min)\n'
                            '         avg(c0), avg(c1), avg(c2), avg(c3), max(c0), max(c1), max(c2), max(c3), min(c0), min(c1), min(c2), min(c3)\n'
                            '    select: æŠ•å½±æŸ¥è¯¢ (c0,c1,c2,c3)')
    sql_group.add_argument('--custom-columns', type=str,
                        help='è‡ªå®šä¹‰æŸ¥è¯¢åˆ— (é€—å·åˆ†éš”):\n'
                                '    ç¤ºä¾‹: "sum(c0),count(*),avg(c1)"\n'
                                '          "c0,c1,c2" (æŠ•å½±æŸ¥è¯¢)\n'
                                '    æ³¨æ„: ä¼šè¦†ç›– --agg-or-select è®¾ç½®')
    sql_group.add_argument('--tbname-or-trows-or-sourcetable', type=str, default='sourcetable',
                        choices=['tbname', 'trows', 'sourcetable'],
                        help='FROMå­å¥ç±»å‹ï¼Œé»˜è®¤sourcetable:\n'
                            '    sourcetable - è¶…çº§è¡¨: from stream_from.stb where ts >= _twstart and ts <= _twend\n'
                            '    sourcetable - å­è¡¨: from stream_from.ctb0_N where ts >= _twstart and ts <= _twend\n'
                            '    tbname: from %%tbname where ts >= _twstart and ts <= _twend\n'
                            '    trows:  from %%trows ')
    
    sql_group.add_argument('--auto-combine', action='store_true',
                    help='''è‡ªåŠ¨ç”Ÿæˆå‚æ•°ç»„åˆï¼Œåˆ›å»º6ä¸ªæµ(å¿½ç•¥ --agg-or-select å’Œ --tbname-or-trows-or-sourcetable å‚æ•°):
  
åŠŸèƒ½: å¯¹æŒ‡å®šçš„å•ä¸ªæµç±»å‹ï¼Œè‡ªåŠ¨ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
  â”œâ”€ tbname + agg:         ä½¿ç”¨ %%tbname + èšåˆæŸ¥è¯¢
  â”œâ”€ tbname + select:      ä½¿ç”¨ %%tbname + æŠ•å½±æŸ¥è¯¢  
  â”œâ”€ trows + agg:          ä½¿ç”¨ %%trows + èšåˆæŸ¥è¯¢
  â”œâ”€ trows + select:       ä½¿ç”¨ %%trows + æŠ•å½±æŸ¥è¯¢
  â”œâ”€ sourcetable + agg:    ä½¿ç”¨å…·ä½“è¡¨å + èšåˆæŸ¥è¯¢
  â””â”€ sourcetable + select: ä½¿ç”¨å…·ä½“è¡¨å + æŠ•å½±æŸ¥è¯¢

é€‚ç”¨èŒƒå›´: ä»…å¯¹å•ä¸ªæµç±»å‹æœ‰æ•ˆ (å¦‚ intervalsliding_stb)
ä¸é€‚ç”¨äº: ç»„åˆæ¨¡æ¿ (*_detailed) å’Œå›ºå®šå‚æ•°æ¨¡æ¿ (tbname_aggç­‰)

ç¤ºä¾‹ç”¨æ³•:
  --sql-type intervalsliding_stb --auto-combine    # ç”Ÿæˆ6ä¸ªç»„åˆ
  --sql-type session_stb --auto-combine           # ç”Ÿæˆ6ä¸ªç»„åˆ
  
æ³¨æ„: ä½¿ç”¨æ­¤å‚æ•°æ—¶ä¼šå¿½ç•¥ --agg-or-select å’Œ --tbname-or-trows-or-sourcetable è®¾ç½®''')
    
    # todo
    sql_group.add_argument('--sql-file', type=str,
                        help='ä»æ–‡ä»¶è¯»å–æµå¼æŸ¥è¯¢SQLï¼Œtodo\n'
                            '''    ç¤ºä¾‹: --sql-file ./my_stream.sql\n
ç¤ºä¾‹ç”¨æ³•1:%(prog)s --table-count 10000 \n --real-time-batch-rows 500 --real-time-batch-sleep 5 --sql-type intervalsliding_stb --time 60\n
ç¤ºä¾‹ç”¨æ³•2:%(prog)s --table-count 10000 \n --real-time-batch-rows 500 --real-time-batch-sleep 5 --sql-type intervalsliding_stb --auto-combine --time 60\n
ç¤ºä¾‹ç”¨æ³•3:%(prog)s --table-count 10000 \n --real-time-batch-rows 500 --real-time-batch-sleep 5 --sql-type intervalsliding_detailed --time 60\n
ç¤ºä¾‹ç”¨æ³•4:%(prog)s --table-count 10000 \n --real-time-batch-rows 500 --real-time-batch-sleep 5 --sql-type tbname_agg --time 60\n
ç¤ºä¾‹ç”¨æ³•5:%(prog)s --table-count 10000 \n --real-time-batch-rows 500 --real-time-batch-sleep 5 --sql-type intervalsliding_detailed --tbname-or-trows-or-sourcetable trows\n
ç¤ºä¾‹ç”¨æ³•6:%(prog)s --table-count 10000 \n --real-time-batch-rows 500 --real-time-batch-sleep 5 --sql-type intervalsliding_detailed --agg-or-select select\n
ç¤ºä¾‹ç”¨æ³•7:%(prog)s --table-count 10000 \n --real-time-batch-rows 500 --real-time-batch-sleep 5 --sql-type intervalsliding_stb --stream-num 100\n\n''')
    
    # æµæ€§èƒ½ç›‘æ§å‚æ•°
    stream_monitor_group = parser.add_argument_group('æµæ€§èƒ½ç›‘æ§å’Œæµè®¡ç®—å»¶è¿Ÿæ£€æµ‹')
    stream_monitor_group.add_argument('--check-stream-delay', action='store_true',
                                    help='''å¯ç”¨æµè®¡ç®—å»¶è¿Ÿç›‘æ§,é»˜è®¤ä¸å¼€å¯,åŠ ä¸Šæ­¤å‚æ•°åä¼šè‡ªåŠ¨ç›‘æ§æµè®¡ç®—å»¶è¿Ÿ:
  
åŠŸèƒ½: å®æ—¶ç›‘æµ‹æºè¡¨ä¸ç›®æ ‡è¡¨çš„æ•°æ®å»¶è¿Ÿ
  â”œâ”€ æ™ºèƒ½å»¶è¿Ÿåˆ†çº§: ä¼˜ç§€/è‰¯å¥½/æ­£å¸¸/è½»å¾®/æ˜æ˜¾/ä¸¥é‡
  â”œâ”€ åŠ¨æ€é˜ˆå€¼è®¡ç®—: åŸºäºæ£€æŸ¥é—´éš”è‡ªåŠ¨è°ƒæ•´  
  â”œâ”€ é—®é¢˜è¯Šæ–­: è‡ªåŠ¨è¯†åˆ«æ— æ•°æ®/è¡¨ä¸å­˜åœ¨ç­‰é—®é¢˜
  â””â”€ ä¼˜åŒ–å»ºè®®: æä¾›å…·ä½“çš„æ€§èƒ½è°ƒä¼˜å»ºè®®

è¾“å‡º: è¯¦ç»†å»¶è¿ŸæŠ¥å‘Šä¿å­˜ä¸º /tmp/*-stream-delay.log''')
    stream_monitor_group.add_argument('--max-delay-threshold', type=int, default=30000,
                                    help='æœ€å¤§å…è®¸å»¶è¿Ÿæ—¶é—´(æ¯«ç§’), é»˜è®¤30000ms(30ç§’)\n'
                                        'è¶…è¿‡æ­¤é˜ˆå€¼è®¤ä¸ºæµè®¡ç®—è·Ÿä¸ä¸Šæ•°æ®å†™å…¥é€Ÿåº¦ï¼Œå¯ä»¥é’ˆå¯¹å…·ä½“çš„æµå’Œé—´éš”è°ƒæ•´')
    stream_monitor_group.add_argument('--delay-check-interval', type=int, default=10,
                                    help='''å»¶è¿Ÿæ£€æŸ¥é—´éš”(ç§’), é»˜è®¤10ç§’, ç›®å‰æ˜¯ç²—ç²’åº¦çš„æ£€æŸ¥ç”Ÿæˆç›®æ ‡è¡¨è¶…çº§è¡¨çš„last(ts), è€Œéæ¯ä¸ªç”Ÿæˆå­è¡¨çš„last(ts)\n
ç¤ºä¾‹ç”¨æ³•:%(prog)s --check-stream-delay \n--max-delay-threshold 60000 --delay-check-interval 5 --sql-type intervalsliding_stb \n\n''')
 
    # æ‰¹é‡æµ‹è¯•å‚æ•°
    batch_group = parser.add_argument_group('æ‰¹é‡æµ‹è¯•ï¼Œè‡ªåŠ¨æ‰§è¡Œå¤šç§å‚æ•°ç»„åˆ')
    batch_group.add_argument('--batch-test', action='store_true',
                            help='''å¯ç”¨æ‰¹é‡æµ‹è¯•æ¨¡å¼:
æ‰§è¡Œæµç¨‹:
  1. è‡ªåŠ¨ç”Ÿæˆæ‰€æœ‰æœ‰æ•ˆçš„å‚æ•°ç»„åˆ (çº¦168ç§ï¼Œå…¶ä¸­æˆåŠŸ102ç§ï¼Œå¤±è´¥66ç§)
  2. ä¸ºæ¯ä¸ªç»„åˆä¾æ¬¡æ‰§è¡Œå®Œæ•´çš„æµ‹è¯•æµç¨‹
  3. æ¯æ¬¡æµ‹è¯•å‰è‡ªåŠ¨æ¸…ç†ç¯å¢ƒå’Œé‡å¯æœåŠ¡
  4. ç”Ÿæˆè¯¦ç»†çš„HTMLæµ‹è¯•æŠ¥å‘Šå’Œæ€§èƒ½æ•°æ®
  5. æ”¯æŒæµ‹è¯•ä¸­æ–­åä»æŒ‡å®šä½ç½®ç»§ç»­

ç‰¹ç‚¹:
  â”œâ”€ å…¨è‡ªåŠ¨åŒ–: æ— éœ€äººå·¥å¹²é¢„ï¼Œè‡ªåŠ¨å¤„ç†æ‰€æœ‰ç»„åˆ
  â”œâ”€ ç¯å¢ƒéš”ç¦»: æ¯æ¬¡æµ‹è¯•å‰å®Œå…¨æ¸…ç†ç¯å¢ƒï¼Œé¿å…å¹²æ‰°
  â”œâ”€ è¯¦ç»†æŠ¥å‘Š: HTMLæ ¼å¼æŠ¥å‘Šï¼ŒåŒ…å«æˆåŠŸç‡ã€è€—æ—¶ç­‰ç»Ÿè®¡
  â”œâ”€ æµå»¶è¿Ÿç›‘æ§: æ‰¹é‡æµ‹è¯•è‡ªåŠ¨å¯ç”¨æµå»¶è¿Ÿæ£€æŸ¥ 
  â””â”€ æ–­ç‚¹ç»­ä¼ : æ”¯æŒä»æŒ‡å®šæµ‹è¯•ç¼–å·å¼€å§‹æ‰§è¡Œ

æ³¨æ„: æ‰¹é‡æµ‹è¯•ä¼šè¦†ç›–å…¶ä»–ç›¸å…³å‚æ•°è®¾ç½®ï¼Œå¹¶å¼ºåˆ¶å¯ç”¨æµå»¶è¿Ÿç›‘æ§''')
    
    batch_group.add_argument('--batch-sql-types', type=str, nargs='+',
                        help='''æŒ‡å®šæ‰¹é‡æµ‹è¯•çš„SQLç±»å‹ï¼Œæ”¯æŒå¤šä¸ªç±»å‹ç©ºæ ¼åˆ†éš”:

å•ä¸ªçª—å£ç±»å‹æ¨¡æ¿:
  intervalsliding_stb, intervalsliding_stb_partition_by_tbname, intervalsliding_stb_partition_by_tag, intervalsliding_tb
  sliding_stb, sliding_stb_partition_by_tbname, sliding_stb_partition_by_tag, sliding_tb
  session_stb, session_stb_partition_by_tbname, session_stb_partition_by_tag, session_tb
  count_stb, count_stb_partition_by_tbname, count_stb_partition_by_tag, count_tb
  event_stb, event_stb_partition_by_tbname, event_stb_partition_by_tag, event_tb
  state_stb, state_stb_partition_by_tbname, state_stb_partition_by_tag, state_tb
  period_stb, period_stb_partition_by_tbname, period_stb_partition_by_tag, period_tb

å›ºå®šå‚æ•°ç»„åˆæ¨¡æ¿(æ‰€æœ‰çª—å£ç±»å‹):
  tbname_agg:        æ‰€æœ‰çª—å£ç±»å‹ + tbname + èšåˆæŸ¥è¯¢ (28ç§ç»„åˆ)
  tbname_select:     æ‰€æœ‰çª—å£ç±»å‹ + tbname + æŠ•å½±æŸ¥è¯¢ (28ç§ç»„åˆ)
  trows_agg:         æ‰€æœ‰çª—å£ç±»å‹ + trows + èšåˆæŸ¥è¯¢ (28ç§ç»„åˆ)
  trows_select:      æ‰€æœ‰çª—å£ç±»å‹ + trows + æŠ•å½±æŸ¥è¯¢ (28ç§ç»„åˆ)
  sourcetable_agg:   æ‰€æœ‰çª—å£ç±»å‹ + sourcetable + èšåˆæŸ¥è¯¢ (28ç§ç»„åˆ)
  sourcetable_select: æ‰€æœ‰çª—å£ç±»å‹ + sourcetable + æŠ•å½±æŸ¥è¯¢ (28ç§ç»„åˆ)

ç»„åˆæ¨¡æ¿(æ¯ç»„4ç§ç»„åˆ):
  intervalsliding_detailed: æ—¶é—´çª—å£çš„24ç§ç»„åˆ
  sliding_detailed:        æ»‘åŠ¨çª—å£çš„24ç§ç»„åˆ  
  session_detailed:        ä¼šè¯çª—å£çš„24ç§ç»„åˆ
  count_detailed:          è®¡æ•°çª—å£çš„24ç§ç»„åˆ
  event_detailed:          äº‹ä»¶çª—å£çš„24ç§ç»„åˆ
  state_detailed:          çŠ¶æ€çª—å£çš„24ç§ç»„åˆ
  period_detailed:         å®šæ—¶è§¦å‘çš„24ç§ç»„åˆ

ç‰¹æ®Šç±»å‹:
  all_detailed:            æ‰€æœ‰çª—å£ç±»å‹çš„detailedç»„åˆ (168ç§)

ç¤ºä¾‹ç”¨æ³•:
  --batch-sql-types tbname_agg                    # åªæµ‹è¯•tbname+aggçš„æ‰€æœ‰çª—å£ç±»å‹
  --batch-sql-types intervalsliding_detailed      # åªæµ‹è¯•æ—¶é—´çª—å£çš„24ç§ç»„åˆ
  --batch-sql-types tbname_agg trows_agg          # æµ‹è¯•tbname_aggå’Œtrows_agg
  --batch-sql-types sliding_stb session_stb       # æµ‹è¯•æŒ‡å®šçš„ä¸¤ä¸ªå•ä¸ªæ¨¡æ¿
  
é»˜è®¤: å¦‚æœä¸æŒ‡å®šæ­¤å‚æ•°ï¼Œåˆ™æµ‹è¯•æ‰€æœ‰æœ‰æ•ˆç»„åˆ(168ç§)''')
    
    batch_group.add_argument('--batch-single-template-mode', type=str, 
                        choices=['default', 'all-combinations'], 
                        default='default',
                        help='''æ‰¹é‡æµ‹è¯•ä¸­å•ä¸ªæ¨¡æ¿çš„å¤„ç†æ–¹å¼:
    default: åªæµ‹è¯•é»˜è®¤å‚æ•°ç»„åˆ (sourcetable + agg)
    all-combinations: æµ‹è¯•æ‰€æœ‰6ç§å‚æ•°ç»„åˆ (3ä¸ªfromç±»å‹ Ã— 2ä¸ªæŸ¥è¯¢ç±»å‹)
    
ç¤ºä¾‹:
    --batch-sql-types intervalsliding_stb --batch-single-template-mode default
        åªæµ‹è¯• 1 ä¸ªç»„åˆ: intervalsliding_stb + sourcetable + agg        
    --batch-sql-types intervalsliding_stb --batch-single-template-mode all-combinations  
        æµ‹è¯• 6 ä¸ªç»„åˆ: intervalsliding_stb çš„æ‰€æœ‰å‚æ•°ç»„åˆ
        
æ³¨æ„: è¯¦ç»†æ¨¡æ¿ (*_detailed) å’Œå›ºå®šå‚æ•°æ¨¡æ¿ (tbname_aggç­‰) ä¸å—æ­¤å‚æ•°å½±å“''')
    
    batch_group.add_argument('--batch-test-time', type=int, default=5,
                            help='æ‰¹é‡æµ‹è¯•ä¸­æ¯ä¸ªæµ‹è¯•çš„è¿è¡Œæ—¶é—´(åˆ†é’Ÿ), é»˜è®¤5åˆ†é’Ÿ\n'
                                'æ€»è€—æ—¶ = ç»„åˆæ•° Ã— æ¯ä¸ªæµ‹è¯•æ—¶é—´\n'
                                'ä¾‹å¦‚168ä¸ªç»„åˆÃ—5åˆ†é’Ÿ = 14å°æ—¶')
    
    batch_group.add_argument('--batch-start-from', type=int, default=0,
                            help='ä»ç¬¬å‡ ä¸ªæµ‹è¯•å¼€å§‹(ä»0å¼€å§‹è®¡æ•°), é»˜è®¤0\n'
                                'ç”¨äºæ–­ç‚¹ç»­ä¼ ï¼Œä¾‹å¦‚ --batch-start-from 50')
    
    batch_group.add_argument('--batch-filter-mode', type=str, choices=['all', 'skip-known-failures', 'only-known-failures'], default='all',
                        help='''æ‰¹é‡æµ‹è¯•è¿‡æ»¤æ¨¡å¼ï¼Œæ§åˆ¶æ˜¯å¦è¿è¡Œå·²çŸ¥å¤±è´¥çš„æµ‹è¯•åœºæ™¯:
    all: è¿è¡Œæ‰€æœ‰168ä¸ªæµ‹è¯•åœºæ™¯ (é»˜è®¤)
    skip-known-failures: è·³è¿‡å·²çŸ¥ä¼šå¤±è´¥çš„66ä¸ªåœºæ™¯ï¼Œåªè¿è¡Œ102ä¸ªæˆåŠŸåœºæ™¯
    only-known-failures: åªè¿è¡Œå·²çŸ¥ä¼šå¤±è´¥çš„66ä¸ªåœºæ™¯ï¼Œç”¨äºè°ƒè¯•å¤±è´¥åŸå› 
  
è¯´æ˜: åŸºäºå†å²æµ‹è¯•æ•°æ®ç»Ÿè®¡ï¼ŒæŸäº›å‚æ•°ç»„åˆç”±äºTDengineé™åˆ¶ä¼šå›ºå®šå¤±è´¥
      ä½¿ç”¨æ­¤å‚æ•°å¯ä»¥é¿å…æµªè´¹æ—¶é—´åœ¨å·²çŸ¥é—®é¢˜ä¸Šï¼Œä¸“æ³¨æµ‹è¯•æœ‰æ•ˆåœºæ™¯''')
    
    batch_group.add_argument('--batch-test-limit', type=int,
                            help='''é™åˆ¶æµ‹è¯•æ•°é‡ï¼Œç”¨äºæµ‹è¯•éƒ¨åˆ†ç»„åˆ\n
ç¤ºä¾‹ç”¨æ³•1:æµ‹è¯•æ‰€æœ‰ç»„åˆ (168ç§)%(prog)s --monitor-interval 10 \n--deployment-mode single --batch-test --batch-test-time 3\n
ç¤ºä¾‹ç”¨æ³•2:æµ‹è¯•ç‰¹å®šçš„å›ºå®šå‚æ•°ç»„åˆ%(prog)s --monitor-interval 10 \n--deployment-mode single --batch-test --batch-sql-types tbname_agg \n
ç¤ºä¾‹ç”¨æ³•3:æµ‹è¯•ç‰¹å®šçš„å›ºå®šå‚æ•°ç»„åˆ%(prog)s --monitor-interval 10 \n--deployment-mode single --batch-test --batch-sql-types tbname_agg trows_agg \n
ç¤ºä¾‹ç”¨æ³•4:æµ‹è¯•ç‰¹å®šçª—å£ç±»å‹çš„è¯¦ç»†ç»„åˆ%(prog)s --monitor-interval 10 \n--deployment-mode single --batch-test --batch-sql-types intervalsliding_detailed --batch-test-time 3\n
ç¤ºä¾‹ç”¨æ³•5:æµ‹è¯•ç‰¹å®šçª—å£ç±»å‹çš„è¯¦ç»†ç»„åˆ%(prog)s --monitor-interval 10 \n--deployment-mode single --batch-test --batch-sql-types sliding_detailed session_detailed --batch-test-time 3\n
ç¤ºä¾‹ç”¨æ³•6:æµ‹è¯•æŒ‡å®šçš„å•ä¸ªæ¨¡æ¿%(prog)s --monitor-interval 10 \n--deployment-mode single --batch-test --batch-sql-types intervalsliding_stb sliding_stb \n
ç¤ºä¾‹ç”¨æ³•7:æ··åˆæµ‹è¯•æ¨¡å¼%(prog)s --monitor-interval 10 \n--deployment-mode single --batch-test --batch-sql-types tbname_agg intervalsliding_detailed sliding_stb \n
ç¤ºä¾‹ç”¨æ³•8:æ–­ç‚¹ç»­ä¼ %(prog)s --monitor-interval 10 \n--deployment-mode single --batch-test --batch-start-from 50 --batch-test-limit 20 --batch-test-time 3\n
ç¤ºä¾‹ç”¨æ³•9:é™åˆ¶æµ‹è¯•æ•°é‡%(prog)s --monitor-interval 10 \n--deployment-mode single --batch-test --batch-sql-types all_detailed --batch-test-limit 10\n
ç¤ºä¾‹ç”¨æ³•10:åªè¿è¡ŒæˆåŠŸçš„æµ‹è¯•%(prog)s --monitor-interval 10 \n--deployment-mode single --batch-test --batch-filter-mode skip-known-failures \n
ç¤ºä¾‹ç”¨æ³•11:åªè¿è¡Œå¤±è´¥çš„æµ‹è¯•%(prog)s --monitor-interval 10 \n--deployment-mode single --batch-test --batch-filter-mode only-known-failures \n
ç¤ºä¾‹ç”¨æ³•12:æˆåŠŸ/å¤±è´¥çš„ç»„åˆ%(prog)s --monitor-interval 10 \n--deployment-mode single --batch-test --batch-sql-types intervalsliding_detailed --batch-filter-mode skip-known-failures\n\n''')
     

   
    # ç³»ç»Ÿé…ç½®å‚æ•°
    system_group = parser.add_argument_group('ç³»ç»Ÿé…ç½®ï¼Œé…ç½®TDengineéƒ¨ç½²æ¶æ„å’Œç³»ç»Ÿå‚æ•°')
    system_group.add_argument('--stream-perf-test-dir', type=str, 
                            default='/home/stream_perf_test_dir',
                            help='å•æœºæˆ–è€…é›†ç¾¤æµ‹è¯•æ ¹ç›®å½•, é»˜è®¤/home/stream_perf_test_dir, å°½é‡é…ç½®æ­¤å‚æ•°é¿å…è¯¯åˆ ç›®å½•å¤–çš„æ–‡ä»¶\n'
                                 'å­˜å‚¨é…ç½®æ–‡ä»¶ã€æ•°æ®æ–‡ä»¶ã€æ—¥å¿—æ–‡ä»¶\n'
                                 'ç¡®ä¿ç›®å½•æœ‰è¶³å¤Ÿç©ºé—´å’Œè¯»å†™æƒé™')
    system_group.add_argument('--monitor-interval', type=int, default=5,
                            help='æ€§èƒ½æ•°æ®é‡‡é›†é—´éš”(ç§’), é»˜è®¤5ç§’\n'
                                  'ç›‘æ§CPUã€å†…å­˜ã€ç£ç›˜IOä½¿ç”¨æƒ…å†µ\nè¿™ä¸ªæ˜¯ç›‘æ§taosdå ç”¨èµ„æºçš„ï¼Œdelay-check-intervalæ˜¯æ£€æŸ¥æµæ•°æ®ç”Ÿæˆçš„\n'
                                  'å»ºè®®å€¼: 1(è¯¦ç»†), 5(æ ‡å‡†), 10(ç²—ç•¥)')
    system_group.add_argument('--deployment-mode', type=str, default='single',
                            choices=['single', 'cluster'],
                            help='éƒ¨ç½²æ¨¡å¼:é»˜è®¤single\n'
                                '    single:  å•èŠ‚ç‚¹æ¨¡å¼ï¼Œdnode+mnode+snode éƒ½åœ¨ä¸€ä¸ªèŠ‚ç‚¹\n'
                                '    cluster: ä¸‰èŠ‚ç‚¹é›†ç¾¤æ¨¡å¼ï¼Œ3ä¸ªdnodeï¼Œ3ä¸ªmnodeï¼Œæºæ•°æ®åº“vgroupséƒ½åœ¨dnode1ï¼Œç›®æ ‡æ•°æ®åº“vgroupséƒ½åœ¨dnode2ï¼Œsnodeç‹¬ç«‹åœ¨dnode3')
    system_group.add_argument('--debug-flag', type=int, default=131,
                            help='TDengineè°ƒè¯•çº§åˆ«, é»˜è®¤131\n'
                                '    å¸¸ç”¨å€¼: 131(é»˜è®¤), 135, 143')
    system_group.add_argument('--monitor-warm-up-time', type=int, default=0,
                            help='æ€§èƒ½ç›‘æ§é¢„çƒ­æ—¶é—´(ç§’), é»˜è®¤0ç§’\n'
                             '    åœ¨æ­¤æœŸé—´æ”¶é›†æ•°æ®ä½†ä¸æ‰“å°åˆ°æ§åˆ¶å°ï¼Œç­‰å¾…ç³»ç»Ÿç¨³å®š\n'
                             '    å½“è¿è¡Œæ—¶é—´ >= 2åˆ†é’Ÿæ—¶ï¼Œè‡ªåŠ¨è®¾ç½®ä¸º60ç§’\n'
                             '    å½“è¿è¡Œæ—¶é—´ < 2åˆ†é’Ÿæ—¶ï¼Œè‡ªåŠ¨è®¾ç½®ä¸º0ç§’\n'
                             '    ä¹Ÿå¯æ‰‹åŠ¨æŒ‡å®š: --monitor-warm-up-time 120')
    system_group.add_argument('--num-of-log-lines', type=int, default=5000000,
                            help='''TDengineæ—¥å¿—æ–‡ä»¶æœ€å¤§è¡Œæ•°, é»˜è®¤5000000,æœ€å¤§å€¼ä¸èƒ½è¶…è¿‡2000000000\n
ç¤ºä¾‹ç”¨æ³•:%(prog)s --monitor-interval 10 \n--deployment-mode single --debug-flag 135 --num-of-log-lines 1000\n\n''')
    
    # æ•°æ®ç®¡ç†å‚æ•°
    data_mgmt_group = parser.add_argument_group('æ•°æ®ç®¡ç†')
    data_mgmt_group.add_argument('--create-data', action='store_true',
                                help='''åˆ›å»ºæµ‹è¯•æ•°æ®å¹¶è‡ªåŠ¨å¤‡ä»½:  
æ‰§è¡Œæµç¨‹:
  1. å‡†å¤‡TDengineç¯å¢ƒ (å¯åŠ¨æœåŠ¡ã€é…ç½®å•èŠ‚ç‚¹æˆ–è€…é›†ç¾¤)
  2. åˆ›å»ºæŒ‡å®šè§„æ¨¡çš„æµ‹è¯•æ•°æ®  
  3. è‡ªåŠ¨å¤‡ä»½åˆ° --stream-perf-test-dir/data_bak
  4. ä¿ç•™ç¯å¢ƒä¾›åç»­æµ‹è¯•å†å²æ•°æ®æµè®¡ç®—ä½¿ç”¨
å»ºè®®: æµ‹è¯•å¤šç»„å†å²æ•°æ®æµè®¡ç®—å‰å…ˆæ‰§è¡Œæ­¤æ“ä½œ\n
ç¤ºä¾‹ç”¨æ³•:åˆ›å»ºå¹¶å¤‡ä»½æµ‹è¯•æ•°æ®%(prog)s --create-data 
--deployment-mode single --table-count 500 --histroy-rows 10000000\n\n''')
    
    data_mgmt_group.add_argument('--restore-data', action='store_true',
                                help='''ä»å¤‡ä»½æ¢å¤æµ‹è¯•æ•°æ®: 
æ‰§è¡Œæµç¨‹:
  1. æ£€æŸ¥å¤‡ä»½æ•°æ®å®Œæ•´æ€§
  2. åœæ­¢ç°æœ‰TDengineæœåŠ¡
  3. æ¢å¤æ•°æ®æ–‡ä»¶
  4. é‡å¯æœåŠ¡å¹¶éªŒè¯
é€‚ç”¨: å¿«é€Ÿæ¢å¤åˆ°å·²çŸ¥æ•°æ®çŠ¶æ€ï¼Œé¿å…é‡å¤æ•°æ®ç”Ÿæˆï¼ŒèŠ‚çœæ—¶é—´\nå¦‚æœæµ‹è¯•å†å²æ•°æ®è¿›è¡Œæµè®¡ç®—ï¼Œå»ºè®®ç”¨-m 2æ¨¡å¼\n
ç¤ºä¾‹ç”¨æ³•:æ¢å¤æ•°æ®å¹¶æµ‹è¯•%(prog)s --restore-data --deployment-mode single''')
    
    args = parser.parse_args()
    
    # æ‰“å°è¿è¡Œå‚æ•°
    print("è¿è¡Œå‚æ•°:")
    print(f"è¿è¡Œæ¨¡å¼: {args.mode}")
    print(f"è¿è¡Œæ—¶é—´: {args.time}åˆ†é’Ÿ")
    print(f"æ€§èƒ½æ–‡ä»¶: {args.file}")
    print(f"å­è¡¨æ•°é‡: {args.table_count}")
    print(f"åˆå§‹æ’å…¥è®°å½•æ•°: {args.histroy_rows}")
    print(f"åç»­æ¯è½®æ’å…¥è®°å½•æ•°: {args.real_time_batch_rows}")
    print(f"æ•°æ®å†™å…¥é—´éš”: {args.real_time_batch_sleep}ç§’")
    print(f"æ•°æ®ä¹±åº: {args.disorder_ratio}")
    print(f"vgroupsæ•°: {args.vgroups}")
    print(f"æµç±»å‹: {args.sql_type}")
    print(f"æµæ•°é‡: {args.stream_num}")
    print(f"é›†ç¾¤ç›®å½•: {args.stream_perf_test_dir}")
    print(f"æ€§èƒ½æ•°æ®é‡‡é›†é—´éš”: {args.monitor_interval}ç§’")
    print(f"éƒ¨ç½²æ¨¡å¼: {args.deployment_mode}")
    print(f"è°ƒè¯•æ ‡å¿—: {args.debug_flag}")
    print(f"æ—¥å¿—è¡Œæ•°: {args.num_of_log_lines}")
    
    print(f"æµå»¶è¿Ÿæ£€æŸ¥: {'å¯ç”¨' if args.check_stream_delay else 'ç¦ç”¨'}")
    if args.check_stream_delay:
        print(f"æœ€å¤§å»¶è¿Ÿé˜ˆå€¼: {args.max_delay_threshold}ms")
        print(f"å»¶è¿Ÿæ£€æŸ¥é—´éš”: {args.delay_check_interval}ç§’")
    
    # å¤„ç†SQLå‚æ•°
    stream_sql = None
    if args.sql_file:
        try:
            with open(args.sql_file, 'r') as f:
                stream_sql = f.read().strip()
                print(f"ä»æ–‡ä»¶åŠ è½½SQL: {args.sql_file}")
        except Exception as e:
            print(f"è¯»å–SQLæ–‡ä»¶å¤±è´¥: {e}")
            return
    elif args.stream_sql:
        stream_sql = args.stream_sql
        print("ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šSQL")
    else:
        print("ä½¿ç”¨é»˜è®¤SQL")
        
    custom_columns = None
    if args.custom_columns:
        custom_columns = [col.strip() for col in args.custom_columns.split(',')]
    
    # åˆ›å»ºStreamStarterå®ä¾‹
    try:
        starter = StreamStarter(
            runtime=args.time,
            perf_file=args.file,
            table_count=args.table_count,
            histroy_rows=args.histroy_rows,
            real_time_batch_rows=args.real_time_batch_rows,
            real_time_batch_sleep=args.real_time_batch_sleep,
            disorder_ratio=args.disorder_ratio,
            vgroups=args.vgroups,
            stream_sql=stream_sql,
            sql_type=args.sql_type,
            stream_num=args.stream_num, 
            stream_perf_test_dir=args.stream_perf_test_dir,
            monitor_interval=args.monitor_interval,
            deployment_mode=args.deployment_mode,
            debug_flag=args.debug_flag, 
            num_of_log_lines=args.num_of_log_lines,            
            agg_or_select=args.agg_or_select,
            tbname_or_trows_or_sourcetable=args.tbname_or_trows_or_sourcetable,
            custom_columns=custom_columns,
            check_stream_delay=args.check_stream_delay,
            max_delay_threshold=args.max_delay_threshold,
            delay_check_interval=args.delay_check_interval,
            auto_combine=args.auto_combine,
            monitor_warm_up_time=args.monitor_warm_up_time
        )
        
        # å¤„ç†æ‰¹é‡æµ‹è¯•
        if args.batch_test:
            print("å¯åŠ¨æ‰¹é‡æµ‹è¯•æ¨¡å¼...")
            
            # å‡†å¤‡åŸºç¡€å‚æ•°
            base_args = {
                'time': args.batch_test_time,
                'stream_perf_test_dir': args.stream_perf_test_dir,
                'table_count': args.table_count,
                'histroy_rows': args.histroy_rows,
                'real_time_batch_rows': args.real_time_batch_rows,
                'real_time_batch_sleep': args.real_time_batch_sleep,
                'disorder_ratio': args.disorder_ratio,
                'vgroups': args.vgroups,
                'monitor_interval': args.monitor_interval,
                'deployment_mode': args.deployment_mode,
                'debug_flag': args.debug_flag,
                'num_of_log_lines': args.num_of_log_lines,
                'max_delay_threshold': args.max_delay_threshold,
                'delay_check_interval': args.delay_check_interval,
                'stream_num': args.stream_num,
                'monitor_warm_up_time': args.monitor_warm_up_time  
            }
        
            print(f"æ‰¹é‡æµ‹è¯•åŸºç¡€é…ç½®:")
            print(f"  æ¯ä¸ªæµ‹è¯•è¿è¡Œæ—¶é—´: {args.batch_test_time}åˆ†é’Ÿ")
            print(f"  å­è¡¨æ•°é‡: {args.table_count}")
            print(f"  æ¯è½®å†™å…¥è®°å½•æ•°: {args.real_time_batch_rows}")
            print(f"  æ•°æ®å†™å…¥é—´éš”: {args.real_time_batch_sleep}ç§’")
            print(f"  éƒ¨ç½²æ¨¡å¼: {args.deployment_mode}")
            print(f"  è¿‡æ»¤æ¨¡å¼: {args.batch_filter_mode}")
            print(f"  æµæ•°é‡: {args.stream_num}") 
            print(f"  ç›‘æ§é¢„çƒ­æ—¶é—´: {args.monitor_warm_up_time}ç§’") 
            
            # å¤„ç†SQLç±»å‹å‚æ•°
            if args.batch_sql_types:
                print(f"  æŒ‡å®šSQLç±»å‹: {', '.join(args.batch_sql_types)}")
                # éªŒè¯æŒ‡å®šçš„SQLç±»å‹æ˜¯å¦æœ‰æ•ˆ
                valid_types = [
                    'tbname_agg', 'tbname_select', 'trows_agg', 'trows_select', 
                    'sourcetable_agg', 'sourcetable_select',
                    'intervalsliding_detailed', 'sliding_detailed', 'session_detailed', 
                    'count_detailed', 'event_detailed', 'state_detailed', 'period_detailed',
                    'all_detailed',
                    'intervalsliding_stb', 'intervalsliding_stb_partition_by_tbname', 
                    'intervalsliding_stb_partition_by_tag', 'intervalsliding_tb',
                    'sliding_stb', 'sliding_stb_partition_by_tbname',
                    'sliding_stb_partition_by_tag', 'sliding_tb',
                    'session_stb', 'session_stb_partition_by_tbname',
                    'session_stb_partition_by_tag', 'session_tb',
                    'count_stb', 'count_stb_partition_by_tbname',
                    'count_stb_partition_by_tag', 'count_tb',
                    'event_stb', 'event_stb_partition_by_tbname',
                    'event_stb_partition_by_tag', 'event_tb',
                    'state_stb', 'state_stb_partition_by_tbname',
                    'state_stb_partition_by_tag', 'state_tb',
                    'period_stb', 'period_stb_partition_by_tbname',
                    'period_stb_partition_by_tag', 'period_tb'
                ]
                
                invalid_types = [t for t in args.batch_sql_types if t not in valid_types]
                if invalid_types:
                    print(f"é”™è¯¯: æ— æ•ˆçš„SQLç±»å‹: {', '.join(invalid_types)}")
                    print(f"æœ‰æ•ˆç±»å‹è¯·å‚è€ƒ --help ä¸­çš„ --batch-sql-types è¯´æ˜")
                    return
            else:
                print(f"  SQLç±»å‹: å…¨éƒ¨ç»„åˆ (é»˜è®¤)")
            
            # åˆ›å»ºæ‰¹é‡æµ‹è¯•å™¨
            batch_tester = StreamBatchTester(
                base_args, 
                args.batch_sql_types,
                filter_mode=args.batch_filter_mode,
                single_template_mode=args.batch_single_template_mode
            )
            
            # æ‰§è¡Œæ‰¹é‡æµ‹è¯•
            batch_tester.run_batch_tests(
                test_time_minutes=args.batch_test_time,
                start_from=args.batch_start_from,
                test_limit=args.batch_test_limit
            )
            
            return
        
        if args.create_data:
            print("\n=== å¼€å§‹åˆ›å»ºæµ‹è¯•æ•°æ® ===")
            print(f"å­è¡¨æ•°é‡: {args.table_count}")
            print(f"æ¯è¡¨è®°å½•æ•°: {args.histroy_rows}")
            print(f"æ•°æ®ä¹±åºç‡: {args.disorder_ratio}")
            print(f"vgroupsæ•°: {args.vgroups}\n")
            
            if starter.create_test_data():
                print("\næµ‹è¯•æ•°æ®åˆ›å»ºå®Œæˆ!")
            return
            
        if args.restore_data:
            print("\n=== å¼€å§‹æ¢å¤æµ‹è¯•æ•°æ® ===")
            if starter.restore_cluster_data():
                print("\næ•°æ®æ¢å¤å®Œæˆ!")
                
                # å¦‚æœæŒ‡å®šäº†SQLç±»å‹ä¸”ä¸æ˜¯é»˜è®¤å€¼ï¼Œæç¤ºå¯ä»¥è¿›è¡Œæµè®¡ç®—æµ‹è¯•
                if args.sql_type != 'select_stream' or args.stream_sql or args.sql_file:
                    print(f"\næç¤º: æ•°æ®å·²æ¢å¤ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æµ‹è¯•æµè®¡ç®—:")
                    print(f"python3 {sys.argv[0]} -m 2 --sql-type {args.sql_type} --time {args.time}")
            return
        
        print("\nå¼€å§‹æ‰§è¡Œ...")
        if args.mode == 1:
            print("æ‰§è¡Œæ¨¡å¼: do_test_stream_with_realtime_data")
            starter.do_test_stream_with_realtime_data()
        elif args.mode == 3:
            print("æ‰§è¡Œæ¨¡å¼: do_query_then_insert")
            starter.do_query_then_insert()
        elif args.mode == 2:
            print("æ‰§è¡Œæ¨¡å¼: do_test_stream_with_restored_data (æ¢å¤æ•°æ®å¹¶æµ‹è¯•æŒ‡å®šæµè®¡ç®—)")
            starter.do_test_stream_with_restored_data()
        else:
            print(f"é”™è¯¯: ä¸æ”¯æŒçš„æ‰§è¡Œæ¨¡å¼ {args.mode}")
            
    except KeyboardInterrupt:
        print("\nç¨‹åºé€€å‡º")
        # æ£€æŸ¥taosdè¿›ç¨‹çŠ¶æ€
        result = subprocess.run('ps -ef | grep "taosd -c" | grep -v grep', 
                                shell=True, capture_output=True, text=True)
        if result.stdout:
            print("\ntaosdè¿›ç¨‹ä»åœ¨è¿è¡Œ:")
            print(result.stdout)
        else:
            print("\nè­¦å‘Š: æœªæ‰¾åˆ°è¿è¡Œä¸­çš„taosdè¿›ç¨‹")
    except Exception as e:
        print(f"\nç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
    finally:
        print("\nå¦‚éœ€æ‰‹åŠ¨åœæ­¢taosdè¿›ç¨‹ï¼Œè¯·æ‰§è¡Œ:")
        print("pkill taosd")

if __name__ == "__main__":
    main()
